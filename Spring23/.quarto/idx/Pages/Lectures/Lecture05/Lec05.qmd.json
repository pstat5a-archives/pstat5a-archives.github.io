{"title":"PSTAT 5A: Lecture 05","markdown":{"yaml":{"title":"PSTAT 5A: Lecture 05","subtitle":"Conditional Probability","author":"Ethan P. Marzban","date":"04/18/23","format":{"revealjs":{"html-math-method":"mathjax","theme":["default","custom.scss"],"incremental":true,"logo":"5a_hex.png","template-partials":["title-slide.html"]}},"editor":"source","title-slide-attributes":{"data-background-image":"5a_hex.png","data-background-size":"contain","data-background-opacity":"0.5","data-background-position":"left"}},"headingText":"Leadup","containsRefs":false,"markdown":"\n\n```{r setup, include=FALSE}\nlibrary(tidyverse)\nlibrary(ggthemes)\n```\n\n\n- Remember how, back in Week 1, we discussed ways to compare two variables?\n\n- At the time, we only considered comparing two numerical variables and comparing one numerical and one categorical variable.\n\n- What about comparing two categorical variables?\n\n- As a concrete example, let's return to...\n\n\n#  {background-color=\"black\" background-image=\"https://media1.giphy.com/media/ewHSMEx2TtEo8/giphy.gif?cid=ecf05e473oli5unidr1x52elmg1gi4ghtyrzg8na14wjuhcr&rid=giphy.gif&ct=g\" background-size=\"contain\"}\n\n## Penguins, Revisited\n\n```{r, message = F, echo = F}\n#| class-output: hscroll\n\nlibrary(palmerpenguins)\nlibrary(tidyverse)\n# penguins\ncat(format(as_tibble(penguins))[-c(3L, 1L)], sep = \"\\n\")\n```\n\n```{css, echo=FALSE}\n.hscroll {\n  overflow-x: auto;\n  white-space: nowrap;\n}\n```\n\n## Penguins, Subsetted\n\n```{r, message = F, echo = F}\n#| class-output: hscroll\n\nlibrary(palmerpenguins)\nlibrary(tidyverse)\n# penguins\npenguins_subsetted <- data.frame(penguins$species, penguins$island)\ncolnames(penguins_subsetted) <- c(\"species\", \"island\")\ncat(format(as_tibble(penguins_subsetted))[-c(3L, 1L)], sep = \"\\n\")\n```\n\n```{css, echo=FALSE}\n.hscroll {\n  overflow-x: auto;\n  white-space: nowrap;\n}\n```\n\n## Some Questions {style=\"font-size:30px\"}\n\n- Here are some questions we could ask:\n\n    - How many Adelie penguins were found on Biscoe island?\n    - Were any Gentoo penguins found on Torgersen island?\n    - What proportion of Chinstrap penguins were found on Dream island?\n\n- These sorts of questions are very nicely answered by way of what is known as a **contingency table**:\n\n:::{.fragment style=\"text-align:center\"}\n```{r}\n#| echo: false\n\ntable(penguins$species, penguins$island)\n```\n:::\n\n- Thus, the answers to the questions above are: \"44\", \"no\", and \"100%\", respectively.\n\n## Why Bring This Up Now?\n\n- You may be asking yourselves: \"why bring this up now? Weren't we talking about *probability*?\"\n\n- Let's re-examine the third question we asked on the previous slide: What proportion of Chinstrap penguins were found on Dream island?\n\n- What we really did when we answered this was to first restrict ourselves to the row of the contingency table corresponding to Chinstrap penguins, tally up the entries in that row, and then divided the number of penguins that were *both* Chinstrap *and* found on Dream Island by the row total.\n\n---\n\n:::{style=\"font-size:32px\"}\n:::{.nonincremental}\n- Let's walk through another example. Suppose we want to know what proportion of Adelie penguins were found on Dream Island.\n:::\n```{r}\n#| echo: false\n\ntable(penguins$species, penguins$island)\n```\n\n- By obtaining the first row total (i.e. the sum of the entries in the first row), we see there were $44 + 56 + 52 = 152$ Adelie penguins in total.\n\n    - Of these $152$ Adelie penguins, only 56 were found on Dream Island. Hence, the desired probability is\n    $$ \\frac{56}{152} \\approx 36.84\\% $$\n\n:::\n\n## Conditional Probability\n\n- This leads us to the main topic of today's lecture: **conditional probabilities.**\n\n:::{.fragment}\n:::{.callout-note}\n## **Definition**\n::: {style=\"font-size: 30px\"}\nIf $E$ and $F$ are two events with $\\mathbb{P}(E) \\neq 0$, then we define the probability of $E$ **given** $F$, notated $\\mathbb{P}(E \\mid F)$, to be\n$$ \\mathbb{P}(E \\mid F) = \\frac{\\mathbb{P}(E \\cap F)}{\\mathbb{P}(F)} $$\nIf $\\mathbb{P}(F) = 0$, then $\\mathbb{P}(E \\mid F)$ is not defined.\n:::\n:::\n:::\n\n## Interpretation {style=\"font-size:30px\"}\n\n- $\\mathbb{P}(E \\mid F)$ essentially gives us the proportion of $F$ that is explained by $E$.\n    - As such, another way to think about conditional probabilities is as an \"if-then\" statement: if $F$ has occurred, what is the probability that $E$ also occurs?\n\n- If we adopt the classical approach to probability, we have\n\\begin{align*}\n  \\mathbb{P}(E \\mid F)    & = \\frac{\\mathbb{P}(E \\cap F)}{\\mathbb{P}(F)}    \\\\\n    & = \\frac{\\left( \\frac{\\#(E \\cap F)}{\\#(\\Omega)} \\right)}{\\left( \\frac{\\#(F)}{\\#(\\Omega)} \\right)} = \\frac{\\#(E \\cap F)}{\\#(F)}\n\\end{align*}\n\n- This is also why contingency tables are so useful in the context of probability- the numerator above will be an entry in the table, and the denominator will be either a row-sum or a column-sum (depending on how the table was constructed).\n\n## Worked-Out Example {style=\"font-size:32px\"}\n\n::: callout-tip\n## Worked-Out Exercise 1\n\n::: {style=\"font-size: 30px\"}\n75 UCSB students were surveyed about whether they like pineapple on pizza or not. In addition to their pineapple preference, their standing was also recorded.\n\n```{r}\n#| echo: false\n\ndf <- matrix(c(\n  rep(c(\"Yes\", \"Freshman\"), 10),\n  rep(c(\"Yes\", \"Sophomore\"), 13),\n  rep(c(\"Yes\", \"Junior\"), 7),\n  rep(c(\"Yes\", \"Senior\"), 5),\n  rep(c(\"No\", \"Freshman\"), 15),\n  rep(c(\"No\", \"Sophomore\"), 18),\n  rep(c(\"No\", \"Junior\"), 6),\n  rep(c(\"No\", \"Senior\"), 1)\n), ncol = 2, byrow = T)\ncolnames(df) <- c(\"Pineapple\", \"Standing\")\ndf <- data.frame(df)\ndf$Pineapple <- factor(df$Pineapple)\ndf$Standing <- factor(df$Standing)\n\ntable(df)\n```\n\nA student is to be randomly selected. If the student is a Freshman, what is the probability that they like pineapple on pizza?\n:::\n:::\n\n- As always, we begin by defining events and notation. Let $P =$ \"the student likes pineapple on pizza\" and $F =$ \"the student is a freshman\". We then seek $\\mathbb{P}(P \\mid F)$.\n\n---\n\n:::{style=\"font-size:33px\"}\n:::{.nonincremental}\n- Because the selection of the student is to be done *at random*, we can use the classical approach to probability.\n:::\n\n- As such, we need only to divide the number of students who are both Freshmen and like pineapple on pizza by the total number of Freshman.\n\n    - The number of students who are both Freshmen and like pineapple on pizza is 10\n    - The total number of Freshman is $15 + 10 = 25$; i.e. the first column-sum\n    - Hence, the desired probability is\n    $$ \\mathbb{P}(P \\mid F) = \\frac{10}{25} = \\boxed{ \\frac{2}{5} = 40\\%} $$\n:::\n\n## Multiplication Rule\n\n- Recall that. provided $\\mathbb{P}(F) \\neq 0$\n$$ \\mathbb{P}(E \\mid F) = \\frac{\\mathbb{P}(E \\cap F)}{\\mathbb{P}(F)} $$\n- We can multiply both sides of this equation by $\\mathbb{P}(F)$ to obtain the so-called **multiplication rule**:\n\n:::{.fragment}\n:::{.callout-important}\n## **The Multiplication Rule**\n::: {style=\"font-size: 30px\"}\n$\\mathbb{P}(E \\cap F) = \\mathbb{P}(E \\mid F) \\cdot \\mathbb{P}(F)$, for any events $E$ and $F$ with $\\mathbb{P}(F) \\neq 0$.\n:::\n:::\n:::\n\n## Leadup\n\n- Note that \"$E$ and $F$\" is the same as \"$F$ and $E$\".\n- That is: $\\mathbb{P}(E \\cap F) = \\mathbb{P}(F \\cap E)$.\n- So, if we interchange the place of $E$ and $F$ in the multiplication rule, we obtain\n$$ \\mathbb{P}(E \\cap F) = \\mathbb{P}(F \\cap E) = \\mathbb{P}(F \\mid E) \\cdot \\mathbb{P}(E) $$\n\n- That is to say, we have\n\\begin{align*}\n  \\mathbb{P}(E \\cap F)    & = \\mathbb{P}(E \\mid F) \\cdot \\mathbb{P}(F) = \\mathbb{P}(F \\mid E) \\cdot \\mathbb{P}(E)\n\\end{align*}\n\n- Dividing the last equation by $\\mathbb{P}(F)$ yields an important result:\n\n## Bayes' Rule\n\n:::{.fragment}\n:::{.callout-important}\n## **Baeys' Rule**\n::: {style=\"font-size: 30px\"}\n$$ \\mathbb{P}(E \\mid F) = \\frac{\\mathbb{P}(F \\mid E) \\cdot \\mathbb{P}(E)}{\\mathbb{P}(F)} $$\nfor events $E$ and $F$ with $\\mathbb{P}(E) \\neq 0$ and $\\mathbb{P}(F) \\neq 0$.\n:::\n:::\n:::\n\n- In a sense, Bayes' Rule gives us a way to \"reverse the order of a conditional\"\n\n## Worked-Out Example\n\n- As an illustration, let's return to our pineapple-on-pizza contingency table:\n\n:::{.fragment}\n```{r}\n#| echo: false\n\ndf <- matrix(c(\n  rep(c(\"Yes\", \"Freshman\"), 10),\n  rep(c(\"Yes\", \"Sophomore\"), 13),\n  rep(c(\"Yes\", \"Junior\"), 7),\n  rep(c(\"Yes\", \"Senior\"), 5),\n  rep(c(\"No\", \"Freshman\"), 15),\n  rep(c(\"No\", \"Sophomore\"), 18),\n  rep(c(\"No\", \"Junior\"), 6),\n  rep(c(\"No\", \"Senior\"), 1)\n), ncol = 2, byrow = T)\ncolnames(df) <- c(\"Pineapple\", \"Standing\")\ndf <- data.frame(df)\ndf$Pineapple <- factor(df$Pineapple)\ndf$Standing <- factor(df$Standing)\n\ntable(df)\n```\n:::\n\n- Letting $P$ and $F$ be defined as before, let's compute $\\mathbb{P}(P \\mid F)$ using Bayes' Rule.\n\n- We need to first compute $\\mathbb{P}(F \\mid P)$, which we see to be\n$$ \\mathbb{P}(F \\mid P) = \\frac{10}{10 + 7 + 5 + 13} = \\frac{10}{35} $$\n\n---\n\n:::{.nonincremental}\n- Additionally, we find\n\\begin{align*}\n\\mathbb{P}(P)     & = \\frac{10 + 7 + 5 + 13}{75} = \\frac{35}{75}    \\\\\n\\mathbb{P}(F)     & = \\frac{15 + 10}{75} = \\frac{25}{75} \n\\end{align*}\n:::\n\n- Hence, by Bayes' Rule,\n\\begin{align*}\n  \\mathbb{P}(P \\mid F)    & = \\frac{\\mathbb{P}(F \\mid P) \\cdot \\mathbb{P}(P)}{\\mathbb{P}(F)}   \\\\\n    & = \\frac{\\left( \\frac{10}{35} \\right) \\cdot \\left( \\frac{35}{75} \\right) }{ \\left( \\frac{25}{75} \\right) } = \\frac{10}{75} \\cdot \\frac{75}{25} = \\frac{10}{25} = \\boxed{\\frac{2}{5} = 40\\%}\n\\end{align*}\n\n## Hang In There!\n\n- At the moment, it may not seem obvious why Bayes' Rule is helpful.\n    - It seems like it just makes more work!\n\n- But, rest assured, we will see a very practical application of Bayes' Rule in a few slides.\n\n- Before we do, there's just one more concept we need to discuss.\n\n## Leadup {style=\"font-size:30px\"}\n\n- Consider an event $F$, and another event $E$.\n- If $F$ happened, it could have happened along with $E$ or it could have happened along with not-$E$.\n- That is,\n$$ F = (F \\cap E) \\cup (F \\cap E^\\complement)$$\n- Now, let's take the probability of both sides. Since the events on the RHS are disjoint, the probability on the RHS just becomes a sum of probabilities:\n$$ \\mathbb{P}(F) = \\mathbb{P}(F \\cap E) + \\mathbb{P}(F \\cap E^\\complement) $$\n- Finally, we apply the Multiplication Rule to the probabilities on the RHS to obtain\n$$ \\mathbb{P}(F) = \\mathbb{P}(F \\mid E) \\cdot \\mathbb{P}(E) + \\mathbb{P}(F \\mid E^\\complement) \\cdot \\mathbb{P}(E^\\complement) $$\n\n## Law of Total Probability\n\n:::{.fragment}\n:::{.callout-important}\n## **The Law of Total Probability**\n::: {style=\"font-size: 30px\"}\nGiven two events $E$ and $F$ with $\\mathbb{P}(E) \\neq 0$ and $\\mathbb{P}(F) \\neq 0$, we have\n$$ \\mathbb{P}(F) = \\mathbb{P}(F \\mid E) \\cdot \\mathbb{P}(E) + \\mathbb{P}(F \\mid E^\\complement) \\cdot \\mathbb{P}(E^\\complement) $$\n:::\n:::\n:::\n\n- This is often useful in the context of a Bayes' Rule problem.\n\n## Worked-Out Example\n\n- Alright, let's get down to business and tackle a slightly more real-world problem.\n\n:::{.fragment}\n::: callout-tip\n## Worked-Out Exercise 1\n\n::: {style=\"font-size: 30px\"}\nIt is known that a particular disease affects 5\\% of the population. There exists a test for this disease, but it is not perfect: there is a 10\\% chance it will return a \"negative\" result for a person who is actually infected, and there is a 8\\% chance it will return a \"positive\" result for a person who is actually healthy. \\\n\nArasha has taken a test for the disease, and it has indicated a \"positive\" result. What is the probability that Arasha actually has the disease?\n:::\n:::\n:::\n\n## Step 1: Define Events\n\n- As always, we start by defining events.\n\n- Let + denote \"the test returns a positive result\" and let $D$ denote \"Arasha actually has the disease.\" \n\n- First of all, note that we are **not** interested in simply finding $\\mathbb{P}(D)$; rather, we are interested in finding $\\mathbb{P}(D \\mid +)$.\n    - This is because Arasha has already been tested and received a positive result; this is information we need to incorporate into our beliefs!\n    \n## Step 2: Translate the Information {style=\"font-size:30px\"}\n\n- With our events from Step 1, we now turn our attention to translating the information provided in the problem.\n\n- Since there is a $10\\%$ chance that the test returns a negative result **given** that a person actually has the disease, we have\n$$ \\mathbb{P}(+^\\complement \\mid D) = 0.1 $$\n\n- Additionally, we are told that there is an $8\\%$ chance that the test returns a positive result **given** that a person does not have disease, we have\n$$ \\mathbb{P}(+ \\mid D^\\complement) = 0.08 $$\n\n- Finally, we are told that 5\\% of the population has the disease; hence,\n$$ \\mathbb{P}(D) = 0.05 $$\n\n\n## Step 3: Translate the Information {style=\"font-size:30px\"}\n\n- But wait- there's more!\n\n- Given that a person *has* the disease, they will either test positive or test negative. \n\n    - What that means is that \n    $$ \\mathbb{P}(+ \\mid D) = 1 - \\mathbb{P}(+^\\complement \\mid D) = 1 - 0.1 = 0.9 $$\n    \n    - Think of this as a modified complement rule\n\n- Similarly,\n$$ \\mathbb{P}(+^\\complement \\mid D^\\complement) = 1 - \\mathbb{P}(+ \\mid D^\\complement) = 1 - 0.08 = 0.92 $$\n\n- Additionally,\n$$ \\mathbb{P}(D^\\complement) =  1 - \\mathbb{P}(D) = 1 - 0.05 = 0.95 $$\n\n## Step 2: Translate the Information {style=\"font-size:30px\"}\n\n- So, here's a summary of everything we know, just from the problem statement:\n\\begin{align*}\n  \\mathbb{P}(+ \\mid D) = 0.9    & \\hspace{15mm} \\mathbb{P}(+^\\complement \\mid D) = 0.1    \\\\\n  \\mathbb{P}(+ \\mid D^\\complement) = 0.08   & \\hspace{15mm}  \\mathbb{P}(P^\\complement \\mid D^\\complement) = 0.92   \\\\\n  \\mathbb{P}(D) = 0.05    & \\hspace{15mm}  \\mathbb{P}(D^\\complement) = 0.95\n\\end{align*}\n\n## Step 3: Solve the Problem\n\n- Now we are in a position to begin solving the problem.\n- Recall that we seek $\\mathbb{P}(D \\mid +)$.\n- But, we only have information on $\\mathbb{P}(+ \\mid D)$.\n- Any ideas what rule/tool we should use?\n    - That's right; **Bayes' Rule!**\n\n- We use Bayes' Rule to write\n$$ \\mathbb{P}(D \\mid +) = \\frac{\\mathbb{P}(+ \\mid D) \\cdot \\mathbb{P}(D)}{\\mathbb{P}(+)} $$\n\n## Step 3: Solve the Problem\n\n- Do we have $\\mathbb{P}(+)?$\n    - No...\n    - But how can we get it?\n    - Yup- Law of Total Probability!\n\n- We use the **Law of Total Probability** to write\n\n:::{.fragment style=\"text-align:center\"}\n\\begin{align*}\n  \\mathbb{P}(+)   & = \\mathbb{P}(+ \\mid D) \\cdot \\mathbb{P}(D) + \\mathbb{P}(+ \\mid D^\\complement) \\cdot \\mathbb{P}(D^\\complement)    \\\\\n    & = (0.9) \\cdot (0.05) + (0.08) \\cdot (0.95) = 0.121\n\\end{align*}\n:::\n\n\n## Step 3: Solve the Problem\n\n- Finally, putting everything together:\n\n:::{.fragment style=\"text-align:center\"}\n\\begin{align*}\n  \\mathbb{P}(D \\mid +)    & = \\frac{\\mathbb{P}(+ \\mid D) \\cdot \\mathbb{P}(D)}{\\mathbb{P}(+)}   \\\\\n    & = \\frac{(0.9) \\cdot (0.05)}{0.121} \\boxed{\\approx 37.19\\%}\n\\end{align*}\n:::\n\n- If that seems low... you're right! But, it is actually in line with the problem- the test for the disease is pretty bad, considering how often it gets things wrong. This is why this probability is low- because the test is so bad, we cannot be confident that Arasha actually has the disease, even though she tested positive!\n\n## Some Terminology {style=\"font-size:30px\"}\n\n- By the way, there's some terminology I'd like to quickly introduce to make our lives easier going forward.\n\n- The **False Positive Rate** of a test is the proportion of times it returns a \"positive\" result, when the truth is actually \"negative\".\n    - So, in the context of epidemiology, the false positive rate of a test is the proportion of times it says someone has a disease when they do *not* actually have the disease.\n  \n- Analogously, the **False Negative Rate** of a test is the proportion of times it returns a \"negative\" result, when the truth is actually \"positive\".\n    - So, in the context of epidemiology, the false negative rate of a test is the proportion of times it says someone does not have a disease when they *do* actually have the disease.\n    \n## A Preview of HW\n\n- There is one very important topic which I decided to put on your homework, so as to not make today's lecture denser than it already is.\n    - It is called \"Independence\", and is a crucial part of probability!\n\n- Speaking of the next homework- please remember that there **will** be a Homework 3 released tomorrow (Wednesday 4/19) and will be due **MONDAY** by 11:59pm. \n    - It will contain some review problems as well!\n\n## Some Problems To Think On:\n\n::: callout-tip\n## Exercise 1\n\n::: {style=\"font-size: 30px\"}\n:::{.nonincremental}\nA recent survey interviewed several UCSB students about their pets. The following data was collected:\n\n```{r}\n#| echo: false\n\ndf <- matrix(c(\n  rep(c(\"Adopted\", \"Cat\"), 5),\n  rep(c(\"Adopted\", \"Dog\"), 8),\n  rep(c(\"Adopted\", \"Bunny\"), 3),\n  rep(c(\"Adopted\", \"Hamster\"), 4),\n  rep(c(\"Not Adopted\", \"Cat\"), 5),\n  rep(c(\"Not Adopted\", \"Dog\"), 7),\n  rep(c(\"Not Adopted\", \"Bunny\"), 1),\n  rep(c(\"Not Adopted\", \"Hamster\"), 7)\n), ncol = 2, byrow = T)\ncolnames(df) <- c(\"Adopted\", \"Animal\")\ndf <- data.frame(df)\ndf$Adopted <- factor(df$Adopted)\ndf$Animal <- factor(df$Animal)\n\ntable(df)\n```\n\na) If a pet is to be selected at random, what is the probability that it is either a cat or adopted?\n\nb) A pet is selected at random: what is the probability that it is an adopted dog?\n\nc) A pet is selected at random: if it is a dog, what is the probability that it was adopted?\n:::\n:::\n:::\n\n---\n\n::: callout-tip\n## Exercise 2\n\n::: {style=\"font-size: 30px\"}\n:::{.nonincremental}\nTwo events $A$ and $B$ are such that $\\mathbb{P}(A) = 5/10$, \\ $\\mathbb{P}(B) = 6/10$, \\ and $\\mathbb{P}(B \\mid A) = 2/5$. \n\na. Find $\\mathbb{P}(A \\cup B)$\nb. Find $\\mathbb{P}(A \\mid B)$.\n:::\n:::\n:::"},"formats":{"revealjs":{"execute":{"fig-width":10,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":false,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","html-math-method":"mathjax","slide-level":2,"to":"revealjs","incremental":true,"output-file":"Lec05.html"},"language":{},"metadata":{"lang":"en","fig-responsive":false,"quarto-version":"1.2.335","auto-stretch":true,"title":"PSTAT 5A: Lecture 05","subtitle":"Conditional Probability","author":"Ethan P. Marzban","date":"04/18/23","editor":"source","title-slide-attributes":{"data-background-image":"5a_hex.png","data-background-size":"contain","data-background-opacity":"0.5","data-background-position":"left"},"theme":["default","custom.scss"],"logo":"5a_hex.png","template-partials":["title-slide.html"]}}}}