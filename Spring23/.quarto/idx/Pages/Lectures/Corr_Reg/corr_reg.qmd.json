{"title":"PSTAT 5A: Lecture 20","markdown":{"yaml":{"title":"PSTAT 5A: Lecture 20","subtitle":"Post-MT2 Review, Part II","author":"Ethan P. Marzban","date":"6/12/23","format":{"revealjs":{"html-math-method":"mathjax","theme":["default","custom.scss"],"incremental":true,"logo":"5a_hex.png","template-partials":["title-slide.html"]}},"editor":"source","title-slide-attributes":{"data-background-image":"5a_hex.png","data-background-size":"contain","data-background-opacity":"0.5","data-background-position":"left"}},"headingText":"Correlation and Regression","containsRefs":false,"markdown":"\n\n```{r setup, include=FALSE}\nlibrary(tidyverse)\nlibrary(ggthemes)\n```\n\n\n## Associations and Correlations\n\n- Recall, from Week 1, that a **scatterplot** is a good way to visualize the relationship between two numerical variables `x` and `y`.\n\n- Two variables can have either a **positive** or a **negative** relationship/association, along with a **linear** or **nonlinear** one.\n\n    - \"Positive\" means a one-unit increase in `x` translates to an increase in `y`\n    - \"Negative\" means a one-unit increase in `x` translates to an degrease in `y`\n    - \"Linear\" means the rate of change is fixed (i.e. constant)\n    - \"Nonlinear\" means the rate of change depends on `x`\n    \n\n---\n\n:::: {.columns}\n\n:::{.column width=\"50%\"} \n- Linear **Negative** Association:\n\n:::{.fragment}\n```{r}\nset.seed(123)\nx <- rnorm(100)\ny <- -2 * x + rnorm(100, 0, 2)\n\ndata.frame(x, y) %>%\n  ggplot(aes(x = x, y = y)) +\n  geom_point(size = 4) +\n  theme_economist(base_size = 24) +\n  ggtitle(\"Y vs. X\") +\n  theme(panel.background = element_rect(\"#f0ebd8\"),\n        plot.background = element_rect(fill = \"#f0ebd8\"),\n        axis.title.y = element_text(size = 16,\n                                    margin = margin(\n                                      t = 0, \n                                      r = 10,\n                                      b = 0, \n                                      l = 0)),\n        axis.title.x = element_text(size = 16,\n                                    margin = margin(\n                                      t = 10, \n                                      r = 0,\n                                      b = 0, \n                                      l = 0)),\n        title = element_text(size = 18)\n  )\n```\n:::\n:::\n\n:::{.column width=\"50%\"}\n- Nonlinear **Negative** Association:\n\n:::{.fragment}\n```{r}\nset.seed(123)\nx <- rchisq(100, 20)\ny <- (1 / x^2) + rnorm(100, 0, 0.0005)\n\ndata.frame(x, y) %>%\n  ggplot(aes(x = x, y = y)) +\n  geom_point(size = 4) +\n  theme_economist(base_size = 24) +\n  ggtitle(\"Y vs. X\") +\n  theme(panel.background = element_rect(\"#f0ebd8\"),\n        plot.background = element_rect(fill = \"#f0ebd8\"),\n        axis.title.y = element_text(size = 16,\n                                    margin = margin(\n                                      t = 0, \n                                      r = 10,\n                                      b = 0, \n                                      l = 0)),\n        axis.title.x = element_text(size = 16,\n                                    margin = margin(\n                                      t = 10, \n                                      r = 0,\n                                      b = 0, \n                                      l = 0)),\n        title = element_text(size = 18)\n  )\n```\n:::\n:::\n\n::::\n\n\n:::: {.columns}\n\n:::{.column width=\"50%\"} \n- Linear **Positive** Association:\n\n:::{.fragment}\n```{r}\nset.seed(123)\nx <- rnorm(100)\ny <- 2 * x + rnorm(100, 0, 2)\n\ndata.frame(x, y) %>%\n  ggplot(aes(x = x, y = y)) +\n  geom_point(size = 4) +\n  theme_economist(base_size = 24) +\n  ggtitle(\"Y vs. X\") +\n  theme(panel.background = element_rect(\"#f0ebd8\"),\n        plot.background = element_rect(fill = \"#f0ebd8\"),\n        axis.title.y = element_text(size = 16,\n                                    margin = margin(\n                                      t = 0, \n                                      r = 10,\n                                      b = 0, \n                                      l = 0)),\n        axis.title.x = element_text(size = 16,\n                                    margin = margin(\n                                      t = 10, \n                                      r = 0,\n                                      b = 0, \n                                      l = 0)),\n        title = element_text(size = 18)\n  )\n```\n:::\n:::\n\n:::{.column width=\"50%\"} \n- Nonlinear **Positive** Association:\n\n:::{.fragment}\n```{r}\nset.seed(123)\nx <- rchisq(100, 20)\ny <- 1 - (1/x^2)  + rnorm(100, 0, 0.0007)\n\ndata.frame(x, y) %>%\n  ggplot(aes(x = x, y = y)) +\n  geom_point(size = 4) +\n  theme_economist(base_size = 24) +\n  ggtitle(\"Y vs. X\") +\n  theme(panel.background = element_rect(\"#f0ebd8\"),\n        plot.background = element_rect(fill = \"#f0ebd8\"),\n        axis.title.y = element_text(size = 16,\n                                    margin = margin(\n                                      t = 0, \n                                      r = 10,\n                                      b = 0, \n                                      l = 0)),\n        axis.title.x = element_text(size = 16,\n                                    margin = margin(\n                                      t = 10, \n                                      r = 0,\n                                      b = 0, \n                                      l = 0)),\n        title = element_text(size = 18)\n  )\n```\n:::\n:::\n\n::::\n\n## No Relationship\n\n- Sometimes, two variables will have no relationship at all:\n\n:::{.fragment}\n```{r}\nset.seed(123)\nx <- rchisq(100, 20)\ny <- rchisq(100, 20)\n\ndata.frame(x, y) %>%\n  ggplot(aes(x = x, y = y)) +\n  geom_point(size = 4) +\n  theme_economist(base_size = 24) +\n  ggtitle(\"Y vs. X\") +\n  theme(panel.background = element_rect(\"#f0ebd8\"),\n        plot.background = element_rect(fill = \"#f0ebd8\"),\n        axis.title.y = element_text(size = 16,\n                                    margin = margin(\n                                      t = 0, \n                                      r = 10,\n                                      b = 0, \n                                      l = 0)),\n        axis.title.x = element_text(size = 16,\n                                    margin = margin(\n                                      t = 10, \n                                      r = 0,\n                                      b = 0, \n                                      l = 0)),\n        title = element_text(size = 18)\n  )\n```\n:::\n\n## Strength of a Relationship\n\n\\\n \n:::: {.columns}\n\n:::{.column width=\"50%\"} \n:::{.fragment}\n```{r}\nset.seed(123)\nx <- rnorm(100)\ny <- 2 * x + rnorm(100, 0, 2.1)\n\ndata.frame(x, y) %>%\n  ggplot(aes(x = x, y = y)) +\n  geom_point(size = 4) +\n  theme_economist(base_size = 24) +\n  ggtitle(\"Y1 vs. X1\") +\n  theme(panel.background = element_rect(\"#f0ebd8\"),\n        plot.background = element_rect(fill = \"#f0ebd8\"),\n        axis.title.y = element_text(size = 16,\n                                    margin = margin(\n                                      t = 0, \n                                      r = 10,\n                                      b = 0, \n                                      l = 0)),\n        axis.title.x = element_text(size = 16,\n                                    margin = margin(\n                                      t = 10, \n                                      r = 0,\n                                      b = 0, \n                                      l = 0)),\n        title = element_text(size = 18)\n  )\n```\n:::\n:::\n\n:::{.column width=\"50%\"} \n:::{.fragment}\n```{r}\nset.seed(123)\nx <- rnorm(100)\ny <- 2 * x + rnorm(100, 0, 0.5)\n\ndata.frame(x, y) %>%\n  ggplot(aes(x = x, y = y)) +\n  geom_point(size = 4) +\n  theme_economist(base_size = 24) +\n  ggtitle(\"Y2 vs. X2\") +\n  theme(panel.background = element_rect(\"#f0ebd8\"),\n        plot.background = element_rect(fill = \"#f0ebd8\"),\n        axis.title.y = element_text(size = 16,\n                                    margin = margin(\n                                      t = 0, \n                                      r = 10,\n                                      b = 0, \n                                      l = 0)),\n        axis.title.x = element_text(size = 16,\n                                    margin = margin(\n                                      t = 10, \n                                      r = 0,\n                                      b = 0, \n                                      l = 0)),\n        title = element_text(size = 18)\n  )\n```\n:::\n:::\n\n::::\n\n## Pearson's _r_\n\n- **Pearson's _r_** (or just the **correlation coefficient**) is a metric used to quantify the strength and direction of a linear relationship between two variables.\n\n- Given variables `x` and `y` (whose elements are denoted using the familiar notation we've been using throughout this course), we compute _r_ using\n$$ r = \\frac{1}{n - 1} \\sum_{i=1}^{n} \\left( \\frac{x_i - \\overline{x}}{s_X} \\right) \\left( \\frac{y_i - \\overline{y}}{s_Y} \\right)  $$\n\n- Recall that $-1 \\leq r \\leq 1$ for any two variables `x` and `y`.\n\n    - Furthermore, `r` will only ever be $-1$ or $1$ exactly when the points in the scatterplot fall perfectly on a line.\n    \n## Regression\n\n- We may also want to *model* the relationship between `x` and `y`.\n\n- Specifically, given a **respone variable** `y` and an **explanatory variable** `x`, we typically assume `x` and `y` are related through the equation\n$$ \\texttt{y} = f(\\texttt{x}) + \\texttt{noise} $$\nwhere $f$ is some function.\n\n    - By the way: on a scatterplot, the response variable will always appear on the vertical axis and the explanatory variable will appear on the horizontal axis.\n\n- Of particular interest to us in this class is when $f$ takes the form of a linear equation: i.e. when our model is of the form\n$$ \\texttt{y} = \\beta_0 + \\beta_1 \\cdot \\texttt{x} + \\texttt{noise} $$\n\n## Regression\n\n- Now, the noise part of our model makes it impossible to know the true values of $\\beta_0$ and $\\beta_1$.\n\n    - In this way, we can think of them as population parameters.\n    \n- As such, we seek to find point estimators $\\widehat{\\beta_0}$ and $\\widehat{\\beta_1}$ that best estimate $\\beta_0$ and $\\beta_1$, respectively.\n\n- To quantify what we mean by \"best\", we employed the condition of minimizing the **residual sum of squares**.\n\n    - Effectively, this means finding the line $\\widehat{\\beta_0} + \\widehat{\\beta_1} \\cdot \\texttt{x}$ that minimizes the average distance between the points in the dataset and the line.\n    \n## Regression\n\n![](rss.svg){width=\"80%\"}\n\n## Regression {style=\"font-size:30px\"}\n\n- Such estimators (i.e. those that minimize the RSS) are said to be **ordinary least squares** (OLS) estimates.\n\n  - The resulting line $\\widehat{\\beta_0} + \\widehat{\\beta_1} \\cdot \\texttt{x}$ is thus called the **OLS Regression Line**\n  \n- It turns out that the OLS estimates of $\\beta_0$ and $\\beta_1$ are:\n\\begin{align*}\n  \\widehat{\\beta_1}   & = \\frac{\\sum_{i=1}^{n} (x_i - \\overline{x})(y_i - \\overline{y})}{\\sum_{i=1}^{n} (x - \\overline{x})^2} = \\frac{s_Y}{s_X} \\cdot r \\\\\n  \\widehat{\\beta_0}   & = \\overline{y} - \\widehat{\\beta_1} \\cdot \\overline{x} \n\\end{align*}\nwhere _r_ denotes **Pearson's Correlation Coefficient**\n$$ r = \\frac{1}{n - 1} \\sum_{i=1}^{n} \\left( \\frac{x_i - \\overline{x}}{s_X} \\right) \\left( \\frac{y_i - \\overline{y}}{s_Y} \\right)  $$\n\n## Regression {style=\"font-size:32px\"}\n\n- The values along the OLS regression line corresponding to `x` values observed in the dataset are called **fitted values**: \n\n:::{.fragment}\n![](fitted.svg){width=\"50%\"}\n:::\n\n- In a sense, the fitted values represent guess/estimate of the *de-noised* value of `y`\n\n## Regression {style=\"font-size:32px\"}\n\n- We can use the OLS regression line to perform **prediction**; i.e. to infer response values associated with explanatory values that were not included in the original dataset.\n\n## Example\n\n::: callout-tip\n## Worked-Out Example 3 (formerly a \"Your Turn\" Exercise)\n\n:::{.nonincremental}\n::: {style=\"font-size: 24px\"}\nAn airline is interested in determining the relationship between flight duration (in minutes) and the net amount of soda consumed (in oz.). Letting `x` denote `flight duration` (the explanatory variable) and `y` denote `amount of soda consumed` (the response variable), a sample of size 100 yielded the following results:\n$$ \\begin{array}{cc}\n  \\displaystyle \\sum_{i=1}^{100} x_i  = 10,\\!211.7;   & \\displaystyle \\sum_{i=1}^{100} (x_i - \\overline{x})^2 =  38,\\!760.68    \\\\\n  \\displaystyle \\sum_{i=1}^{100} y_i  = 14,\\!3995.8;   & \\displaystyle \\sum_{i=1}^{100} (y_i - \\overline{y})^2 =  87.23984   \\\\\n \\displaystyle \\sum_{i=1}^{100} (x_i - \\overline{x})(y_i - \\overline{y}) = 379.945 \\\\\n\\end{array} $$\n\na. Find the equation of the OLS Regression line.\nb. If a particular flight has a duration of 110 minutes, how many ounces of soda would we expect to be consumed on the flight?\n:::\n:::\n:::\n\n## Solutions\n\na. \n\\begin{align*}\n  \\widehat{\\beta_0}   & = \\frac{\\sum_{i=1}^{n}(x_i - \\overline{x})(y_i - \\overline{y})} {\\sum_{i=1}^{n} (x_i - \\overline{x})^2} = \\frac{379.945}{38,\\!760.68} \\approx \\boxed{0.0098}   \\\\\n  \\widehat{\\beta_0}   & = \\overline{y} - \\widehat{\\beta_1} \\cdot \\overline{x} = \\boxed{1438.957}\n\\end{align*}\nTherefore,\n$$ \\widehat{(\\texttt{amt. of soda})} = 1436.159 + (0.0098) \\cdot (\\texttt{flight duration}) $$\n\n\\\n\nb. $\\widehat{y}^{(110)} = 1438.957 + (0.0098)(110) = \\boxed{1440.035 \\text{ oz.}}$\n\n## Extrapolation\n\n- Remember that it is dangerous to try and use the OLS regression line to predict response values for explanatory variables that are far outside of the scope of the original data.\n\n- For example, if the dataset in the previous example only included flights between 100 minutes and 230 minutes, it would be dangerous to try to predict the amount of soda that would be consumed on a 13-hr flight (780 mins) using the OLS regression line, as we cannot be certain that the relationship between `amt. of soda` and `flight duration` remains linear for larger values of `flight duration.`\n\n- Recall that this relates to **extrapolation**.\n\n## Inference on the Slope {style=\"font-size:30px\"}\n\n- We also talked about how we can perform inference on the slope $\\beta_1$ of the OLS regression line.\n\n- Specifically, we may want to test\n$$ \\left[ \\begin{array}{rl}\n  H_0:    & \\beta_1 = 0   \\\\\n  H_A:    & \\beta_1 \\neq 0\n\\end{array} \\right. $$\n\n    - The reason we want to test this is that, if we have reason to believe that $\\beta_1$ *could* be zero, then there might not be a linear relationship between `y` and `x` at all!\n\n- Under normality conditions,\n$$ \\frac{\\widehat{\\beta_1} - \\beta_1}{\\mathrm{SD}(\\widehat{\\beta_1})} \\stackrel{H_0}{\\sim} t_{n - 2} $$\n\n\n\n## Example\n\n::: callout-tip\n## Worked-Out Example 4\n\n:::{.nonincremental}\n::: {style=\"font-size: 24px\"}\nThe results of regressing a variable `y` onto another variable `x` are shown below:\n\n| | Estimate | Std. Error | _t_-value | Pr(>\\|t\\|) |\n|----:|:-----:|:-----:|:-----:|:-----:|\n| **Intercept** | -0.05185   |   0.24779 |  -0.209  |  0.836   \n| **Slope** | 0.08783   |   0.07869 |  1.116 | 0.272\n\nIs it possible that there exists no linear relationship between `y` and `x`? (Use a 5\\% level of significance wherever necessary.) Explain.\n:::\n:::\n:::\n\na. Since the _p_value of testing $H_0: \\beta_1 = 0$ vs $H_A: \\beta_1 \\neq 0$ is $0.272$, which is greater than a significance level of 5\\%, we would fail to reject the null; that is, it **is** possible that there exists no linear relationship between `y` and `x`.\n\n# Sampling, and the Structure of Studies\n\n## Sampling Procedures {style=\"font-size:28px\"}\n\n- Finally, last lecture, we returned to the basics- data!\n\n- Specifically, we discussed different ways data can be collected; i.e. the different **sampling procedures** that are available to us.\n\n- In a **simple random sample**, every individual in the population has an equal chance of being included in the sample.\n\n    - This can sometimes be costly, or even lead to biased samples.\n    \n- In a **stratified sampling** scheme, the population is first divided into several *strata* (groups), and an SRS is taken from each stratum.\n\n    - This has the benefit of creating a potentially more representative sample, though can still be quite costly. Results are also heavily dependent on the strata that were created.\n    \n- A **cluster sampling** scheme again divides the population into groups (now called *clusters*), takes an SRS of clusters, and then takes an SRS from the selected clusters.\n\n    - This has the benefit of being (potentially) cheaper, but can again lead to biased samples and is also heavily dependent on the clusters that were created.\n    \n    \n## Sampling Procedures\n\n- A **convenience sample** is one in which individuals are included (or excluded) from the sample based on convenience; e.g. people who are nearby (geographically) are included whereas people who are farther away are not.\n\n    - Convenience Samples are cheap and, well, convenient, but can lead to very skewed or biased results. \n    \n- Speaking of bias, there was another form of bias we discussed: **non-response bias**. \n\n    - This occurs when certain individuals (or potentially even demographics, genders, etc.) do not participate in a survey, despite having been included in the sample of surveyed individuals.\n    \n## Other Distinctions\n\n- In an **observational study**, treatment is neither administered nor withheld from subjects.\n\n- In an **experiment**, treatment *is* administered (or possibly withheld) from subjects.\n\n- In a **longitudinal study**, subjects are tracked over a period of time. (Observations are therefore **correlated**)\n\n- In a **cross-sectional study**, there is no tracking of subjects over time.\n\n\n\n\n## Example\n:::{.fragment}\n::: callout-tip\n## Example (1.20 from *OpenIntro*)\n\n:::{.nonincremental}\n::: {style=\"font-size: 28px\"}\nOn a large college campus first-year students and sophomores live in dorms located on the eastern part of the campus and juniors and seniors live in dorms located on the western part of the campus. Suppose you want to collect student opinions on a new housing structure the college administration is proposing and you want to make sure your survey equally represents opinions from students from all years.\n\n(a) What type of study is this?\n(b) Suggest a sampling strategy for carrying out this study.\n:::\n:::\n:::\n:::\n\n\n## Solutions\n\na. Treatment has neither been administered nor withheld, meaning this is an **observational study**.\n\n\\\n\nb. **Stratified sampling** seems like the way to go, with `western campus` and `eastern campus` being the two strata. \n\n    - Specifically, we should take an SRS from both `western campus` and `eastern campus` students, to ensure that students across all years are (somewhat) equally represented."},"formats":{"revealjs":{"execute":{"fig-width":10,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":false,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","html-math-method":"mathjax","slide-level":2,"to":"revealjs","incremental":true,"output-file":"corr_reg.html"},"language":{},"metadata":{"lang":"en","fig-responsive":false,"quarto-version":"1.2.335","auto-stretch":true,"title":"PSTAT 5A: Lecture 20","subtitle":"Post-MT2 Review, Part II","author":"Ethan P. Marzban","date":"6/12/23","editor":"source","title-slide-attributes":{"data-background-image":"5a_hex.png","data-background-size":"contain","data-background-opacity":"0.5","data-background-position":"left"},"theme":["default","custom.scss"],"logo":"5a_hex.png","template-partials":["title-slide.html"]}}}}