{"title":"PSTAT 5A: Lecture 14","markdown":{"yaml":{"title":"PSTAT 5A: Lecture 14","subtitle":"Review for Midterm 2","author":"Ethan P. Marzban","date":"5/18/23","format":{"revealjs":{"html-math-method":"mathjax","theme":["default","custom.scss"],"incremental":true,"logo":"5a_hex.png","template-partials":["title-slide.html"]}},"editor":"source","title-slide-attributes":{"data-background-image":"5a_hex.png","data-background-size":"contain","data-background-opacity":"0.5","data-background-position":"left"}},"headingText":"Hypothesis Testing","containsRefs":false,"markdown":"\n\n```{r setup, include=FALSE}\nlibrary(tidyverse)\nlibrary(ggthemes)\n```\n\n\n## Hypothesis Testing {style=\"font-size:32px\"}\n\n- Recall that in the framework of **hypothesis testing**, we wish to utilize *data* to assess the plausibility/validity of a **hypothesis**, called the **null hypothesis**.\n\n    - Specifically, we wish to determine whether or not the data provides support to reject the null in favor of the **alternative hypothesis** or not.\n\n- In the case of hypothesis testing for a population proportion $p$, our null takes the form $H_0: p = p_0$ and there are several different alternative hypotheses we could consider:\n\n    - **Two-tailed alternative/test:** $H_A: \\ p \\neq p_0$\n    - **Lower-tailed alternative/test:** $H_A: \\ p < p_0$\n    - **Upper-tailed alternative/test:** $H_A: \\ p > p_0$\n    - **Simple-vs-simple alternative/test:** $H_A: \\ p = p_1$ for $p_1 \\neq p_0$\n\n## Hypothesis Testing for a Proportion {style=\"font-size:32px\"}\n\n- The different alternative hypotheses lead to different forms of our **hypothesis test**.\n\n- Last lecture, we discussed how to construct a two-sided hypothesis test.\n\n- On the homework, I ask you to derive the lower-tailed hypothesis test, and I also provide you with the form for an upper-tailed hypothesis test.\n\n- To help you with Problem 1 on the upcoming homework, allow me to re-do the derivations we did last lecture, but this time more in the style of Problem 1 on HW01.\n\n## Two-Sided Test for a Proportion {style=\"font-size:32px\"}\n\n- We start with the test statistic\n$$ \\mathrm{TS} = \\frac{\\widehat{P} - p_0}{\\sqrt{\\frac{p_0 (1 - p_0)}{n}}} $$\n\n- If the null were true, i.e. if the true value of $p$ were $p_0$, then this test statistic would follow the Normal Distribution provided we are able to invoke the Central Limit Theorem for proportions, which we can do only if\n\n    1) $np_0 \\geq 10$\n    2) $n(1 - p_0) \\geq 10$\n\n## Two-Sided Test for a Proportion {style=\"font-size:32px\"}\n\n- If these conditions hold, we then know that (by the Central Limit Theorem for Proportions) \n$$ \\widehat{P} \\stackrel{H_0}{\\sim} \\mathcal{N}\\left( p_0, \\ \\sqrt{\\frac{p_0 (1 - p_0)}{n}} \\right) $$\nwhich means, by our Standardization Result, that \n$$\\mathrm{TS} \\stackrel{H_0}{\\sim} \\mathcal{N}(0, \\ 1)$$\n\n\n\n## Two-Sided Test for a Proportion {style=\"font-size:30px\"}\n\n- Now, let's think about when we would reject the null that $p = p_0$ in favor of the alternative that $p \\neq p_0$.\n\n    - If we observed a value of $\\widehat{p}$ that was much greater than $p_0$ (i.e. if $\\mathrm{TS}$ was much larger than 0), we would probably reject the null in favor of the alternative.\n    \n    - But, if we observed a value of $\\widehat{p}$ that was much *smaller* than $p_0$ (i.e. if $\\mathrm{TS}$ was much *smaller* than 0), we would probably *also* reject the null in favor of the alternative.\n    \n    - Therefore, we reject when the magnitude of $\\mathrm{TS}$ is large; i.e. with $|\\mathrm{TS}| > c$ for some constant $c$.\n    \n- This means our test should take the form\n$$ \\texttt{decision}(\\mathrm{TS}) = \\begin{cases} \\texttt{reject } H_0 & \\text{if } |\\mathrm{TS}| > c \\\\ \\texttt{fail to reject } H_0 & \\text{otherwise}\\\\ \\end{cases}  $$\n    \n## Two-Sided Test for a Proportion {style=\"font-size:30px\"}\n\n- Finally, we need to consider how large to make our cutoff (or **critical value**) $c$.\n\n- This is where we need to consider the two types of errors we could commit: **Type I** and **Type II Errors**\n\n    - Type I error: the null was rejected when it was in fact true (convicting an innocent person)\n    \n    - Type II error: the null was not rejected when it was in fact false (letting a guilty person go free)\n\n- We call the probability of committing a Type I error the **level of significance** $\\alpha$, which we fix before beginning our testing procedure.\n\n- In terms of the critical value, this means $c$ should satisfy the equation\n$$ \\mathbb{P}_{H_0}(|\\mathrm{TS}| > c) = \\alpha $$\nwhere $\\mathbb{P}_{H_0}$ means \"probability of, assuming the null is actually true (i.e. that $p = p_0$)\".\n\n\n## Two-Sided Test for a Proportion {style=\"font-size:30px\"}\n\n- Since we know that $\\mathrm{TS}$ follows a standard normal distribution under the null, this means that $c$ should be the $(\\alpha / 2) \\times 100$^th^ percentile of the standard normal distribution.\n\n    - For example, if $\\alpha = 0.05$, then the critical value is the $(0.05) / 2 \\times 100 = 2.5$^th^ percentile of the standard normal distribution, which we see to be $1.96$.\n\n- As such, the final form of our test is\n$$ \\texttt{decision}(\\mathrm{TS}) = \\begin{cases} \\texttt{reject } H_0 & \\text{if } |\\mathrm{TS}| > z_{1 - \\alpha/2} \\\\ \\texttt{fail to reject } H_0 & \\text{otherwise}\\\\ \\end{cases}  $$\nwhere we can use the notation $z_{1 - \\alpha/2}$ to denote the $(\\alpha / 2) \\times 100$^th^ percentile of the standard normal distribution, **scaled by negative 1**.\n\n## Quick Note {style=\"font-size:30px\"}\n\n- It turns out that the $(\\alpha / 2) \\times 100$^th^ percentile of the standard normal distribution scaled by negative 1 is equivalent to the $(1 - \\alpha / 2)$^th^ percentile of the standard normal distribution, not scaled by negative 1.\n\n:::{.fragment}\n```{python}\n#| echo: True\nimport scipy.stats as sps\n\n-sps.norm.ppf(0.05/2)\n```\n:::\n\n:::{.fragment}\n```{python}\n#| echo: True\nimport scipy.stats as sps\n\nsps.norm.ppf(1 - 0.05/2)\n```\n:::\n\n- Basically, when doing a two-sided test, ensure that the critical value is positive.\n\n## Worked-Out Example {style=\"font-size:30px\"}\n\n:::{.fragment}\n::: callout-tip\n## Worked-Out Example 1\n\n:::{.nonincremental}\n::: {style=\"font-size: 28px\"}\nAdministration within a Statistics department at an unnamed university claims to admit 24\\% of all applicants. A disgruntled student, dubious of the administration's claims, takes a representative sample of 120 students who applied to the Statistics major, and found that 20\\% of these students were actually admitted into the major. \\\n\nConduct a two-sided hypothesis test at a 5\\% level of significance on the administrator's claims that 24\\% of applicants into the Statistics major are admitted. Be sure you phrase your conclusion clearly, and in the context of the problem.\n:::\n:::\n:::\n:::\n\n## Solutions {style=\"font-size:30px\"}\n\n- We first phrase the hypotheses.\n\n- Let $p$ denote the true proportion of applicants who get admitted into the major. Since we are performing a two-sided test, our hypotheses take the form\n$$ \\left[ \\begin{array}{rr}\n  H_0:    p = 0.24    \\\\\n  H_A:    p \\neq 0.24\n\\end{array} \\right.$$\n\n- Now we compute the value of the test statistic:\n$$ \\mathrm{TS} = \\frac{\\widehat{p} - p_0}{\\sqrt{\\frac{p_0(1 - p_0)}{n}}} = \\frac{0.2 - 0.24}{\\sqrt{\\frac{(0.24) \\cdot (1 - 0.24)}{120}}} \\approx -1.026 $$\n\n- Next, we compute the critical value. Since we are using an $\\alpha = 0.05$ level of significance, we will use the critical value $1.96$\n\n## Solutions {style=\"font-size:30px\"}\n\n- Finally, we perform the test: we will reject the null in favor of the alternative if $|\\mathrm{TS}|$ is larger than the critical value.\n\n- In this case, $|\\mathrm{TS}| = |-1.026| = 1.026 < 1.96$ meaning we fail to reject the null:\n\n:::{.fragment}\n> At an $\\alpha = 0.05$ level of significance, there was insufficient evidence to reject the null hypothesis that 24\\% of applicants are admitted into the major as opposed to the alternative that the true admittance rate was *not* 24\\%.\n:::\n\n# Random Variables\n\n## Basics of Random Variables {style=\"font-size:32px\"}\n\n- Recall long ago we discussed the notion of an experiment: any procedure we can repeat an infinite number of times where each time we repeat the experiment the same fixed set of things (i.e. the *outcomes*) can occur.\n\n- A **random variable**, loosely speaking, is some sort of numerical variable that keeps track of certain quantities relating to an experiment.\n\n- For example, if we toss 7 coins and let $X$ denote the number of heads we observe in these 7 coin tosses, then $X$ would be a random variable.\n\n- The set of all values a random variable can attain is called the **state space**, and is denoted $S_X$.\n\n- We classify random variables based on their state space:\n    - If $S_X$ has jumps, we say $X$ is a **discrete random variable**\n    - If $S_X$ does not have jumps, we say $X$ is a **continuous random variable**\n\n## Discrete Random Variables {style=\"font-size:30px\"}\n\n- Discrete random variables are described/summarized by a **probability mass function** (p.m.f.), which is a specification of the values the random variable can take (i.e. the state space) along with the probabilities with which the random variable attains those values.\n\n    - P.M.F.'s are often displayed in tabular form: e.g.\n    $$ \\begin{array}{r|cccc}\n      \\boldsymbol{k}              & -1    & 0   & 1   & 2   \\\\\n      \\hline\n      \\boldsymbol{\\mathbb{P}(X = k)}   & 0.1   & 0.2   & 0.3   & 0.4\n    \\end{array}$$\n    \n    - Note that the probability values in a P.M.F. must sum to 1.\n    \n- Quantities like $\\mathbb{P}(X \\leq k)$ are found by summing up the values of $\\mathbb{P}(X = x)$ for all values of $x$ in the state space that are less than $k$.\n\n    - For example, in the example above,\n    $$\\mathbb{P}(X \\leq 0.5) = \\mathbb{P}(X = -1) + \\mathbb{P}(X = 0)  = 0.1 + 0.2 = 0.3 $$\n    \n## Expected Value {style=\"font-size:30px\"}\n\n- The **expected value** of a random variable $X$, denoted $\\mathbb{E}[X]$, represents a sort of \"average\" of $X$, and is computed as\n$$ \\mathbb{E}[X] = \\sum_{\\text{all $k$}} k \\cdot \\mathbb{P}(X = k) $$\n\n    - Again, don't be scared by the sigma notation! It just represents a sum.\n    \n    - So, for example, using our P.M.F. from the previous slide,\n    \\begin{align*}\n      \\mathbb{E}[X]   & = (-1) \\cdot \\mathbb{P}(X = -1) + (0) \\cdot \\mathbb{P}(X = 0)    \\\\\n      & \\hspace{10mm} + (1) \\cdot \\mathbb{P}(X = 1) + (2) \\cdot \\mathbb{P}(X = 2)    \\\\[3mm]\n        & = (-1) \\cdot (0.1) + (0) \\cdot (0.2) + (1) \\cdot (0.3) + (2) \\cdot (0.4)     \\\\[3mm]\n        & = \\boxed{1}\n    \\end{align*}\n    \n## Variance and Standard Deviation {style=\"font-size:30px\"}\n\n- There are two formulas we can use for the **variance** of a random variable $X$:\n$$ \\mathrm{Var}(X) = \\sum_{\\text{all $k$}} (k - \\mathbb{E}[X])^2 \\cdot \\mathbb{P}(X = k) $$\nor\n$$ \\mathrm{Var}(X) = \\left(\\sum_{\\text{all $k$}} k^2 \\cdot \\mathbb{P}(X = k) \\right) - (\\mathbb{E}[X])^2 $$\n\n- The **standard deviation** of a random variable is simply the square root of the variance:\n$$ \\mathrm{SD}(X) = \\sqrt{\\mathrm{Var}(X)} $$\n\n## Variance and Standard Deviation {style=\"font-size:30px\"}\n\n- For example, using the PMF from a few slides ago, the first formula for variance tells us to compute\n\\begin{align*}\n\\mathrm{Var}(X)     & = \\sum_{\\text{all $k$}} (k - \\mathbb{E}[X])^2 \\cdot \\mathbb{P}(X = k)    \\\\\n    & = (-1 - 1)^2 \\cdot \\mathbb{P}(X = -1) + (0 - 1)^2 \\cdot \\mathbb{P}(X = 0)    \\\\\n      & \\hspace{10mm} + (1 - 1)^2 \\cdot \\mathbb{P}(X = 1) + (2 - 1)^2 \\cdot \\mathbb{P}(X = 2)    \\\\[3mm]\n        & = (-1 - 1)^2 \\cdot (0.1) + (0 - 1)^2 \\cdot (0.2)    \\\\\n        & \\hspace{10mm}+ (1 - 1)^2 \\cdot (0.3) + (2 - 1)^2 \\cdot (0.4)   \\\\[3mm]\n        & = \\boxed{1}\n\\end{align*}\n\n## Variance and Standard Deviation {style=\"font-size:30px\"}\n\n:::{.nonincremental}\n- Using the second formula for variance, we first compute\n\\begin{align*}\n\\sum_{\\text{all $k$}} k^2 \\mathbb{P}(X = k)    &  = (-1)^2 \\cdot \\mathbb{P}(X = -1) + (0)^2 \\cdot \\mathbb{P}(X = 0)    \\\\\n      & \\hspace{10mm} + (1)^2 \\cdot \\mathbb{P}(X = 1) + (2 - 1)^2 \\cdot \\mathbb{P}(X = 2)    \\\\[3mm]\n        & = (-1)^2 \\cdot (0.1) + (0)^2 \\cdot (0.2)    \\\\\n        & \\hspace{10mm}+ (1)^2 \\cdot (0.3) + (2)^2 \\cdot (0.4)   \\\\[3mm]\n        & = 2\n\\end{align*}\nwhich means\n$$ \\mathrm{Var}(X) = 2 - (1)^2 = \\boxed{1}$$\n:::\n\n## Binomial Distribution {style=\"font-size:32px\"}\n\n- Suppose we have $n$ independent trials, each resulting in \"success\" with probability $p$ and \"failure\" with probability $1 - p$. If $X$ denotes the number of successes in these $n$ trials, we say $X$ follow the **Binomial distribution** with parameters $n$ and $p$, notated\n$$ X \\sim \\mathrm{Bin}(n, \\ p) $$\n\n- If $X \\sim \\mathrm{Bin}(n, \\ p)$, then:\n    - $S_X = \\{0, 1, 2, \\cdots, n\\}$\n    - $\\mathbb{P}(X = k) = \\binom{n}{k} p^k (1 - p)^{n - k}$\n    - $\\mathbb{E}[X] = np$\n    - $\\mathrm{Var}(X) = np(1 - p)$\n  \n    \n## Binomial Distribution {style=\"font-size:32px\"}\n\n- In order to verify that the Binomial distribution is appropriate to use, we need to check the Binomial Criteria:\n\n    1) Independence across trials\n    2) Fixed number $n$  of trials\n    3) Well-defined notion of \"success\" and \"failure\"\n    4) Fixed probability $p$ of success across trials.\n    \n- If you are going to use the Binomial distribution in a problem, you **must** check **all four** of these!\n\n## Case Study: Airline Bookings\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/ZFNstNKgEDI?start=76\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n\n## Worked-Out Example {style=\"font-size:30px\"}\n\n:::{.fragment}\n::: callout-tip\n## Worked-Out Example 2\n\n:::{.nonincremental}\n::: {style=\"font-size: 30px\"}\nSuppose that *GauchoAir* has found that each passenger that books a ticket on the GA5A flight from SBA to GCV (*GauchoVille*) actually shows up with probability 90\\%. \n\nIf flight GA5A has only 186 seats, but sells 195 tickets, what is the probability that *GauchoAir* will need to re-book certain passengers?\n:::\n:::\n:::\n:::\n\n\n## Solution {style=\"font-size:30px\"}\n\n- As always, we start by defining quantities.\n\n- Let $X$ denote the number of passengers, out of the 195 booked on the flight, that actually show up for the flight. \n\n- The video claims that $X$ follows a Binomial Distribution- let's work with that claim for a moment (and then we can revisit that assumption later).\n\n- Specifically, $X \\sim \\mathrm{Bin}(195, \\ 90)$. \n\n- Now, the airline will only need to re-book passengers when the number of passengers that arive ($X$) exceeds the capacity of the plane (186).\n\n- So, the quantity we seek is $\\mathbb{P}(X > 186) = \\mathbb{P}(X \\geq 187)$.\n\n- Though we could do this by hand, let's use Python.\n\n- We can also discuss whether we think the Binomial critera really are satisfied in this case or not.\n\n\n## Geometric Distribution {style=\"font-size:32px\"}\n\n- Another distribution we encountered (this time in Discussion Section) is the so-called **Geometric Distribution**.\n\n- If $X$ counts the number of trials (including the final trial) *until* we observe our first \"success\", and if each trial results in a \"success\" with probability $p$, then $X$ is said to follow the Geometric distribution with parameter $p$:\n$$ X \\sim \\mathrm{Geom}(p) $$\n\n- If $X \\sim \\mathrm{Geom}(p)$, we have:\n    - $S_X = \\{1, 2, 3, \\cdots \\}$\n    - $\\mathbb{P}(X = k) = (1 - p)^{k - 1} \\cdot p$\n    - $\\mathbb{E}[X] = \\frac{1}{p}$\n    - $\\mathrm{Var}(X) = \\frac{1 - p}{p^2}$ (wasn't explicitly mentioned before, but it's good to know!)\n    \n## Continuous Random Variables {style=\"font-size:32px\"}\n\n- Not all random variables are discrete- many are continuous!\n- Continuous random variables are characterized by a **probability density function** (pdf) $f_X(x)$, which must obey the following two properties:\n    1) $f_X(x)$ must be nonnegative everywhere\n    2) The area underneath the graph of $f_X(x)$ must be 1\n\n- The graph of a pdf is called a **density curve**\n- Probabilities are found as areas underneath the density curve.\n\n## Areas Under the Density Curve {style=\"font-size:32px\"}\n\n\\\n\n```{r, fig = T, fig.align = 'center', fig.width = 7, fig.height = 4}\n#| echo: False\n\nf <- function(x){\n  0.3 * dbeta(x, 2, 6) + 0.7 * dbeta(x, 6, 2)\n}\n\nx <- seq(0, 1, by = 0.1)\ndata.frame(x) %>% ggplot(aes(x = x)) +\n  stat_function(fun = f,\n                xlim = c(0.25, 0.75),\n                geom = \"area\",\n                fill = \"#a4caeb\") +\n  stat_function(fun = f,\n                size = 1) +\n  theme_economist_white() +\n  theme(\n    panel.background = element_rect(\"#f0ebd8\"),\n    plot.background = element_rect(fill = \"#f0ebd8\")\n  ) +\n  xlab(\"\") +\n  ylab(\"\") \n```\n\n- For example, the area above represents $\\mathbb{P}(0.25 \\leq X \\leq 0.75)$.\n\n## Uniform Distribution {style=\"font-size:32px\"}\n\n- An specific example of a continuous distribution is the **Uniform distribution** with parameters $a$ and $b$: $X \\sim \\mathrm{Unif}(a, \\ b)$.\n\n- The p.d.f. is given by\n$$ f_X(x) = \\begin{cases} \\frac{1}{b - a} & \\text{if } a \\leq x \\leq b \\\\ 0 & \\text{otherwise} \\\\ \\end{cases} $$\n\n:::{.fragment style=\"text-align:center\"}\n![](unif.svg){width=\"60%\"}\n:::\n\n## Effect of Changing $a$ and $b$\n\n```{ojs}\nviewof a = Inputs.range(\n  [-3, 3], \n  {value: 0, step: 0.1, label: \"a=\"}\n)\n\nviewof b = Inputs.range(\n  [-3, 3], \n  {value: 1, step: 0.1, label: \"b=\"}\n)\n```\n\n```{ojs}\nmargin2 = ({top: 20, right: 30, bottom: 30, left: 40})\n\nheight2 = 400\n\nx_values2 = d32.scaleLinear()\n.domain(d32.extent(data2, d => d.x))\n.range([margin2.left, width - margin2.right])\n\ny_values2 = d32.scaleLinear()\n.domain([Math.min(d32.min(data2, d => d.y),0), Math.max(1,d32.max(data2, d => d.y))]).nice()\n.range([height2 - margin2.bottom, margin2.top])\n\nline2 = d32.line()\n.x(d => x_values2(d.x))\n.y(d => y_values2(d.y))\n\nxAxis2 = g => g\n.attr(\"transform\", `translate(0,${height2 - margin2.bottom})`)\n.call(d32.axisBottom(x_values2)\n      .ticks(width / 80)\n      .tickSizeOuter(0))\n\nyAxis2 = g => g\n.attr(\"transform\", `translate(${margin2.left},0)`)\n.call(d32.axisLeft(y_values2)\n      .tickValues(d32.scaleLinear().domain(y_values2.domain()).ticks()))\n\nfunction unif_pdf (input_value, mu, sigsq) {\nif(input_value < a){\n  return 0\n} else if(input_value > b){\n  return 0\n} else{\n  return 1 / (b - a)\n}\n}\n\nabs_x2=6\n\ndata2 = {\n  let values = [];\n  for (let x = -abs_x2; x < abs_x2; x=x+0.01) values.push({\"x\":x,\"y\":unif_pdf(x, µ, sigsquared)});\n  return values;\n}\n\nd32 = require(\"https://d3js.org/d3.v5.min.js\")\n\nchart2 = {\n  const svg = d32.select(DOM.svg(width, height2));\n  \n  svg.append(\"g\")\n  .call(xAxis2);\n  \n  svg.append(\"g\")\n  .call(yAxis2);\n  \n  svg.append(\"path\")\n  .datum(data2)\n  .attr(\"fill\", \"none\")\n  .attr(\"stroke\", \"steelblue\")\n  .attr(\"stroke-width\", 4)\n  .attr(\"stroke-linejoin\", \"round\")\n  .attr(\"stroke-linecap\", \"round\")\n  .attr(\"d\", line);\n  \n  return svg.node();\n}\n\n```\n\n:::{style=\"font-size:18px; text-align:right\"}\n*Credit to https://observablehq.com/@dswalter/normal-distribution for the base of the applet code*\n:::\n\n\n## Uniform Distribution {style=\"font-size:32px\"}\n\n- The expected value and variance are:\n$$ \\mathbb{E}[X] = \\frac{a + b}{2} ; \\quad \\mathrm{Var}(X) = \\frac{(b - a)^2}{12} $$\n\n- Again, probabilities are found as areas underneath the density curve:\n\n:::{.fragment}\n```{r}\n#| fig.height = 3,\n#| fig.width = 7,\n#| fig.align = 'center'\n\nggplot(data.frame(x = c(-0.5, 1.5)), aes(x)) +\n  stat_function(fun = dunif,\n                args = list(0, 1),\n                xlim = c(0.25, 0.75),\n                geom = \"area\",\n                fill = \"#a4caeb\",\n                n = 250) +\n  stat_function(fun = dunif,\n                args = list(0, 1),\n                size = 1.5,\n                n = 250) +\n  theme_economist_white() +\n  theme(\n    panel.background = element_rect(\"#f0ebd8\"),\n    plot.background = element_rect(fill = \"#f0ebd8\")\n  ) +\n  xlab(\"\") +\n  ylab(\"\") +\n  ylim(c(0, 1.25)) +\n  scale_x_continuous(breaks = round(\n    seq(-0.5, 1.5, by = 0.5), \n    1)\n  ) +\n  theme(plot.margin = margin(-1, 0, 0, 0, \"cm\"))\n\n```\n:::\n\n## Tail Probabilities {style=\"font-size:32px\"}\n\n- Visualizing probabilities as areas also enables us to write more complicated probabilistic expressions as differences of tail probabilities:\n\n\n::::{.columns}\n\n:::{.column width=\"30%\"}\n\\\n:::\n\n:::{.column width=\"40%\"}\n:::{.fragment}\n```{r}\n#| fig.height = 3,\n#| fig.width = 7,\n#| fig.align = 'center'\n\nf <- function(x){\n  0.3 * dbeta(x, 2, 6) + 0.7 * dbeta(x, 6, 2)\n}\n\nx <- seq(0, 1, by = 0.1)\ndata.frame(x) %>% ggplot(aes(x = x)) +\n  stat_function(fun = f,\n                xlim = c(0.12, 0.85),\n                geom = \"area\",\n                fill = \"#a4caeb\") +\n  stat_function(fun = f,\n                size = 1) +\n  theme_economist_white() +\n  theme(\n    panel.background = element_rect(\"#f0ebd8\"),\n    plot.background = element_rect(fill = \"#f0ebd8\")\n  ) +\n  xlab(\"\") +\n  ylab(\"\")  \n```\n:::\n:::\n\n\n:::{.column width=\"30%\"}\n:::{.fragment}\n\\\ncan be decomposed as\n:::\n:::\n\n::::\n\n\n::::{.columns}\n\n:::{.column width=\"40%\"}\n:::{.fragment}\n```{r}\n#| fig.height = 3,\n#| fig.width = 7,\n#| fig.align = 'center'\n\nf <- function(x){\n  0.3 * dbeta(x, 2, 6) + 0.7 * dbeta(x, 6, 2)\n}\n\nx <- seq(0, 1, by = 0.1)\ndata.frame(x) %>% ggplot(aes(x = x)) +\n  stat_function(fun = f,\n                xlim = c(0, 0.85),\n                geom = \"area\",\n                fill = \"#a4caeb\") +\n  stat_function(fun = f,\n                size = 1) +\n  theme_economist_white() +\n  theme(\n    panel.background = element_rect(\"#f0ebd8\"),\n    plot.background = element_rect(fill = \"#f0ebd8\")\n  ) +\n  xlab(\"\") +\n  ylab(\"\")  \n```\n:::\n:::\n\n\n:::{.column width=\"20%\"}\n:::{.fragment}\n\\\n$$ \\huge - $$\n:::\n:::\n\n:::{.column width=\"40%\"}\n:::{.fragment}\n```{r}\n#| fig.height = 3,\n#| fig.width = 7,\n#| fig.align = 'center'\n\nf <- function(x){\n  0.3 * dbeta(x, 2, 6) + 0.7 * dbeta(x, 6, 2)\n}\n\nx <- seq(0, 1, by = 0.1)\ndata.frame(x) %>% ggplot(aes(x = x)) +\n  stat_function(fun = f,\n                xlim = c(0, 0.12),\n                geom = \"area\",\n                fill = \"#a4caeb\") +\n  stat_function(fun = f,\n                size = 1) +\n  theme_economist_white() +\n  theme(\n    panel.background = element_rect(\"#f0ebd8\"),\n    plot.background = element_rect(fill = \"#f0ebd8\")\n  ) +\n  xlab(\"\") +\n  ylab(\"\")  \n```\n:::\n:::\n\n::::\n\n:::{.fragment}\n$$ \\mathbb{P}(a \\leq X \\leq b) = \\underbrace{\\mathbb{P}(X \\leq b)}_{\\text{c.d.f. at $b$}} - \\underbrace{\\mathbb{P}(X \\leq a)}_{\\text{c.d.f. at $a$}} $$\n:::\n\n## Chalkboard Example\n\n:::{.fragment}\n::: callout-tip\n## Chalkboard Example 1\n\n:::{.nonincremental}\n::: {style=\"font-size: 30px\"}\nThe time (in minutes) spent waiting in line at *Starbucks* is found to vary uniformly between 5mins and 15mins.\n\nWhat is the c.d.f. of wait times? (I.e., find the probability that a randomly selected person spends less than $x$ minutes waiting in line, for an arbitrary value $x$. Yes, your final answer will depend on $x$; that's why the c.d.f. is a *function*!)\n:::\n:::\n:::\n:::\n\n\n## Chalkboard Example\n\n:::{.fragment}\n::: callout-tip\n## Chalkboard Example 2\n\n:::{.nonincremental}\n::: {style=\"font-size: 30px\"}\nThe time (in minutes) spent waiting in line at *Starbucks* is found to vary uniformly between 5mins and 15mins.\n\nA random sample of 10 customers is taken; what is the probability that exactly 4 of these customers will spend between 10 and 13 minutes waiting in line?\n:::\n:::\n:::\n:::\n\n## Normal Distribution {style=\"font-size:30px\"}\n\n- We also learned about the **Normal Distribution**: $X \\sim \\mathcal{N}(\\mu, \\ \\sigma)$\n\n- The normal density curve is bell-shaped\n\n\n## Changing $\\mu$ and $\\sigma$\n\n\n```{ojs}\nviewof µ = Inputs.range(\n  [-3, 3], \n  {value: 0, step: 0.1, label: \"µ:\"}\n)\n\nviewof σ = Inputs.range(\n  [0.2, 3.1], \n  {value: 1, step: 0.01, label: \"σ:\"}\n)\n\nsigsquared = σ**2\n```\n\n```{ojs}\nmargin = ({top: 20, right: 30, bottom: 30, left: 40})\n\nheight = 400\n\nx_values = d3.scaleLinear()\n    .domain(d3.extent(data, d => d.x))\n    .range([margin.left, width - margin.right])\n\ny_values = d3.scaleLinear()\n    .domain([Math.min(d3.min(data, d => d.y),0), Math.max(1,d3.max(data, d => d.y))]).nice()\n    .range([height - margin.bottom, margin.top])\n    \nline = d3.line()\n    .x(d => x_values(d.x))\n    .y(d => y_values(d.y))\n\nxAxis = g => g\n  .attr(\"transform\", `translate(0,${height - margin.bottom})`)\n  .call(d3.axisBottom(x_values)\n      .ticks(width / 80)\n      .tickSizeOuter(0))\n\nyAxis = g => g\n  .attr(\"transform\", `translate(${margin.left},0)`)\n  .call(d3.axisLeft(y_values)\n      .tickValues(d3.scaleLinear().domain(y_values.domain()).ticks()))\n    \nfunction normal_pdf (input_value, mu, sigsq) {\n  let left_chunk = 1/(Math.sqrt(2*Math.PI*sigsq))\n  let right_top = -((input_value-mu)**2)\n  let right_bottom = 2*sigsq\n  return left_chunk * Math.exp(right_top/right_bottom)\n}\n\nabs_x=6\n\ndata = {\n  let values = [];\n  for (let x = -abs_x; x < abs_x; x=x+0.01) values.push({\"x\":x,\"y\":normal_pdf(x, µ, sigsquared)});\n  return values;\n}\n\nd3 = require(\"https://d3js.org/d3.v5.min.js\")\n\nchart = {\n  const svg = d3.select(DOM.svg(width, height));\n\n  svg.append(\"g\")\n      .call(xAxis);\n\n  svg.append(\"g\")\n      .call(yAxis);\n  \n  svg.append(\"path\")\n      .datum(data)\n      .attr(\"fill\", \"none\")\n      .attr(\"stroke\", \"steelblue\")\n      .attr(\"stroke-width\", 4)\n      .attr(\"stroke-linejoin\", \"round\")\n      .attr(\"stroke-linecap\", \"round\")\n      .attr(\"d\", line);\n  \n  return svg.node();\n}\n\n```\n\n:::{style=\"font-size:18px; text-align:right\"}\n*Credit to https://observablehq.com/@dswalter/normal-distribution for the majority of the applet code*\n:::\n\n\n## Standardization {style=\"font-size:30px\"}\n\n- If $X \\sim \\mathcal{N}(\\mu, \\ \\sigma)$,\n$$ \\left( \\frac{X - \\mu}{\\sigma} \\right) \\sim \\mathcal{N}(0, \\ 1)$$\n\n\\\n\n- Also, if $X \\sim \\mathcal{N}(\\mu, \\ \\sigma)$:\n    - $\\mathbb{E}[X] = \\mu$\n    - $\\mathrm{Var}(X) = \\sigma^2$\n    \n\n# Inferential Statistics\n\n## Inferential Statistics {style=\"font-size:30px\"}\n\n- The primary goal of inferential statistics is to take samples from some **population**, and use **summary statistics** to try and make **inferences** about **population parameters**\n\n- For example, we could take samples, compute **sample proportions $\\widehat{P}$**, and try to make inferences about the **population proportion** $p$.\n\n- We could also take samples, compute **sample means $\\overline{X}$**, and try to make inferences about the **population mean** $\\mu$.\n\n- Our summary statistics will often be **point estimators** (i.e. quantities that have expected value equal to the corresponding population parameter), which are *random variables* as they depend on the sample taken.\n\n    - For example, different samples of people will have different average heights.\n    \n- The distribution of a point estimator is called the **sampling distribution** of the estimator.\n\n## Sampling Distribution of $\\widehat{P}$ {style=\"font-size:28px\"}\n\n- Given a population with population proportion $p$, we use $\\widehat{P}$ as a point estimator of $p$.\n\n- Assume the **success-failure conditions** are met; i.e.\n    1) $n p \\geq 10$\n    2) $n (1 - p) \\geq 10$\n  \n- Then, the **Central Limit Theorem for Proportions** tells us that\n$$ \\widehat{P} \\sim \\mathcal{N}\\left(p, \\ \\sqrt{\\frac{p(1 - p)}{n}} \\right) $$\n\n- If we don't have access to $p$ directly (as is often the case), we use the **substitution approximation** to check whether\n    1) $n \\widehat{p} \\geq 10$\n    2) $n (1 - \\widehat{p}) \\geq 10$\n\n## Sampling Distribution of $\\overline{X}$ {style=\"font-size:24px\"}\n\n- Given a population with population mean $\\mu$ and population standard deviation $\\sigma$, we use $\\overline{X}$ as a point estimator of $\\mu$.\n\n- If the population is normally distributed, then \n$$ \\overline{X} \\sim \\mathcal{N}\\left(\\mu, \\ \\frac{\\sigma}{\\sqrt{n}} \\right) $$\nor, equivalently,\n$$ \\frac{\\overline{X} - \\mu}{\\sigma / \\sqrt{n}} \\sim \\mathcal{N}\\left(0, \\ 1 \\right) $$\n\n- If the population is *not* normally distributed, but the sample size $n$ is at least 30, then the **Central Limit Theorem for the Sample Mean** (or just the **Central Limit Theorem**) tells us\n$$ \\frac{\\overline{X} - \\mu}{\\sigma / \\sqrt{n}} \\sim \\mathcal{N}\\left(0, \\ 1 \\right) $$\n\n\n## Sampling Distribution of $\\overline{X}$ {style=\"font-size:24px\"}\n\n- If the population is non-normal, the sample size is large, and we don't have access to $\\sigma$ (but access to $s$, the sample standard deviation instead), then\n$$ \\frac{\\overline{X} - \\mu}{s / \\sqrt{n}} \\sim t_{n - 1}$$\n\n- Recall that the $t-$distribution *looks* like a standard normal distribution, but has wider tails than the standard normal distribution (which accounts for the additional uncertainty injected into the problem by using $s$, a random variable, in place of $\\sigma$, a deterministic constant).\n\n- Also recall that $t_{\\infty}$ (i.e. the $t-$distribution with an infinite **degrees of freedom**) is the same thing as the standard normal distribution.\n\n## Confidence Intervals {style=\"font-size:30px\"}\n\n- Instead of using point estimators (which are random) to estimate population parameters (which are deterministic), it may make more sense to provide an *interval* that, with some **confidence level**, contains the true parameter value.\n\n- In general, when constructing a confidence interval for a parameter $\\theta$, we use\n$$ \\widehat{\\theta} \\pm c \\cdot \\mathrm{SD}(\\widehat{\\theta}) $$\nwhere $c$ is some constant that depends on our confidence level.\n\n    - Again, think of the fishing analogy from the textbook- if we want to be *more certain* we'll catch a fish, we should cast a *wider* net; i.e. higher confidence levels will lead to *wider* intervals.\n    \n- The coefficient $c$ will also depend on the sampling distribution of $\\widehat{\\theta}$.\n\n## Confidence Intervals for a Population Proportion {style=\"font-size:30px\"}\n\n- To construct a confidence interval for an unknown population proportion $p$, we use\n$$ \\widehat{p} \\pm (-c) \\cdot \\sqrt{\\frac{\\widehat{p} \\cdot (1 - \\widehat{p})}{n}} $$\nwhere $c$ denotes the $(1 - \\alpha) / 2 \\times 100$^th^ percentile of the standard normal distribution.\n    - Again, just remember that the coefficient should be positive\n\n## Confidence Intervals for a Population Mean {style=\"font-size:30px\"}\n\n- To construct a confidence interval for an unknown population mean $\\mu$, we use\n$$ \\overline{x} \\pm (z_{\\alpha}) \\cdot \\frac{\\sigma}{\\sqrt{n}} $$\nor\n$$ \\overline{x} \\pm (-t_{n - 1, \\ \\alpha}) \\cdot \\frac{s}{\\sqrt{n}} $$\ndepending on the conditions listed in the previous section of these slides.\n\n\n## Worked-Out Example {style=\"font-size:30px\"}\n\n:::{.fragment}\n::: callout-tip\n## Worked-Out Example 3\n\n:::{.nonincremental}\n::: {style=\"font-size: 30px\"}\nSaoirse would like to construct a 95\\% confidence interval for the true proportion of California Residents that speak Spanish. To that end, she took a representative sample of 120 CA residents and found that 36 of these residents speak Spanish.\n\na. Identify the population\nb. Define the parameter of interest.\nc. Define the random variable of interest.\ne. Construct a 95\\% confidence interval for the true proportion of CA residents that speak Spanish.\n:::\n:::\n:::\n:::\n\n## Solutions {style=\"font-size:30px\"}\n\na. The population is the set of all California residents.\n\nb. The parameter of interest is $p$, the true proportion of CA residents that speak Spanish.\n\nc. The random variable of interest is $\\widehat{P}$, the proportion of people in a representative sample of 120 CA residents that speak spanish.\n\ne. We check the success-failure conditions, with the substitution approximation:\n    - $n \\widehat{p} = (120) \\cdot \\left( \\frac{36}{120} \\right) = 36 \\ \\checkmark$\n    - $n (1 - \\widehat{p}) = (120) \\cdot \\left( \\frac{84}{120} \\right) = 84 \\ \\checkmark$\n    \n- Since these conditions are met, we can proceed in constructing our confidence interval as\n$$ 0.3 \\pm 1.96 \\cdot \\sqrt{\\frac{0.3 \\cdot 0.7}{120}} = \\boxed{[0.218 \\ , \\ 0.382]} $$"},"formats":{"revealjs":{"execute":{"fig-width":10,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":false,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","html-math-method":"mathjax","slide-level":2,"to":"revealjs","incremental":true,"output-file":"mt2rev.html"},"language":{},"metadata":{"lang":"en","fig-responsive":false,"quarto-version":"1.2.335","auto-stretch":true,"title":"PSTAT 5A: Lecture 14","subtitle":"Review for Midterm 2","author":"Ethan P. Marzban","date":"5/18/23","editor":"source","title-slide-attributes":{"data-background-image":"5a_hex.png","data-background-size":"contain","data-background-opacity":"0.5","data-background-position":"left"},"theme":["default","custom.scss"],"logo":"5a_hex.png","template-partials":["title-slide.html"]}}}}