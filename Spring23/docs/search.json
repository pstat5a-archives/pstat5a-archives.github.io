[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PSTAT 5A: Understanding Data",
    "section": "",
    "text": "Welcome to the official course site for PSTAT 5A (titled Understanding Data) at the University of California, Santa Barbara! Please note that this is the site for Lecture Section 200 of the Spring 2023 iteration of the course, with Ethan Marzban. (Lecture Section 100 will be utilizing Canvas.)\n\nAll relevant information for the course can be found on this site, with the (perhaps crucial) exception of quizzes, which will take place on the course Canvas site."
  },
  {
    "objectID": "Pages/Labs/Lab03/lab03.html#comparisons",
    "href": "Pages/Labs/Lab03/lab03.html#comparisons",
    "title": "Lab03",
    "section": "Comparisons",
    "text": "Comparisons\nHere’s a question: is 2 less than 3? Well, yes it is! If we wanted to confirm this, we could simply ask Python whether 2 is less than 3 by running\n\n2 < 3\n\nTrue\n\n\nNotice, however, how Python answered this question: it simply returned True. Let’s see what the data type of True is:\n\ntype(True)\n\nbool\n\n\nTrue is of the type bool, which is short for boolean. There are only two boolean quantities in Python: True and False. Let’s see how we can generate a False value:\n\n3 < 2\n\nFalse\n\n\nHere is a list of comparison operators, taken from the Inferential Thinking textbook:\n\n\n\nComparison\nOperator\nTrue Example\nFalse Example\n\n\n\n\nLess than\n<\n2 < 3\n2 < 2\n\n\nGreater than\n>\n3 < 2\n3 > 3\n\n\nLess than or equal\n<=\n2 <= 2\n3 <= 2\n\n\nGreater than or equal\n>=\n3 >= 3\n2 >= 3\n\n\nEqual\n==\n3 == 3\n3 == 2\n\n\nNot equal\n!=\n3 != 2\n2 != 2\n\n\n\nOne nice thing about Python is that it allows for multiple simultaneous comparisons. For example,\n\n2 < 3 < 4\n\nTrue\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIn a multiple comparison, Python will only return True when all of the included comparisons are true.\n\n\nFor instance, 2 < 3 < 1 would return False, because even though 2 is less than 3 it is not true that 3 is less than 1.\n\nBelieve it or not, you can compare strings as well! Python compares strings alphabetically; that is, letters at the beginning of the alphabet are considered to have smaller ordinal value than letters at the end of the alphabet. For example:\n\n\"apple\" < \"banana\"\n\nTrue\n\n\n\n\"zebra\" < \"zanzibar\"\n\nFalse\n\n\n\n\"cat\" <= \"catenary\"\n\nTrue\n\n\n\n\n\n\n\n\nTask 1\n\n\n\nCheck how \"statistics\" and \"Statistics\" (note the capitalization!) compare. Use this to answer the question: when Python is comparing strings, does it give precedence to capital letters or not?\n\n\nFinally, we discuss how comparisons work in the context of lists and arrays. The way Python compares lists is by what is known as lexicographical order. From the official Python help documentation, this means\n\nfirst the first two items are compared, and if they differ this determines the outcome of the comparison; if they are equal, the next two items are compared, and so on, until either sequence is exhausted.\n\nFor instance, [1, 2, 3] < [2, 1, 1] would return True since 1 (the first element of the first list) is less than 2 (the first element of the second list).\n\nThe comparison of arrays is a little more straightforward, except\n\n\n\n\n\n\nImportant\n\n\n\nWhen comparing two arrays, the arrays must be of the same length.\n\n\nTo see exactly how comparison of arrays works, let’s work through a Task:\n\n\n\n\n\n\nTask 2\n\n\n\nMake an array with the elements 1, 2, and 3, and call this x. Make another array with the elements 2, 3, 1, and call this y. Run x < y, and comment on the result.\n\n\nWhat the previous task illustrates is that Python compares arrays element-wise."
  },
  {
    "objectID": "Pages/Labs/Lab03/lab03.html#conditionals",
    "href": "Pages/Labs/Lab03/lab03.html#conditionals",
    "title": "Lab03",
    "section": "Conditionals",
    "text": "Conditionals\nNow, we can use comparisons for much more than verifying simple arithmetic relationships. One of the main areas in which comparisons arise is the area of conditional expressions.\n\nSimply put, conditional expressions are how we can convey a set of choices to Python. As an example, let’s consider finding someone’s city based on their zip code. To simplify, let’s assume the only zip codes we consider are 9311, 93120, and 93150. From postal data, we know that:\n\na zip code of 93117 corresponds to Goleta\na zip code of 93120 corresponds to Santa Barbara\na zip code of 93150 corresponds to Montecito\n\nWe can rephrase this information in terms of “if” statements:\n\nIf a person has a zip code of 93117, then they are in Goleta\nOtherwise, if they have a zip code of 93120, then they are in Santa Barbara\nOtherwise, if they have a zip code of 93150, then they are in Montecito\n\nThis is precisely the syntax we would use when translating this experiment into Python syntax:\n\nif zip_code == 93117:\n  location = \"Goleta\"\nelif zip_code == 93120:\n  location = \"Santa Barbara\"\nelif zip_code == 93150:\n  location = \"Montecito\"\n\nBy the way: elif is an abbreviation for else if, which itself can be thought of as equivalent to otherwise, if.\n\nHere’s the general syntax of a conditional expression in Python:\n\nif <condition 1>:\n  <task 1>\nelif <condition 2>:\n  <task 2>\n...\nelse:\n  <final task>\n\nWhen executing the above conditional statement, Python first checks whether <condition 1> returns a value of True or False. If it returns a value of True, then <task 1> is executed and the statement ends. Otherwise, Python checks whether <task 2> is True or False; if it is True then <condition 2> is executed, etc.\n\n\n\n\n\n\n\nImportant\n\n\n\nIn the example code above: if <condition 1> is True, then no tasks beyond <task 1> are evaluated. If <condition 2> is True, then no tasks beyond <task 2> are evaluated. And so on and so forth.\n\n\n\n\n\n\n\n\nTask 3\n\n\n\nConsider the code:\n\nx = 2\n\nif x < 2:\n    x = \"hello\"\nelif x < 3:\n    x = \"goodbye\"\nelse:\n    x = \"take care\"\n\nBefore running any code, write down what you think the result of executing x would be. Then, run the loop, execute x, and check whether your answer was correct or not.\n\n\n\n\n\n\n\n\nCaution\n\n\n\nIndentation is very important in Python.\n\n\nFor example, if instead of the conditional expression in Task 3 we had instead put\n\nx = 2\n\nif x < 2:\nx = \"hello\"\nelif x < 3:\nx = \"goodbye\"\nelse:\nx = \"take care\"\n\nthen we would have received an error!"
  },
  {
    "objectID": "Pages/Labs/Lab03/lab03.html#functions",
    "href": "Pages/Labs/Lab03/lab03.html#functions",
    "title": "Lab03",
    "section": "Functions",
    "text": "Functions\nFinally, let’s quickly discuss Python functions. We’ve already been using quite a few functions:\n\n\n\n\n\n\nTask 4\n\n\n\nIn a Markdown cell, write down three functions we’ve used in the previous labs.\n\n\nIf you recall, the general syntax for calling a function is:\n\n<function name>(<arg1>, <arg2>, ... )\n\nwhere <function name> denotes the function name and <arg1>, <arg2>, etc. denote the arguments of the function.\n\nCreating your own function in Python is actually fairly simple! Here is the syntax we use:\n\ndef <function name>(<list out the argument names>):\n  \"\"\"include a 'docstring' here\"\"\"\n  <body of the function>\n  return <what you want the function to output>\n\nFor example,\n\ndef f(x, y):\n  \"\"\"returns x^2 + y^2\"\"\"\n  return x**2 + y**2\n\ncreates a function f that can be called on two arguments, x and y, and returns the sum of squares of the arguments; e.g.\n\nf(3, 4)  # should return 3^2 + 4^2 = 25\n\n25\n\n\nBy the way, the docstring referenced above is a verbal description of what the function does. (Recall from Lab01 that it is just a multi-line comment, since it is enclosed in triple quotation marks!). All functions should include a docstring to convey to the user what the function does.\n\n\n\n\n\n\nImportant\n\n\n\nIf you don’t include a return statement in the definition of a function, then your function will never return anything.\n\n\nFor instance,\n\ndef g(x, y):\n  \"\"\"should return x^2 + y^2\"\"\"\n  x**2 + y**2\n\ng(3, 4)\n\n\n\n\n\n\n\nTask 5\n\n\n\nWrite a function called cent_to_far() which takes in a single temperature c as measured in degrees Centigrade and returns the corresponding temperature in degrees Farenheit. Check that cent_to_far(0) correctly returns 0 and cent_to_far(68) correctly returns 69.7777. As a reminder: \\[ {}^{\\circ}\\mathrm{F} = \\frac{5}{9} ({}^{\\circ}\\mathrm{C}) + 32 \\]\n\n\nFinally, let’s combine some things by way of a concluding Task:\n\n\n\n\n\n\nTask 6\n\n\n\nWrite a function called parity() that returns the parity (i.e. whether a number is even or odd) of an input x. Call your parity() function on 2 and then 3 to make sure your function behaves as expected. Some hints:\n\nRecall that % is the modulus operator in Python. Specifically, x % y returns the remainder of performing y divided by x.\nRecall that even numbers are divisible by 2 (so what does this mean about the remainder of dividing x by 2 if x is even?)"
  },
  {
    "objectID": "Pages/Labs/Lab04/lab04.html",
    "href": "Pages/Labs/Lab04/lab04.html",
    "title": "Lab04",
    "section": "",
    "text": "It’s finally time for us to revisit our notions of descriptive statistics (from Week 1 of the course), now in the context of Python!"
  },
  {
    "objectID": "Pages/Labs/Lab04/lab04.html#modules-revisited",
    "href": "Pages/Labs/Lab04/lab04.html#modules-revisited",
    "title": "Lab04",
    "section": "Modules, Revisited",
    "text": "Modules, Revisited\nBefore we talk about plotting, we will need to quickly talk about modules again. Recall from Lab01 that modules are Python files containing definitions for functions and classes. Up until now, we’ve been importing all functions and classes from a module using the command\n\nfrom <module name> import *\n\nThere is another way to import modules, which is the following:\n\nimport <module name> as <abbreviation>\n\nFor example,\n\nimport numpy np\n\nnot only imports the numpy module but imports it with the abbreviation (i.e. nickname) np so that we can simply write np in place of numpy.\n\nThe reason this is particularly useful is because module names can sometimes be quite long, so being able to refer to the module with a shortened nickname will save a lot of time!\n\nIn general, if we import a module using\n\nimport <module name> as <abbreviation>\n\nwe reference functions from <module name> using the syntax\n\n<abbreviation>.<function name>()\n\nFor example, after having imported the numpy module with the nickname np, we access the sin() function contained in the numpy module by calling\n\nnp.sin()\n\n\n\n\n\n\n\nTask 1\n\n\n\n\nImport the numpy module as np, and check that np.sin(0) returns a value of 0.\nImport the datascience module as ds, and check that\n\n\nds.Table().with_columns(\n  \"Col1\", [1, 2, 3],\n  \"Col2\", [2, 3, 4]\n)\n\ncorrectly displays as\n\n\n\n\n    \n        \n            Col1 Col2\n        \n    \n    \n        \n            1    2   \n        \n        \n            2    3   \n        \n        \n            3    4   \n        \n    \n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIf you import a module with an abbreviation <abbreviation>, you must always use the abbreviation when referencing the module; not the original module name.\n\n\nFor example, after importing numpy as np, running numpy.sin() would return an error."
  },
  {
    "objectID": "Pages/Labs/Lab04/lab04.html#numerical-summaries",
    "href": "Pages/Labs/Lab04/lab04.html#numerical-summaries",
    "title": "Lab04",
    "section": "Numerical Summaries",
    "text": "Numerical Summaries\n\nMeasures of Central Tendency\nRecall that for a list of numbers \\(X = \\{x_i\\}_{i=1}^{n}\\), the mean is defined to be \\[ \\overline{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i = \\frac{1}{n} (x_1 + \\cdots + x_n) \\] Computing the mean of a list or array of numbers in Python is relatively simple, using the np.mean() function [recall that we imported the numpy module with the abbreviation np, meaning np.mean() is a shorthand for numpy.mean()]. Similarly, to compute the median of a list or array we can use np.median().\n\n\n\n\n\n\nTask 2\n\n\n\nLet x_list be a list containing the elements 1, 2, and 3, and let x_array be an array containing the elements 1, 2, and 3. Compute the mean and median of x_list and x_array using the appropriate functions from the numpy module.\n\n\n\n\nMeasures of Spread\nRecall that we also discussed several measures of spread:\n\nStandard deviation\nIQR (Interquartile Range)\nRange\n\nSure enough, the numpy module contains several functions which help us compute these measures. Let’s examine each separately.\n\n\n\n\n\n\nTask 3\n\n\n\n\nLook up the help file on the function np.ptp(), and describe what it does. Also, answer the question: what does ptp actually stand for?\nNow, apply the np.ptp() function on your x_list and x_array variables from Task 1 above and check that it functions like you expect.\n\n\n\nNext, we tackle a slightly peculiar function: np.std(). We expect this to compute the standard deviation of a list/array, but…\n\n\n\n\n\n\nTask 4\n\n\n\n\nCompute the standard deviation of the x_list variable from Task 1 by hand, and write down the answer using a comment or Markdown cell.\nNow, run np.std(x_list). Does this answer agree with what you found in part (a) above?\nNow, recompute the standard deviation of x_list by hand but this time use \\((1/n)\\) instead of \\((1 / n - 1)\\) in the formula. How does this answer compare with the result of np.std(x_list)?\n\n\n\nThe result of the previous Task is the following: given a list x = [x1, x2, ..., xn], running np.std(x) actually computes \\[ \\sqrt{ \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\overline{x})^2 } \\] as opposed to our usual definition of standard deviation \\[ s_X = \\sqrt{ \\frac{1}{n - 1} \\sum_{i=1}^{n} (x_i - \\overline{x})^2} \\] We can actually fix this issue by passing in an additional argument to the np.std() function:\n\n\n\n\n\n\nTask 4 (cont’d)\n\n\n\n\nRun np.std(x_list, ddof = 1) and check whether this matches the result of part (a) above.\n\n\n\n\n\n\n\n\n\nResult\n\n\n\nTo compute the standard deviation of a list x, we run np.std(x, ddof = 1).\n\n\nFinally, we turn to the IQR: to compute the IQR of a list/array x, we use (after importing numpy as np)\n\nnp.diff(np.percentile(x, [25,75]))[0]"
  },
  {
    "objectID": "Pages/Labs/Lab04/lab04.html#visualizations",
    "href": "Pages/Labs/Lab04/lab04.html#visualizations",
    "title": "Lab04",
    "section": "Visualizations",
    "text": "Visualizations\nIt’s finally time to make pretty pictures! The module we will use to generate visualizations in this class is the matplotlib module (though there are quite a few other modules that work for visualizations as well). The official website for matplotlib can be found at https://matplotlib.org/.\n\nBefore we generate any plots, we will need to run the following code once:\n\n%matplotlib inline\nimport matplotlib\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-v0_8-whitegrid')\n\nHere’s what these lines of code are doing:\n\n%matplotlib inline tells Jupyter to actually display our plots in our notebook (if we didn’t include this line, our plots wouldn’t display)\nimport matplotlib imports the matplotlib module\nimport matplotlib.pyplot as plt imports the pyplot submodule (a submodule is just a module contained within another larger module) with the abbreviation plt.\nplt.style.use('seaborn-v0_8-whitegrid') tells Jupyter to use a specific theme (called seaborn-v0_8-whitegrid) when generating plots.\n\nAgain, notice the beauty of the import <module> as <abbreviation> syntax- after running the third line above, we no longer need to write matplotlib.pyplot, just plt! Also, there are lots of other themes you can use when generating your plots: after completing this lab, I encourage you to consult this reference guide for a list of a few other pyplot themes.\n\nBoxplots and Histograms\nNow, let’s proceed on to make some plots. The first two types of plots we will look at are the two we used to describe numerical data: namely, boxplots and histograms. The functions we will use are the plt.boxplot() and plt.his() functions, respectively.\n\n\n\n\n\n\nTask 5\n\n\n\n\nMake a list called y that contains the following elements: [1, 2, 3, 4, 5, 4, 3, 5, 4, 1, 2].\nRun plt.boxplot(y); (be sure to include the semicolon!). With any luck, your plot should look like:\n\n\n\n\n\n\n\nLet’s make our boxplot horizontal, as opposed to vertical. Consult the help file on the matplotlib.pyplot.boxplot() function here and figure out how to position your boxplot horizontally. Your new plot should look like:\n\n\n\n\n\n\n\nNext, let’s add some color to our plot. Within your call to plt.boxplot(), add the following: patch_artist=True, boxprops = dict(facecolor = \"aquamarine\") (don’t worry too much about what exactly this code is doing). Your boxplot should now look like this:\n\n\n\n\n\n\n\nFinally, let’s add a Title! Right below your call to plt.boxplot(), add the following: plt.title(\"My First Python Boxplot\"); (again, note the semicolons). Your final plot should look like this:\n\n\n\n\n\n\n\nTime for a review: based on the boxplot we just generated, what is the IQR of y? Write your answer in a Markdown cell. Then, use the syntax discussed in the previous section of this Lab to use Python to compute the IQR of y, and comment on the result.\n\n\n\nOf course, boxplots are not the only way to summarize numerical variables: we also have histograms!\n\n\n\n\n\n\nTask 6\n\n\n\nCall the plt.hist() function on the y list defined in Task 3, and use the help file to add arguments to your call to plt.hist() function to generate the following plot:\n\n\n\n\n\nPay attention to the number of bins!\n\n\n\n\nScatterplots\nWe should also quickly discuss how to generate scatterplots in Python.\n\n\n\n\n\n\nTask 7\n\n\n\n\nCopy-paste the following code into a code cell, and then run that cell (don’t worry about what this code is doing- we’ll discuss that in a future lab).\n\n\nnp.random.seed(5)\n\nx1 = np.random.normal(0, 1, 100)\nx2 = x1 + np.random.normal(0, 1, 100)\n\nplt.scatter(x1, x2);\n\nYour plot should look like this:\n\n\n\n\n\n\nAdd an x-axis label that says \"x1\" and a y-axis label that says \"x2\". Your final plot should look like:\n\n\n\n\n\n\n\nDoes there appear to be an association between the variables x1 and x2? If so, is the association positive or negative? Linear or nonlinear? Answer using a comment or a Markdown Cell."
  },
  {
    "objectID": "Pages/Labs/Lab04/lab04.html#plotting-a-function",
    "href": "Pages/Labs/Lab04/lab04.html#plotting-a-function",
    "title": "Lab04",
    "section": "Plotting a Function",
    "text": "Plotting a Function\nFinally, I’d like to take a quick detour from descriptive statistics and talk about how to plot a function using Python. As a concrete example, let’s try and plot a sine curve from \\(0\\) to \\(2\\pi\\).\nIf you recall, on Lab01 we used the sin() function from the math module- it turns out that the numpy module (which, recall, we have imported as np) also has a sin() function, so let’s use that one today:\n\nnp.sin()\n\nNext, we create a set of finely-spaced points between our two desired endpoints (in this case, \\(0\\) and \\(2\\pi\\), respectively). We will do so using the np.linspace() function, which works as follows:\n\nnp.linspace(start, stop, num)\n\ncreates a set of num evenly-spaced values between start and stop, respectively. For instance:\n\nnp.linspace(0, 1, 10)\n\narray([ 0.        ,  0.11111111,  0.22222222,  0.33333333,  0.44444444,\n        0.55555556,  0.66666667,  0.77777778,  0.88888889,  1.        ])\n\n\nIn the context of plotting, the more points we generate the smoother our plot will seem (you will see what this means in a minute). As such, let’s start with 150 points between 0 and 2 * pi:\n\nx = np.linspace(0, 2 * np.pi, 150)\n\nFinally, we call the plt.plot() function on x and np.sin(x) to generate our plot:\n\nplt.figure(figsize=(4.5, 2.25))\nplt.plot(x, np.sin(x))\n\n\n\n\nLet’s see what would have happened if we used fewer values in our np.linspace() call:\n\nxnew = np.linspace(0, 2 * np.pi, 10)\nplt.plot(xnew, np.sin(xnew))\n\n\n\n\n\n\nSo, the more points we include in our call to np.linspace(), the smoother our final function will look!\n\nSo, to summarize, here is the general “recipe” to plot a function f() between two values a and b in Python:\n\nLet x = np.linspace(a, b, <some large value>)\nCall plt.plot(x, f(x))\nAdd labels/titles as necessary\n\n\n\n\n\n\n\nTask 8\n\n\n\nGenerate a plot of the function \\(f(x) = x - x^2 \\sin(x)\\) between \\(x = -10\\) and \\(x = 10\\). Experiment around with the number of values generated by np.linspace() to ensure your plot is relatively smooth. Be sure to include axis labels; also, change the color of the graph to red. Your final plot should look something like this:"
  },
  {
    "objectID": "Pages/Labs/Lab04/lab04.html#note-on-submission",
    "href": "Pages/Labs/Lab04/lab04.html#note-on-submission",
    "title": "Lab04",
    "section": "Note on Submission",
    "text": "Note on Submission\nPlease note- from here on out, we will expect you to modify your notebook metadata to include your name and NetID (not your Perm Number!). For a refresher on how to do that, please consult Lab01."
  },
  {
    "objectID": "Pages/Labs/Lab05/lab05.html#random-number-generation",
    "href": "Pages/Labs/Lab05/lab05.html#random-number-generation",
    "title": "Lab05",
    "section": "Random Number Generation",
    "text": "Random Number Generation\nAs data scientists, it will be useful for us to know how to generate random numbers using Python. There are several different modules that contain functions for random number generation; the one we will use first is the numpy.random module:\n\nimport numpy.random as npr\n\nThe first function we will explore today is the npr.randint() function. This function enables us to select a random integer from the set of integers between two specified values: for example, to generate a single number from the set of integers in the set \\([a, b)\\) we would run\n\nnpr.randint(a, b)\n\nNote that the b value is not included as a value that can be selected; for instance, npr.randint(1, 5) generates a random number from the set \\(\\{1, 2, 3, 4\\}\\).\n\n\n\n\n\n\nTask 1\n\n\n\n\nWrite code to simulate rolling a fair six-sided die 5 times. Think about how this might be translated to a context involving generating random numbers; also, you may need to consult the help file on the npr.random() function.\nUse your code from part (a) to answer the following question: when using npr.randint() to generate multiple random numbers, in what data class is the result stored?\n\n\n\nNow, when it comes to random number generation, there is a very important concept known as setting a seed.\n\n\n\n\n\n\nTask 2\n\n\n\n\nWrite npr.randint(1, 7) in a code cell, and run it three times. In a Markdown cell just below this cell, answer the following question: did you get the same result each time you ran the code cell?\nIn a new code cell write\n\n\nnpr.seed(15)\nnpr.randint(1, 7)\n\nRun this new cell three times and again answer the question: did you get the same result each time you ran the code cell?\n\nNow, turn to your neighbor and check whether you both got the same result as each other when completing task (b) above?\n\n\n\nAs you can see, setting a seed, in a sense, removes a certain amount of randomness in Python. After you set a seed, your random number generator will generate the same number (or set of numbers) every time you run it. Though it may seem unclear as to why we would want this, you may be able to imagine that setting the seed is extremely important when it comes to replicability, a concept we will return to later in the course."
  },
  {
    "objectID": "Pages/Labs/Lab05/lab05.html#distributions-in-python",
    "href": "Pages/Labs/Lab05/lab05.html#distributions-in-python",
    "title": "Lab05",
    "section": "Distributions in Python",
    "text": "Distributions in Python\nIf you recall, one of the first things we did in Lab (back in Week 1!) was to use Python as a calculator. At the time, we only used Python to compute relatively simple quantities. Now that we’ve talked a bit about distributions, you can see how Python might be able to simpliy our lives greatly!\nFor instance, take the probability mass function (p.m.f.) of the \\(\\mathrm{Bin}(n, p)\\) distribution: if \\(X \\sim \\mathrm{Bin}(n, p)\\), then \\[ \\mathbb{P}(X = k) = \\binom{n}{k} \\cdot p^k \\cdot (1 - p)^{n - k} \\] Can we get Python to compute this for us? Or, remember how when we want to find areas under a normal density curve we have to use tables- can we perhaps compute these areas using Python?\nThe answer to both of these questions is, naturally, “yes”! Specifically, we will make use of the scipy.stats module which contains a plethora of functions relating to the distributions we learned in this class (as well as other distributions we won’t have time to cover).\n\nimport scipy.stats as sps\n\nLet’s tackle the Binomial distribution first. The function sps.binom.pmf() allows us to compute the p.m.f. of the Binomial distribution (with specified parameters) at a particular point.\n\n\n\n\n\n\nTask 3\n\n\n\nLet \\(X \\sim \\mathrm{Bin}(143, 0.153)\\). Compute the following using the sps.binom.pmf() function:\n\n\\(\\mathbb{P}(X = 20)\\)\n\\(\\mathbb{P}(X = 40)\\) [make sure you understand the output of this; feel free to ask your TA if you are confused!]\n\n\n\nNow, let’s talk about areas under the normal curve. If we want to find the following area:\n\nwe would run the following code:\n\nsps.norm.cdf(t, mu, sigma)\n\n\n\n\n\n\n\nTask 4\n\n\n\n\nIf \\(X \\sim \\mathcal{N}(3, 0.5)\\), compute \\(\\mathbb{P}(X \\leq 2)\\).\nIf \\(X \\sim \\mathcal{N}(-2, \\ 1)\\), compute \\(\\mathbb{P}(X \\geq 1)\\).\nIf \\(X \\sim \\mathcal{N}(0, 1)\\), compute \\(\\mathbb{P}(-1 \\leq X \\leq 1)\\).\n\n\n\nRecall that we talked about the uniform distribution; you’ll work with the Python functions that deal with the uniform distribution on the upcoming Homework."
  },
  {
    "objectID": "Pages/Labs/Lab05/lab05.html#simulation",
    "href": "Pages/Labs/Lab05/lab05.html#simulation",
    "title": "Lab05",
    "section": "Simulation",
    "text": "Simulation\nNow, let’s tie things together slightly. As data scientists, we obviously love to use data! However, sometimes data can be too time-consuming, costly, or otherwise unfeasible to collect in large quantities. In certain situations, simulations can help address these issues.\nWhen asked to define a “simulation” in the context of data science, ChatGPT returned the following:\n\n[…] a simulation is a computational model or program that is used to replicate real-world scenarios or systems in order to analyze their behavior, predict outcomes, or test hypotheses.\n\nThis is actually a great definition: simulations are designed to simulate (i.e. mimic) real-world situations to generate new observations/outcomes that (we hope) closely resemble the real-world outcomes.\nFor example, suppose we believe that weights of rats in a particular situation are normally distributed with mean 3.8oz and a standard deviation of 0.5oz. Instead of actually going out and collecting the weights of, say, 10 different rats and recording them, we could simulate collecting these weights by generating a series of random numbers that follow the \\(\\mathcal{N}(3.8, \\ 0.5)\\) distribution:\n\n\narray([3.2571847 , 4.29867272, 3.94148925, 3.04685264, 3.51069987,\n       4.62571827, 2.58666038, 3.58554369, 4.43296813, 3.3666298 ])\n\n\nThere are (once again) several modules that contain functions designed to simulate draws from different distributions: for now, we’ll stick with the scipy.stats module.\nTo simulate n draws from a \\(\\mathcal{N}(\\)mu, sigma\\()\\) distribution we use the code\n\nsps.norm.rvs(mu, sigma, n)\n\n(note that, by default, the sample size comes at the end!) To simulate n draws from a \\(\\mathrm{Unif}(\\)a, b\\()\\) distribution we use the code\n\nsps.uniform.rvs(a, b, n)\n\n\n\n\n\n\n\nTask 5\n\n\n\n\nThe time spent waiting in line at Romaine’s is uniformly distributed between 2 mins and 10 mins. Simulate the process of waiting in line at Romaine’s one hundred times; store your result in a variable called x and display only the first 10 elements of x. (Hint: Remember how to index variables!)\nThe temperature of a healthy adult is normally distributed with mean 98.2 degrees Fahrenheit and standard deviation 2.4 degrees Fahrenheit. Simulate the process of selecting 150 healthy adults and recording their temperatures (in degrees Fahrenheit); store your result in a variable called y and display only the first 10 elements of y. (Hint: Remember how to index variables!)\n\n\n\nIt turns out you can use simulations to approximate probabilities that would otherwise be very difficult to compute by hand. You will explore this topic further on the upcoming Homework assignment."
  },
  {
    "objectID": "Pages/Labs/Lab02/lab02.html#data-classes",
    "href": "Pages/Labs/Lab02/lab02.html#data-classes",
    "title": "Lab02",
    "section": "Data Classes",
    "text": "Data Classes\nLast week, we were introduced to the notion of data types. Recall that “data type” can be thought of as the category (or type) of data- i.e. integer, float, character, etc.\n\nIn Python, however, we often need to aggregate data into larger structures, often referred to as data classes.\n\nLists\nPerhaps the most fundamental data structure in Python is that of a list. Just like lists in real life or in mathematics, Python lists are just collections of items enclosed in square brackets:\n\n[<item 1>, <item 2>, ..., <item n>]\n\nAgain, the items in a list can be of any data type; we can even mix and match data types!\n\n\n\n\n\n\nTask 1\n\n\n\nCreate a list containing the elements 1, \"hi\", 3.4, and \"PSTAT 5A\". Assign this list to a variable called list1.\n\n\nJust as we were able to use a Python function (type()) to check the type of a particular piece of data, we can also use Python to check the structure or class of a piece of data. It turns out that we use the same function as before- namely, type()!\n\n\n\n\n\n\nTask 2\n\n\n\nRun the code type(list1)."
  },
  {
    "objectID": "Pages/Labs/Lab02/lab02.html#indexing",
    "href": "Pages/Labs/Lab02/lab02.html#indexing",
    "title": "Lab02",
    "section": "Indexing",
    "text": "Indexing\nAlright, now that we can store data in lists, how can we access elements in a list? The answer is to use what is known as indexing.\n\nGiven a list x, we access the ith element using the code\n\nx[i]\n\nThe reason we call this “indexing” is because the number that goes between the brackets is the index of the element that we want.\n\n\n\n\n\n\nCaution\n\n\n\nPython begins indexing at 0.\n\n\nWhat does this mean? Well, let’s see by way of an example.\n\n\n\n\n\n\nTask 3\n\n\n\n\nCreate a list with the numbers 1 through 10, inclusive.\nRun the code x[1].\nRun the code x[0].\n\n\n\nSo, what we would colloquially call the first element of a list, Python calls the zeroeth element.\n\n\nAlright, let’s put together some of the concepts we just learned.\n\n\n\n\n\n\nTask 4\n\n\n\nCreate a list called x that contains the elements 1, \"two\", 3.5, \"four\", and \"five five\". Answer the following questions WITHOUT running any code, writing your answers as a comment in a code cell:\n\nWhat would be the output of type(x)?\nWhat would be the output of type(x[1])?\nWhat would be the output of x[0]?\n\nNow, run code to verify your answers to the above three questions."
  },
  {
    "objectID": "Pages/Labs/Lab02/lab02.html#tables",
    "href": "Pages/Labs/Lab02/lab02.html#tables",
    "title": "Lab02",
    "section": "Tables",
    "text": "Tables\nAnother very useful data structure in Python is that of a table. Python tables behave pretty much the same as the tables we’ve used in, say, math- they are a grid of values arranged sequentially.\n\nTables can be created using the Table() function in Python, which itself comes from the datascience module. The general syntax of creating a table with the Table() function is:\n\nTable().with_columns(\n  \"<col 1 name>\", [<col 1, val 1>, <col 1, val 2>, ... ],\n  \"<col 2 name>\", [<col 2, val 1>, <col 2, val 2>, ... ],\n  ...\n)\n\nFor example,\n\nTable().with_columns(\n  \"Name\", [\"Ethan\", \"Morgan\", \"Amy\"],\n  \"ID\", [12345, 10394, 20343],\n  \"Office\", [\"South Hall\", \"South Hall\", \"North Hall\"]\n)\n\n\n\n    \n        \n            Name ID Office\n        \n    \n    \n        \n            Ethan  12345 South Hall\n        \n        \n            Morgan 10394 South Hall\n        \n        \n            Amy    20343 North Hall\n        \n    \n\n\n\nThere is nothing stopping us from assigning a table to a variable! For example, after running\n\ntable1 = Table().with_columns(\n  \"Name\", [\"Ethan\", \"Morgan\", \"Amy\"],\n  \"ID\", [12345, 10394, 20343],\n  \"Fav_Drink\", [\"Iced Tea\", \"Coffee\", \"Sprite\"]\n)\n\nthe variable table1 is equivalent to the table displayed above:\n\ntable1\n\n\n\n    \n        \n            Name ID Fav_Drink\n        \n    \n    \n        \n            Ethan  12345 Iced Tea \n        \n        \n            Morgan 10394 Coffee   \n        \n        \n            Amy    20343 Sprite   \n        \n    \n\n\n\n\n\n\n\n\n\nTerminology\n\n\n\nSometimes in Python we will encounter expressions of the form\n\n<object type>.<function name>()\n\nIn this syntax, the function <function name> is said to be a method. For example, the function with_columns() is a method for the Table object.\n\n\nThe datascience module contains a plethora of methods we can use to manage tables. For example, the select() method can be used to select columns by name:\n\ntable1.select(\"ID\")\n\n\n\n    \n        \n            ID\n        \n    \n    \n        \n            12345\n        \n        \n            10394\n        \n        \n            20343\n        \n    \n\n\n\n\n\n\n\n\n\nSyntax\n\n\n\nMethods are always appended to either a function that creates a blank object type (like Table()) or a variable of the correct type.\n\n\n\n\n\n\n\n\nTask 5\n\n\n\nRead the list of methods for Table objects at http://data8.org/datascience/tables.html, and write down (in a code cell, using comments) at least three different methods, including a short description of what each method does. For example:\n\n# .with_columns(): adds specified columns to a table.\n\n\n\n\n\n\n\n\n\nTask 6\n\n\n\n\nCreate the following table, and assign it to a variable called profs:\n\n\n\n\n\n    \n        \n            Professor Office Course\n        \n    \n    \n        \n            Dr. Swenson    South Hall PSTAT 130 \n        \n        \n            Dr. Wainwright Old Gym    PSTAT 120A\n        \n        \n            Dr. Mouti      Old Gym    PSTAT 126 \n        \n    \n\n\n\nRun a cell containing only the code profs to make sure (visually) that your table looks correct.\n\nSelect the column called Course from profs.\nAppend (i.e. add) a new row to the profs table, containing the following information:\n\n\n\n\n\n    \n        \n            Professor Office Course\n        \n    \n    \n        \n            Dr. Ravat South Hall PSTAT 120B\n        \n    \n\n\n\nRun a cell containing only the code profs to make sure (visually) that the appending was successful.\n\n\nSuppose we want to select rows of a table that satisfy a given condition. For example, if we wanted to find the information of only people who like Sprite in the table1 table above, we would call\n\ntable1.where(\"Fav_Drink\", \"Sprite\")\n\n\n\n    \n        \n            Name ID Fav_Drink\n        \n    \n    \n        \n            Amy  20343 Sprite   \n        \n    \n\n\n\nWhat would happen if we tried to select the rows of table1 with Coke in the Fav_Drink column? Well, since there is nobody in table1 that has coke as their favorite drink, we should hope that Python returns an empty table.\n\ntable1.where(\"Fav_Drink\", \"Coke\")\n\n\n\n    \n        \n            Name ID Fav_Drink\n        \n    \n    \n    \n\n\n\nSure enough, Python has returned an empty table!"
  },
  {
    "objectID": "Pages/Labs/Lab02/lab02.html#arrays",
    "href": "Pages/Labs/Lab02/lab02.html#arrays",
    "title": "Lab02",
    "section": "Arrays",
    "text": "Arrays\nThe final Data Structure we will examine in this class is that of an array. Arrays behave very similarly to Tables, with a few differences. For one, the syntax used to create an array is slightly different:\n\nmake_array(<item 1>, <item 2>, <item 3>, ...)\n\nFor example,\n\nmake_array(\"Spring\", \"Summer\", \"Autumn\", \"Winter\")\n\narray(['Spring', 'Summer', 'Autumn', 'Winter'],\n      dtype='<U6')\n\n\nYou may ask- what’s that dtype='<U6' symbol at the end of the output? For now, don’t worry about it, as we will revisit this later."
  },
  {
    "objectID": "Pages/Labs/Lab02/lab02.html#lists-vs.-arrays",
    "href": "Pages/Labs/Lab02/lab02.html#lists-vs.-arrays",
    "title": "Lab02",
    "section": "Lists vs. Arrays",
    "text": "Lists vs. Arrays\nSo, we now know about three different data classes in Python: lists, tables, and arrays. At first glance, lists and arrays may seem somewhat similar. However, there are a few key differences between them:\n\n\n\n\n\n\nTask 7\n\n\n\nMake a list called my_list containing the elements 1, 2, and 3, and make an array called my_array also containing the elements 1, 2, and 3. Run the following commands in separate code cells:\n\nsum(my_list)\nsum(my_array)\nmy_list + 2\nmy_array + 2\n\n\n\nWhat the previous Task illustrates is the fact that arrays lend themselves to element-wise operations, whereas lists do not. One important limitation about arrays, though, is that the elements in an array must all be of the same data type. If you try to make an array consisting of elements that are different data types Python will still run, however it will not run in the way you expect it to!"
  },
  {
    "objectID": "Pages/Labs/Lab07/lab07.html#percentiles",
    "href": "Pages/Labs/Lab07/lab07.html#percentiles",
    "title": "Lab07",
    "section": "Percentiles",
    "text": "Percentiles\nAs we have seen in lecture, confidence intervals for a population parameter \\(\\theta\\) take the form \\[ \\widehat{\\theta} \\pm c \\cdot \\mathrm{s.e.} \\] where \\(\\mathrm{s.e.}\\) denotes the standard error (i.e. standard deviation) of the point estimator \\(\\widehat{\\Theta}\\), and \\(c\\) is an appropriately-selected percentile from the distribution of \\(\\widehat{\\Theta}\\).\nUp until now, we have primarily been finding the constant \\(c\\) using the various tables at our disposal (i.e. the normal table, and the \\(t\\) table). Though being able to read these tables is a useful skill (and a skill that is potentially testable on quizzes and exams…), using computers to compute these percentiles can greatly increase efficiency.\nOn Homework 6 we were exposed to the function scipy.stats.norm.ppf() to compute the percentiles of the normal distribution. Recall that the syntax\n\nscipy.stats.norm.ppf(p, m, s)\n\ncomputes the pth percentile of the \\(\\mathcal{N}\\)(m, s) distribution. There are analagous functions that allow us to compute percentiles of other distributions; for example, scipy.stats.t.ppf() can be used to find the percentiles of the \\(t\\) distribution.\n\n\n\n\n\n\nTask 1\n\n\n\n\nFind the 10th percentile of the \\(t_{31}\\) distribution and check that this value agrees with the value provided in the \\(t-\\)table appearing on the course website. (Remember that the percentile you find will need to be scaled by \\(-1\\) to match the value provided in the table.)\nFind the 0.5th percentile of the \\(t_{11}\\) distribution and check that this value agrees with the value provided in the \\(t-\\)table appearing on the course website. (Remember that the percentile you find will need to be scaled by \\(-1\\) to match the value provided in the table.)"
  },
  {
    "objectID": "Pages/Labs/Lab07/lab07.html#default-values-in-a-function",
    "href": "Pages/Labs/Lab07/lab07.html#default-values-in-a-function",
    "title": "Lab07",
    "section": "Default Values in a Function",
    "text": "Default Values in a Function\nBy the way, it should be noted that some arguments in certain functions have what are known as default values. For example, the code scipy.stats.norm.ppf(0.025) will run even though we haven’t explicitly specified an m and s value. If we consult the help file for the scipy.stats.norm.ppf() function, we will see that the arguments m and s are assigned default values of 0 and 1 respectively. What this means is that, we don’t explicitly specify a value for m or s, Python will automatically assign them a value of 0 and 1, respectively.\n\nimport scipy.stats as sps\nsps.norm.ppf(0.025)\n\n-1.9599639845400545\n\n\n\n\n\n\n\n\nTask 2\n\n\n\n\nWhat are the arguments that the function sps.t.ppf() (assuming we have imported scipy.stats as sps) takes?\nHow many of the arguments take default values? What are the default values?\n\n\n\nWhen defining a function ourselves, we can also specify default values for some (or even all) of the arguments! The syntax we use is:\n\ndef <function name>(<arg1> = <default1>, <arg2> = <default2>, ...):\n  <body of function>\n\nFor example,\n\ndef f(x, y = 1):\n  return x + y\n\nf(3)\n\n4\n\n\n\n\n\n\n\n\nTask 3\n\n\n\nWrite a function called dice_roll() that takes in two inputs:\n\nnum_sides (with a default value of 6)\nnum_rolls (with a default value of 1)\n\nthat simulates rolling a num_sides-sided die num_rolls times. Test that your function behaves as follows:\n\ndice_roll(12, 5) # rolling a 12-sided die 5 times\n\n[3, 2, 0, 3, 6]\n\n\n\ndice_roll(12) # rolling a 12-sided die once\n\n[1]\n\n\n\ndice_roll() # rolling a 6-sided die once\n\n[4]"
  },
  {
    "objectID": "Pages/Labs/Lab07/lab07.html#testing-distributional-fits",
    "href": "Pages/Labs/Lab07/lab07.html#testing-distributional-fits",
    "title": "Lab07",
    "section": "Testing Distributional Fits",
    "text": "Testing Distributional Fits\nIf you recall, one of the first questions we need to ask ourselves when constructing a confidence interval for a population mean is whether or not the underlying distribution is normal. This begs the question: how can we tell if something follows a normal distribution?\nOne idea might be to look at a histogram. For example, given a list of numbers x sampled from an unknown distribution, we could simply generate a histogram of x and see if it is bell-shaped:\n\n\n\n\n\nThis is, however, not a good idea in practice, as there are many distributions that have bell-shaped density curves but are not normal; for example, \\(t-\\)distribution. In other words, even though the histogram above is bell-shaped, how do we know it came from a normal distribution and not a \\(t-\\)distribution? The answer is- we don’t!\nInstead, statisticians and datascientists use QQ-plots to assess distributional fits. The general idea of a QQ-plot is as follows: if x truly was sampled from a normal distribution, its sample percentiles should correspond closely with the percentiles of the standard normal distribution. As such, a QQ-plot plots the sample percentiles (sometimes called quantiles, hence the name QQ-plot as opposed to PP-plot) against the standard normal percentiles.\n\nThere are several different functions in several different Python modules which can be used to construct QQ-plots; the one we will use is the probplot() function from the scipy.stats package:\n\n\n\n\n\n\nplt.figure(figsize=(6, 3))\nsps.probplot(x, plot = plt);\n\nWhy is Python displaying a diagonal line? Well, let’s think about what our QQ-plot would look like if x were actually sampled from a normal distribution. Again, all sample quantiles of x would be pretty close to the corresponding quantiles of the standard normal distribution; i.e. the QQ-plot would look pretty close to a diagonal line!\nSaid differently: significant deviations from the diagonal line in a QQ-plot indicate non-normality. The plot above does not have too many significant deviations from the line, meaning we would say that x is likely sampled from a normal distribution. As an example of a QQ-plot that indicates non-normality:\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhen using a QQ-plot to assess normality, always check the behavior in the tails (i.e. left- and right-extremes of the plot), as this is often where non-normality becomes the clearest.\n\n\n\n\n\n\n\n\nTask 4\n\n\n\nGenerate a series of 100 draws from the \\(\\mathcal{N}(3, \\ 2.1)\\) distribution and store these in a variable called x. Generate another series of 100 draws, this time from the \\(t_{2}\\) distribution, and store these in a variable called y. Then, generate QQ-plots of both x and y, and comment on how you would be able to tell that x was normally distributed whereas y was not, even if you hadn’t been told explicitly which distributions x and y came from. Hint: you may need to look up the help file on scipy.stats.probplot() when making the QQ-plot for y."
  },
  {
    "objectID": "Pages/Labs/Lab09/Lab09.html",
    "href": "Pages/Labs/Lab09/Lab09.html",
    "title": "Lab09",
    "section": "",
    "text": "In this week’s lab, we will explore a real-world dataset."
  },
  {
    "objectID": "Pages/Labs/Lab09/Lab09.html#introduction-to-the-dataset",
    "href": "Pages/Labs/Lab09/Lab09.html#introduction-to-the-dataset",
    "title": "Lab09",
    "section": "Introduction to the Dataset",
    "text": "Introduction to the Dataset\nThe dataset we will explore is called air22.csv, and can be accessed at https://pstat5a.github.io/Files/Datasets/air22.csv. It contains observations from the Bureau of Transportation Statistics https://www.transtats.bts.gov/ on flights taking place during 2022.\nHere is an abridged version of the data dictionary:\n\nyear: the year in which the data was collected\nmonth: the month in which the data was collected\ncarrier: the carrier abbreviation (e.g. AS for Alaska, AA for American Airlines, etc.)\ncarrier_name: the full name of the carrier (e.g. Alaska, American, etc.)\nairport: the airport IATA code (e.g. SEA for SeaTac International Airport, etc.)\nairport_name: the full name of the airport\narr_flights: the number of flights that arrived\narr_del15: the number of flights that were delayed for more than 15 minutes\ncarrier_ct: the number of flights delayed due to carrier-related issues (e.g. no crew, delayed captain, etc.)\nweather_ct: the number of flights delayed due to weather-related issues\nnas_ct: the number of flights delayed due to National Air Security (NAS)-related issues (e.g. heavy traffic)\nsecurity_ct: the number of flights cancelled due to security-related issues\nlate_aircraft_ct: the number of flights delayed due to the arriving aircraft being delayed\narr_cancelled: the number of cancelled flights\narr_diverted: the number of flights that were diverted\narr_delay: the total time (in minutes) of delayed flight\ncarrier_delay: the total time (in minutes) of delay due to air carrier-related issues\nweather_delay: the total time (in minutes) of delay due to air weather-related issues\nnas_delay: the total time (in minutes) of delay due to air NAS-related issues\nsecurity_delay: the total time (in minutes) of delay due to air security-related issues\nlate_aircraft_delay: the total time (in minutes) of delay due to the arriving aircraft being delayed"
  },
  {
    "objectID": "Pages/Labs/Lab09/Lab09.html#part-1-importing",
    "href": "Pages/Labs/Lab09/Lab09.html#part-1-importing",
    "title": "Lab09",
    "section": "Part 1: Importing",
    "text": "Part 1: Importing\nWe should start by importing the dataset into our JupyterHub environment! In this class, we will almost exclusively import datasets to datascience tables (remember those, from Lab 1?) The syntax we use to import a dataset and store it in a table called table_name is:\n\ntable_name = Table.read_table(<location of dataset>)\n\nBy default, the read_table method will use the first line of data as labels for each of the columns. If, however, your data does not include labels as its first line you will need to manually specify the labels:\n\ntable_name = Table.read_table(<location of dataset>, names = [labl1, ...])\n\n\n\n\n\n\n\nAction Item\n\n\n\nImport the air22.csv dataset, and save it as a table called air. Hint: make sure you import any necessary modules first!\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe list of Table()-related methods at http://data8.org/datascience/tables.html may prove very useful for this lab!"
  },
  {
    "objectID": "Pages/Labs/Lab09/Lab09.html#part-2-getting-a-feel-for-the-dataset",
    "href": "Pages/Labs/Lab09/Lab09.html#part-2-getting-a-feel-for-the-dataset",
    "title": "Lab09",
    "section": "Part 2: Getting a Feel for the Dataset",
    "text": "Part 2: Getting a Feel for the Dataset\nLet’s start of easy by answering a few questions. You should use code to determine the answers to these questions, but should write your answers using descriptive sentences in a markdown cell.\n\n\n\n\n\n\nAction Item\n\n\n\nHow many rows are in the dataset? (Recall: in the language introduced in Week 1, this is the number of observational units in the data matrix.)\n\n\n\n\n\n\n\n\nAction Item\n\n\n\nHow many columns are in the dataset? (Recall: in the language introduced in Week 1, this is the number of variables in the data matrix.)\n\n\n\n\n\n\n\n\nAction Item\n\n\n\nReturn a list of the column names in the dataset. Hint: Recall that the column names, in the context of the datascience module, are sometimes called the labels of the table. Again, consult the help file if you need help!"
  },
  {
    "objectID": "Pages/Labs/Lab09/Lab09.html#part-3-accessing-specific-elements-of-the-dataset",
    "href": "Pages/Labs/Lab09/Lab09.html#part-3-accessing-specific-elements-of-the-dataset",
    "title": "Lab09",
    "section": "Part 3: Accessing Specific Elements of the Dataset",
    "text": "Part 3: Accessing Specific Elements of the Dataset\nThere are a few methods we can use to access specific parts of a Table:\n\ntable_name.column(<index or label>): returns an array containing the data in the specified column\ntable_name.row(<index or label>): returns an array containing the data in the specified column\ntable_name.column(i).item(j): returns the value in column i + 1, row j + 1 (note the plus ones!)\n\n\n\n\n\n\n\nAction Item\n\n\n\nDisplay only the arr_del15 column of the dataset.\n\n\n\n\n\n\n\n\nAction Item\n\n\n\nWhat years are included in the dataset?\n\n\n\n\n\n\n\n\nAction Item\n\n\n\nCreate a histogram of the delay times caused by weather-related issues. Use 100 bins, and include axis labels as well as a plot title.\n\n\nWe can also get creative, and use comparisons to subset various parts of our dataset. For example,\n\nair.column(\"carrier\") == \"AS\"\n\nwill return a Boolean vector (i.e. a vector of True and False elements), with True elements corresponding to values in the carrier column that have value \"AS\". In words: this would give us the indices of rows corresponding to data on Alaska Airlines.\n\n\n\n\n\n\nAction Item\n\n\n\nHow many observational units were recorded from Alaska Airlines? Hint: Think about how a sum could help us here."
  },
  {
    "objectID": "Pages/Labs/Lab09/Lab09.html#part-5-a-mini-project",
    "href": "Pages/Labs/Lab09/Lab09.html#part-5-a-mini-project",
    "title": "Lab09",
    "section": "Part 5: A Mini-Project",
    "text": "Part 5: A Mini-Project\nAlright, let’s close off this lab with a bit of a mini-project. Our goal is to examine the number of arrivals over time. Specifically, we would ultimately like to plot the average number of arrivals per month vs. month.\n\n\n\n\n\n\nAction Item\n\n\n\nExplain, in words, what the following line of code is doing:\n\nair.row(air.column(1) == 1)\n\n\n\n\n\n\n\n\n\nAction Item\n\n\n\nExplain, in words, what the following line of code is doing:\n\nair.row(air.column(1) == 2)[6]\n\nHint: arr_flights is the 7th column in the dataset.\n\n\n\n\n\n\n\n\nAction Item\n\n\n\nBased on your answers to the previous two Action Items, complete the following code to compute the average number of flights per month:\n\nmeans = []\nfor k in _________________:\n  means.______(np.nanmean(air.row(air.column(___) == ___)[___]))\n\n(By the way, the nanmeans() function is a variant of the mean() function from the numpy module that ignores missing values when computing the mean.)\n\n\n\n\n\n\n\n\nAction Item\n\n\n\nPlot your means list against the list containing the integers 1, 2, 3, …, 12 (this list functions like a list of month names.)"
  },
  {
    "objectID": "Pages/Labs/Lab08/lab08.html",
    "href": "Pages/Labs/Lab08/lab08.html",
    "title": "Lab08",
    "section": "",
    "text": "In this week’s lab, we will take a quick break from Python and instead learn some tools for making our Jupyter Notebooks look a little nicer (and help incorporate some mathematical equations into them!).\nAlso, the format of this lab is a little different in that there aren’t any numbered “Tasks”; instead, there are “Action Items” which you need to perform in a .ipynb file. As such, you don’t need to label your tasks; we will only be looking at your final product (i.e. your final PDF)."
  },
  {
    "objectID": "Pages/Labs/Lab08/lab08.html#introduction-to-markdown-syntax",
    "href": "Pages/Labs/Lab08/lab08.html#introduction-to-markdown-syntax",
    "title": "Lab08",
    "section": "Introduction to Markdown Syntax",
    "text": "Introduction to Markdown Syntax\nRemember way back in Week 1, when we started adding “Markdown Cells” to our Jupyter Notebooks? Unbeknownst to you, you’ve actually been using a different computing language than Python! Wikipedia defines Markdown as:\n\n[…] a lightweight markup language for creating formatted text using a plain-text editor.\n\nWhat this means is that Markdown is a language that is specifically designed to communicate with your computer and facilitate the formatting of text and equations."
  },
  {
    "objectID": "Pages/Labs/Lab08/lab08.html#markdown-syntax",
    "href": "Pages/Labs/Lab08/lab08.html#markdown-syntax",
    "title": "Lab08",
    "section": "Markdown Syntax",
    "text": "Markdown Syntax\nAs with Python, Markdown has some specific syntax as well. We will take a few moments to explore some of this syntax.\n\n1. Headers\n\n\n\n\n\n\nImportant Information\n\n\n\nThere are several different “levels” of headers:\n\nA main (first-level) header\nA second-level header\nA third-level header\nAnd so on and so forth\n\nIn Markdown, the level of the header is specified by how many hashtags (#’s) we include before the header text.\n\n\nFor example: all of the headers used in this explanation document are either second- or third-level headers.\n\n\n\n\n\n\nAction Item\n\n\n\nAdd a first-level (main) header right after the setup code chunk, and have it say Section 1: Markdown Syntax. Run this cell, and ensure it displays properly.\n\n\n\n\n\n\n\n\nAction Item\n\n\n\nAfter your first-level (main) Section 1: Markdown Syntax header, add a second-level header that says Subsection 1.1. Again, run this cell and ensure that this header appears smaller than the first-level (main) header you created in the action item above.\n\n\n\n\n2. Text Formatting\nAs we have previously seen, text written in a Markdown cell will display exactly as it is written in your final PDF. We can, however, also spice up the formatting of our text!\n\n\n\n\n\n\nNote\n\n\n\nThere are several different text formatting options available to us in Markdown:\n\nBoldface Text: **Boldface Text**\nItalicized Text: _Italicized Text_ or *Italicized Text*\nFixed-Width Text: `Fixed-Width Text`\n\n\n\n\n\n\n\n\n\nAction Item\n\n\n\nUnderneath your second-level header \"Subsection 1.1\", write the following sentence including all formatting as it appears here:\n\nThe quick brown fox jumps over the lazy dog\n\n\n\nBy the way, notice how we managed to display the sentence above as a block-quote? The syntax to do that in Markdown is using a single > symbol, followed by a space. For example,\n\nThis text\n\nwas generated using the code > This text.\n\n\n\n\n\n\nCaution\n\n\n\nBe mindful about line breaks and spaces when using Markdown syntax.\n\n\n\n\n\n\n\n\nAction Item\n\n\n\nUnderneath your “quick brown fox” text, add a block-quote that says\n\nThat was a really cool sentence!\n\nRun your Markdown cell, and ensure the block-quote is displaying properly. (Again: line breaks and indentation will be important!)\n\n\n\n\n3. Itemized and Enumerated Lists\n\n\n\n\n\n\nImportant Information\n\n\n\nThere are two main types of lists: itemized and enumerated lists. Itemized lists have no explicit ordering: these are sometimes called “bulleted” lists, and look like this:\n\nItem 1\nItem 2\nItem 3\n\nEnumerated lists do have an explicit ordering:\n\nItem 1\nItem 2\nItem 3\n\nIn Markdown, itemized lists are created by navigating to a new line, writing a dash (-) or an asterisk (*), followed by a space, followed by the text you wish to have displayed in the corresponding item. For instance: the itemized list above was generated using the code\n\n- Item 1\n- Item 2\n- Item 3\n\nIn Markdown, enumerated lists are created by navigating to a new line, writing the number of the item (e.g. 1, 2, etc.) either either a close-parenthesis ()), or a period (.) For instance: both of the following codes will generate an enumerated list:\n\n1) Item 1\n2) Item 2\n3) Item 3\n\n\n1. Item 1\n2. Item 2\n3. Item 3\n\nTo add sublevel items, navitage to a new line, hit the space bar 4 times, and then add the corresponding item symbol (i.e. - or * for itemized lists; 1) or a) or 1. or a. for enumerated lists; etc.)\n\nAs an example, this is a main-level item\n\nFollowed by a sublevel item!\n\n\nThis was generated using\n\n- As an example, this is a main-level item\n    - Followed by a sublevel item!\n\n\n\n\n\n\n\n\n\nAction Item\n\n\n\nAdd a new second-level header to your document that says Subsection 1.2: Itemized and Enumerated Lists. Underneath this, include the following list (making sure it displays properly when you run the cell!)\n\nIt is very important to check the Binomial Conditions before using the Binomial Distribution!\n\nFailure to check the necessary conditions can lead to incorrect results.\nIncorrect results are not good!\n\n\n\n\n\n\n4. Equations in Markdown\nOne of the very nice things about Markdown is that it is able to (very nicely) combine both plain-text as well as equations!\n\nThe language used to specify equations in Markdown is called LaTeX (pronounced “lay-TECH”, or “lah-TECH”). LaTeX equations have two main styles: inline, like \\(x + y\\), and “display-style”, like \\[ \\sum_{i=1}^{k} x_i \\]\n\n\n\n\n\n\nImportant Information\n\n\n\nIn LaTeX, inline equations are specified by single dollar signs and display-style equations are specified by double-dollar signs. For instance:\n\n$x + y$ will display as \\(x + y\\)\n$$x + y$$ will display as \\[x + y\\]\n\n\n\nInside either an inline or display-style equation environment, we can use LaTeX-specific syntax to generate equations.\n\n\n\n\n\n\nImportant Information\n\n\n\n\nStandard mathematical symbols display in LaTeX as they would in text: this includes addition (+), subtraction (-), division (/), and multiplication (*).\nThe symbol for “less than or equal to” (≤) is typeset as \\leq; the symbol for “greater than or equal to” (≥) is typeset as \\geq.\nExponents are generated using the caret (^): for example, $x^2$ displays as \\(x^2\\)\nSubscripts are generated using the underscore (_): for example, $x_2$ displays as \\(x_2\\)\n\n\n\n\n\n\n\n\n\nAction Item\n\n\n\nAdd a second-level (main) header that says \"Subsection 1.3: Typesetting Equations\", and underneath it write the following sentence (including all LaTeX formatting!):\n\nThe Pythagorean Theorem states that \\(a^2 + b^2 = c^2\\).\n\nRun this cell, and ensure it displays properly.\n\n\nWe can also typeset more advanced formulas and symbols:\n\n\n\n\n\n\nImportant Information\n\n\n\n\nSums (using sigma-notation) are generated using the \\sum command: for example, $\\sum_{x = 0}^{1} x$ displays as \\(\\sum_{x = 0}^{1} x\\)\nIntegrals are generated using the \\int command: for example, $\\int_{a}^{b} f(x) \\ dx$ displays as \\(\\int_{a}^{b} f(x) \\ dx\\)\nFractions are generated using the \\frac command: for example, $\\frac{1}{2}$ displays as \\(\\frac{1}{2}\\)\nSquare-roots are generated using the \\sqrt command: for example, $\\sqrt{2}$ displays as \\(\\sqrt{2}\\)\nThe symbol \\(\\pi\\) is generated using $\\pi$\nAdding a bar on top of a variable can be done using \\overline{}; e.g. $\\overline{y}$ typesets as \\(\\overline{y}\\)\nThe probability symbol can be typeset using $\\mathbb{P}$; e.g. \\(\\mathbb{P}\\).\nThe symbol for Expected Value can be typeset using $\\mathbb{E}$; e.g. \\(\\mathbb{E}\\).\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn LaTeX, curly braces ({, }) are used to delineate “chunks” of code/text. For example, if you just write $x^-2$, this displays as \\(x^-2\\). If we want to display \\(x^{-2}\\), we need to use $x^{-2}$.\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhen writing fractions inside parentheses, it is important to use \\left( and \\right) to ensure the size of the parentheses scale with the fractions. For example, \\[ (\\frac{1}{2}) \\] which was generated using $$ (\\frac{1}{2}) $$, looks a bit worse than \\[ \\left(\\frac{1}{2}\\right) \\] which was generated using $$ \\left(\\frac{1}{2}\\right) $$.\n\n\n\n\n\n\n\n\nAction Item\n\n\n\nAdd the following equation to your document: \\[ f_X(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{1}{2} \\left( \\frac{x - \\mu}{\\sigma} \\right)^2}  \\]\n\n\nThere is nothing stopping us from including equations in itemized and enumerated lists!\n\n\n\n\n\n\nAction Item\n\n\n\nAdd the following list to your document, ensuring that it displays properly when the cell is run (pay attention to the formatting of the text as well!):\n\nPythagorean Theorem: \\(a^2 + b^2 = c^2\\)\nEuler’s Identity: \\(e^{i \\pi} + 1 = 0\\)\n\n\n\nIt may also be useful to learn how to make piecewise-defined equations using LaTeX. To typeset piecewise-defined functions, we use the cases environment: \\[ \\begin{cases}\n    a   & \\text{if condition 1}   \\\\\n    b   & \\text{if condition 2}   \\\\\n    c   & \\text{if condition 3}   \\\\\n\\end{cases} \\] was generated using the code\n$$ \\begin{cases} \n    a   & \\text{if condition 1}   \\\\\n    b   & \\text{if condition 2}   \\\\\n    c   & \\text{if condition 3}   \\\\\n\\end{cases} $$\n\n\n\n\n\n\nAction Item\n\n\n\nAdd the probability density function \\(f_X(x)\\) of the \\(\\mathrm{Unif}(a, \\ b)\\) distribution to your Notebook: \\[ f_X(x) = \\begin{cases} \\frac{1}{b - a} & \\text{if } a \\leq x \\leq b \\\\ 0 & \\text{otherwise} \\\\ \\end{cases} \\]\n\n\nFinally, we should talk about how to align equations. The easiest way to align display-style equations is using an align* environment. For example: \\[\\begin{align*}\n  a + b       & = c + d + e   \\\\\n  a + b + 0   & = c + d\n\\end{align*}\\] was typeset using\n\\begin{align*}\n  a + b       & = c + d + e   \\\\\n  a + b + 0   & = c + d + 0\n\\end{align*}\n\n\n\n\n\n\nAction Item\n\n\n\nAdd the following set of equations into your Markdown document, taking care to use appropriate alignment.\n\\[\\begin{align*}\n  \\overline{x}            & = \\frac{1}{n} \\sum_{i=1}^{n} x_i    \\\\\n  n \\overline{x}    & = \\sum_{i=1}^{n} x_i\n\\end{align*}\\]\n\n\nThis is only just the surface of what LaTeX can do! If you are planning on going into any STEM-related field, we highly encourage you to delve deeper into the world of LaTeX as it will be a very valuable skill to have.\n\n\nHyperlinks\nSometimes it will be necessary to include hyperlinks into our reports. With Markdown, this is fairly straightforward!\n\n\n\n\n\n\nImportant Information\n\n\n\nThe syntax of including a hyperlink in a Markdown cell is as follows:\n\n[text](link)\n\nFor example, Click Here was generated using [Click Here](https://google.com).\n\n\n\n\n\n\n\n\nAction Item\n\n\n\nMake a new second-level header in your document called Section 1.4: Hyperlinks. Underneath this header, create clickable text that says “PSTAT Department Website” and, when clicked, navigates the user to the main website of the Department of Statistics and Applied Probability here at UCSB (https://pstat.ucsb.edu). Your final result should look and function like PSTAT Department Website."
  },
  {
    "objectID": "Pages/Labs/Lab01/lab01.html",
    "href": "Pages/Labs/Lab01/lab01.html",
    "title": "Lab01",
    "section": "",
    "text": "This page will be updated soon with the first Lab."
  },
  {
    "objectID": "Pages/Labs/Lab06/lab06.html#introduction-to-loops",
    "href": "Pages/Labs/Lab06/lab06.html#introduction-to-loops",
    "title": "Lab06",
    "section": "Introduction to Loops",
    "text": "Introduction to Loops\nSuppose we have the following outcomes of an experiment:\n\nx = ['success', 'failure', 'failure', 'success', 'failure', 'failure', 'failure', 'success']\n\nHow might we write code to count the number of successes in this string of outcomes? There are several different ways to accomplish this: one involves the main topic of today’s lab, which is a for loop.\n\n\n\nfigure source: https://i.kym-cdn.com/photos/images/newsfeed/001/393/656/da7.jpg\n\n\nHere’s the general idea: we would like to perform an element-wise comparison; that is, we would like to iteratively check whether each element of x is a success or a failure. The “brute-force” way would be to check each element individually, using comparisons:\n\nx[0] == 'success'\n\nTrue\n\n\n\nx[1] == 'success'\n\nFalse\n\n\n\nx[2] == 'success'\n\nFalse\n\n\nAs you can imagine, though, this would get incredibly tedious, especially if x were large! This is where for loops become useful: they allow us to automate this iterative process.\nBefore returning to this success/failure problem, let’s look at an example to see how for loops work.\n\nfor fruit in ['apple', 'banana', 'pear']:\n  print(fruit)\n\napple\nbanana\npear\n\n\nHere are how the different components work:\n\nThe for keyword signifies the beginning of the for loop.\nThe name fruit is the variable.\nThe list following the in keyword contains all of the different values the variable will take during the execution of the for loop.\nThe code after the initial colon : is called the body of the loop. (Note that the body of a for loop must be indented properly!) Here is how the body is executed:\n\nFirst, the variable fruit is assigned the first value in the list of possible values specified in the first line of the loop\nThen, after assigning fruit this value, the code in the body is executed once.\nNext, the variable fruit is assigned the second value of the list of values, and the body is run again.\nThis continues until the list of all possible values is exhausted.\n\n\nSometimes, it may be useful to sketch a diagram/table to keep track of the code at each iteration of the loop:\n\n\n\n\nFIRST ITERATION\n\n\n\n\nStart of Iteration\n\n\n\n\nfruit: ‘apple’\n\n\n\n\n\n\nEnd of Iteration\n\n\n\n\nfruit: ‘apple’\n\n\n\n\n\n\n\nSECOND ITERATION\n\n\n\n\nStart of Iteration\n\n\n\n\nfruit: ‘banana’\n\n\n\n\n\n\nEnd of Iteration\n\n\n\n\nfruit: ‘banana’\n\n\n\n\n\n\n\nTHIRD ITERATION\n\n\n\n\nStart of Iteration\n\n\n\n\nfruit: ‘pear’\n\n\n\n\n\n\nEnd of Iteration\n\n\n\n\nfruit: ‘pear’\n\n\n\n\n\n\nIt may seem strange to keep track of the values of the variables at the end of each iteration. The reason we do so is because sometimes the body of the loop will actually change the value of a variable! For example, consider the code\n\nfor n in [1, 2, 3]:\n  n += 2\n  print(n)\n\n3\n4\n5\n\n\nthe associated diagram would look like\n\n\n\n\nFIRST ITERATION\n\n\n\n\nStart of Iteration\n\n\n\n\nn: 1\n\n\n\n\n\n\nEnd of Iteration\n\n\n\n\nn: 3\n\n\n\n\n\n\n\nSECOND ITERATION\n\n\n\n\nStart of Iteration\n\n\n\n\nn: 2\n\n\n\n\n\n\nEnd of Iteration\n\n\n\n\nn: 4\n\n\n\n\n\n\n\nTHIRD ITERATION\n\n\n\n\nStart of Iteration\n\n\n\n\nn: 3\n\n\n\n\n\n\nEnd of Iteration\n\n\n\n\nn: 5\n\n\n\n\n\n\nBy the way, notice the shorthand notation += that was used above:\n\n\n\n\n\n\nTip\n\n\n\nThe code x += y is equivalent to x = x + y.\n\n\nFinally, one thing that should be mentioned is that you can call the variable in a loop whatever you like!\n\nfor yummy in ['apple', 'banana', 'pear']:\n  print(yummy)\n\napple\nbanana\npear\n\n\n\n\n\n\n\n\nTask 1\n\n\n\nCopy-paste the code\n\nx = ['success', 'failure', 'failure', 'success', 'failure', 'failure', 'failure', 'success']\n\ninto a cell, and run it. Then, create a for loop that iterates through the elements of x and at each iteration prints True if the corresponding element of x is a 'success' and False if the corresponding element of x is a 'failure'. Your final output should look like:\n\n\nTrue\nFalse\nFalse\nTrue\nFalse\nFalse\nFalse\nTrue\n\n\n\n\nBy the way, the set of values a variable will take during a for loop doesn’t have to be a list- it could also be an array! This is particularly useful when there are multiple things we would like to iterate over. For example:\n\nimport datascience as ds\ncredit_scores = ds.make_array(\n  [\"Anne\", 750],\n  [\"Barbara\", 755],\n  [\"Cassandra\", 745]\n)\n\nfor k in credit_scores:\n  print(k[0], \"has a credit score of\", k[1])\n\nAnne has a credit score of 750\nBarbara has a credit score of 755\nCassandra has a credit score of 745\n\n\n\n\n\n\n\n\nTask 2\n\n\n\nMake a table like the one above that keeps track of the variables and their values in the above loop. You do not need to turn this in; do it on a separate sheet of paper and in your .ipynb file simply state “I have done Task 2 on a separate sheet of paper.”\n\n\nNow, we never quite finished our problem of counting the number of successes in the variable x. We were able to iterate through the elements of x to determine which were successes and which were failures, but we never counted the number of successes.\n\nHere is the general idea:\n\nWe initialize a counter variable, which starts off with the value of 0.\nThen, we iterate through the elements of x as we did in Task 1 above. Instead of printing True or False, however, we use a conditional statement to add 1 to count if the corresponding element of x (i.e. the element of x under consideration in the current iteration of the loop).\nFinally, we see what the value of our counter variable is- this will be exactly the number of successes in x!\n\n\n\n\n\n\n\nTask 3\n\n\n\nCombine everything we’ve learned so far to count the number of successes in x. Here is a rough template of how your code should look:\n\ncount = 0     # initialize the counter variable\n\n<for loop code here, containing a conditional and a 'count += 1'>\n\ncount       # display the final value of our counter variable\n\n\n\nThere is another way to iterate through the elements in a list, and this is to use indexing. Before talking about how this works, we should quickly introduce another function: the arange() function from the numpy module. Here is how a general call to numpy.arange() works:\n\nnumpy.arange(a, b, n)\n\nThis code returns the array of evenly spaced integers between a and b - including a but excluding b, where each element is s more than the previous element. That is, the code above is equivalent to array([a, a+s, a+2s, ...]) As a concrete example:\n\nimport numpy as np\nnp.arange(0, 5, 2)\n\narray([0, 2, 4])\n\n\nThe arange() function is particularly useful when we are iterating using indices. For example, given a list x = [1, 2, 3, 4, 5], we can loop through the entries of x using:\n\nfor k in np.arange(0, len(x)):\n  print(x[k])\n\n1\n2\n3\n4\n5\n\n\nNote that this is equivalent to\n\nfor k in x:\n  print(k)\n\n1\n2\n3\n4\n5\n\n\n\n\n\n\n\n\nTask 4\n\n\n\nRewrite your loop from Task 3, except now iterate through the indices of x. Check that your output is the same as in Task 3.\n\n\n\nQuick Aside: arange() vs linspace()\nSome of you may recall that we previously used the numpy.linspace() function to generate a list of numbers between two specified endpoints. The key difference between these two functions is that:\n\narange() allows you to specify the step size\nlinspace() allows you to specify the final number of elements\n\n\n\n\n\n\n\nTask 5\n\n\n\nGenerate the list of numbers [1, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 2.9, 2] in two ways: one using arange() and the other using linspace()."
  },
  {
    "objectID": "Pages/Labs/Lab06/lab06.html#sampling-from-a-population",
    "href": "Pages/Labs/Lab06/lab06.html#sampling-from-a-population",
    "title": "Lab06",
    "section": "Sampling from a Population",
    "text": "Sampling from a Population\nTo sample k numbers from a list of numbers called y, we can use the choices() function from the module called random. Specifically, if we import random as rnd, the command\n\nrnd.choices(y, k)\n\ngenerates a list of k elements, all sampled from y.\n\n\n\n\n\n\nTask 6\n\n\n\nSimulate rolling a fair 6-sided die 100 times, and store the results of these rolls in a variable called x. (Hint: Think how you can use the choices() function to do this.)"
  },
  {
    "objectID": "Pages/Labs/Lab06/lab06.html#looking-ahead",
    "href": "Pages/Labs/Lab06/lab06.html#looking-ahead",
    "title": "Lab06",
    "section": "Looking Ahead",
    "text": "Looking Ahead\nOn the upcoming homework, you will work toward recreating the simulation we did back in Lecture 10 to construct the sampling distribution of \\(\\widehat{P}\\). This will involve using loops, so please make sure you understand the above material well!"
  },
  {
    "objectID": "Pages/Labs/Lab01_v2/lab01_v2.html",
    "href": "Pages/Labs/Lab01_v2/lab01_v2.html",
    "title": "Lab01",
    "section": "",
    "text": "Welcome to the first PSTAT 5A Computing Lab! As we will soon learn, computing software provides an incredibly useful tool in Statistical analyses. Each software comes with its own unique programming language- in this class, we will use the language known as Python, though many other programming languages exist."
  },
  {
    "objectID": "Pages/Labs/Lab01_v2/lab01_v2.html#structure-of-labs",
    "href": "Pages/Labs/Lab01_v2/lab01_v2.html#structure-of-labs",
    "title": "Lab01",
    "section": "Structure of Labs",
    "text": "Structure of Labs\nEvery week we (the course staff) will publish a lab document, which is intended to be completed during your Lab Section (i.e. your section Section) of the week.\n\nEach lab document will consist of a combination of text, tips, and the occasional task for you to complete based on the text provided. Your TA will cover exactly what you need to turn in at the end of each lab in order to receive credit, but you should read all lab material carefully and thoroughly as content from labs will appear on quizzes and exams."
  },
  {
    "objectID": "Pages/Labs/Lab01_v2/lab01_v2.html#what-is-programming",
    "href": "Pages/Labs/Lab01_v2/lab01_v2.html#what-is-programming",
    "title": "Lab01",
    "section": "What Is Programming?",
    "text": "What Is Programming?\nComputers, though incredibly useful, are fairly complex machines. To communicate with them, we need to use a specific language, known as a Programming Language. There are a number of programming languages currently in use, with names such as R, Julia, MatLab, and - the language we will use for this course - Python.\n\nPython programs can be written in a number of different environments, such as a text editor (e.g. Notepad, VS Code, etc.) or a Terminal window. For this class, we will use Jupyter Notebook (where Jupyter is pronounced like the planet), an interactive environment that has the added benefit of being hosted online, meaning you do not have to download anything onto your personal machines in order to run Python code!"
  },
  {
    "objectID": "Pages/Labs/Lab01_v2/lab01_v2.html#getting-started",
    "href": "Pages/Labs/Lab01_v2/lab01_v2.html#getting-started",
    "title": "Lab01",
    "section": "Getting Started",
    "text": "Getting Started\n\nNavigate to https://pstat5a.lsit.ucsb.edu\nClick the “Sign in with your UCSB NetID” button, and sign in.\nUnder “Notebook”, click “Python 3 (ipykernel)” (see below).\n\n\nCongratulations- you have just made your first Jupyter notebook! Now, it’s time for our first task:\n\n\n\n\n\n\nTask 1\n\n\n\nChange the name of your notebook to “Lab01” using the following steps:\n\nIn the lefthand menu bar, find the notebook you just created (by default this will be something like “Untitled” or “Untitled1”), and right-click and click “Rename” (see picture below)\n\n\n\nRename your file to “Lab01”, and then hit the return (enter) key on your keyboard. You should see the filename in the menubar update:\n\n\n\n\n\nJupyterHub Environment\nLet’s take a minute to familiarize ourselves with the JupyterHub environment. Every Jupyter notebook is comprised of what are known as cells; these are the shaded grey rectangles that appear in a Jupyter notebook.\n\nIf your cell has a grey background (like in the image above), it is inactive. To activate a cell, place your cursor inside it, and click:\n\nmeans it is selected and active, and ready to be populated with text and/or code.\n\n\n\n\n\n\nImportant\n\n\n\nWhen you run code using the “Run” button at the top of your environment, only the active cell will be executed.\n\n\n\n\nCells\nThere are two main types of cells we will be using in this class: Markdown cells (which include text/descriptions, but no code) and code cells (which contain code that needs to be run). We’ll be talking a bit more about Markdown cells in a few weeks.\n\n\n\n\n\n\nTask 2\n\n\n\n\nIf you haven’t already, click into the code cell that was automatically created when you created your document to activate it.\nClick on the dropdown menu that currently says “code” (near the center of the top of your interface), and select “Markdown”\n\n\n\nClick back into the cell, copy-paste the text [including the hashtag!] # Task 2, and then run the cell by clicking on the button that looks like a “play” symbol at the top of your window:\n\n\n\nNote that after running your cell from step 3 above, Jupyter automatically created a new code cell. Click into this code cell and run the code 2 + 2.\n\nWhen you are done, your notebook should look something like this:\n\n\n\nNotice that after running a cell, Jupyter automatically adds a new cell right after it!\n\n\n\n\n\n\nTip\n\n\n\nTo run a cell and automatically create a new cell underneath it, use the keyboard shortcut SHIFT + ENTER.\n\n\nBy the way, do you notice the little In [1]: at the left of our first cell? This is Jupyter’s way of letting us know the order in which the code cells have been executed. The 1 in our cell from Task 2 above corresponds to the fact that this was the 1st code cell we executed in our document."
  },
  {
    "objectID": "Pages/Labs/Lab01_v2/lab01_v2.html#coding-with-python",
    "href": "Pages/Labs/Lab01_v2/lab01_v2.html#coding-with-python",
    "title": "Lab01",
    "section": "Coding with Python",
    "text": "Coding with Python\nThere is a reason we use the word “language” to describe programming languages- that is because they function quite like a human language. This means that they each have their own syntax (i.e. set of grammar rules). It is precisely the Syntax of the Python language that we will be learning over the course of these Computing Labs!\n\nPrograms are made up of expressions, like 2 + 2. We evaluate expressions by running (or executing) them in a programming language. Expressions are like the sentences of programming- they contain complex pieces of information that are conveyed between the user and the computer.\n\nMuch like sentences in other languages, expressions must obey a rigid syntax. For example, when we want to perform addition in Python we must use the + symbol; we can’t, for example, say 2 plus 2.\n\nWhat happens when we violate a syntax rule? Well…\n\n\n\n\n\n\nTask 3\n\n\n\n\nCreate a mardown cell and write # Task 3\nCreate a code cell, and run 2 plus 2. (You should get an error!)\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFor this class, we expect you to precede each code cell from a particular task with a markdown cell that says # Task X (where X is the number of the task).\n\nWe will stop explicitly writing this step in the tasks below, but you are still expected to include a labeling cell!\n\n\nWell, what is this error saying? Let’s examine it more closely.\n\n  File \"<ipython-input-2-5196071441ec>\", line 1\n    2 plus 2\n      ^\nSyntaxError: invalid syntax\n\nIndeed, Python is telling us exactly what went wrong- the SyntaxError part of the error message tells us that we violated one of the syntax rules of Python, and the ^ pointing to the p in plus is telling us that the exact syntax error occurred when we tried to use the word plus.\n\n\n\n\n\n\nTip\n\n\n\nAlways read error messages!\n\n\nThe messages that Python displays when we get an error are Python’s way of trying to communicate with us what is going wrong!"
  },
  {
    "objectID": "Pages/Labs/Lab01_v2/lab01_v2.html#python-as-a-calculator",
    "href": "Pages/Labs/Lab01_v2/lab01_v2.html#python-as-a-calculator",
    "title": "Lab01",
    "section": "Python as a Calculator",
    "text": "Python as a Calculator\nAlright, let’s get our hands dirty with some real programming! One of the many uses of Python is to help us compute arithmetic quantities very quickly. As a rule-of-thumb, Python adheres to the order of operations:\n\nParentheses\nExponents\nMultiplication\nDivision\nAddition\nSubtraction\n\nHere is a list of mathematical operators and their corresponding Python syntax:\n\n\n\nOperation\nPython Operator\nExample\nResult\n\n\n\n\nAddition\n+\n2 + 2\n4\n\n\nSubtraction\n-\n2 - 2\n0\n\n\nMultiplication\n*\n2 * 2\n4\n\n\nDivision\n/\n2 / 2\n1\n\n\nExponentiation\n**\n2 ** 2\n4\n\n\n\n\n\n\n\n\n\nTask 4\n\n\n\nCompute the following:\n\n\\(\\displaystyle \\frac{2 + 3}{4 + 5^6}\\)\n\\(\\displaystyle (1 - 3 \\cdot 4^5)^{6}\\)\n\n\n\nNaturally, Python is capable of much more than just basic arithmetic!\n\n\n\n\n\n\nTask 5\n\n\n\nCreate a code chunk and run sin(1) to compute the sine of 1.\n\n\nUh-oh- looks like we’ve encountered another error! Indeed, even the most experience coder will often run up against errors like this, and need to subsequently enter the stage of debugging their code.\n\nWe’re now getting a new error: this time, it’s a NameError. As the name suggests, this is Python’s way of telling us that it doesn’t recognize the name of something we’ve written. In fact, it’s explicitly saying:\n\nNameError: name 'sin' is not defined,\n\nThis is Python’s way of telling us that it (somehow) doesn’t know what sin means.\n\nTo answer the question of why this is, we need to take a bit of a detour into the world of modules."
  },
  {
    "objectID": "Pages/Labs/Lab01_v2/lab01_v2.html#python-modules",
    "href": "Pages/Labs/Lab01_v2/lab01_v2.html#python-modules",
    "title": "Lab01",
    "section": "Python Modules",
    "text": "Python Modules\nIt is important to note that all Python objects take up space in the form of memory (i.e. storage space on your computer). Nowadays, with the recent innovations in computers, this is not so much of an issue but historically, when many of these programming languages were first being created, optimizing space was of the utmost concern. (Even today, efficiency is a guiding tenet of most programmers!)\n\nThink of it this way- if you are doing work on code that doesn’t involve much trigonometry, there isn’t a whole lot of need to have the sin function readily available. The idea programmers had was to compartmentalize, and store certain functions in what are known as modules.\n\nModules are Python files containing definitions for functions and classes (we’ll talk about classes a little later). While data types and built-in functions in the Python standard library are available for immediate use, modules need to be imported first.\n\nThe syntax for importing all functions from a module is:\n\nfrom <module name> import *\n\nSometimes, we may not want to import the entirety of a module and instead import only a couple of functions from that module. In that case, we would use the syntax:\n\nfrom <module name> import <function name>\n\nWe’ll talk a bit more about modules in a future lab. For now, let’s return to our task of computing \\(\\sin(1)\\).\n\n\n\n\n\n\nTask 5 (cont’d)\n\n\n\nIt turns out that the sin() function is located in the math module Load all functions from the math module, and then try re-running sin(1)."
  },
  {
    "objectID": "Pages/Labs/Lab01_v2/lab01_v2.html#functions",
    "href": "Pages/Labs/Lab01_v2/lab01_v2.html#functions",
    "title": "Lab01",
    "section": "Functions",
    "text": "Functions\nWe will talk extensively about Python functions in a few weeks. For now, suffice it to say that Python functions work just like mathematical functions: for example, note how we used the sin() function in the previous task. One piece of terminology that is somewhat specific to programming is the notion of calling- when we say to call a function on an argument, we mean to pass that argument through the function. So, for example, in Task 5 we called the sin() function on the argument 1."
  },
  {
    "objectID": "Pages/Labs/Lab01_v2/lab01_v2.html#variable-assignment",
    "href": "Pages/Labs/Lab01_v2/lab01_v2.html#variable-assignment",
    "title": "Lab01",
    "section": "Variable Assignment",
    "text": "Variable Assignment\nLet’s talk a bit about variables. Just like in math, variables in a programming language refer to a placeholder name for a particular piece of information (be it a function, value, etc.) The act of storing information in a variable is called assignment, and in Python variable assignment is performed using the = symbol.\n\n<variable name> = <what you want to associate with the variable>\n\nFor example, after running\n\nx = 2\n\nthe quantity x will always be synonymous with the quantity 2, and running x + 2 will return a value of 4 (as 2 + 2 = 4).\n\nPython affords a lot of flexibility when it comes to variable names- that is, we can pick almost anything we want to be a variable name! There are, however, some exceptions:\n\nVariable names cannot start with a number\nVariable names cannot include a space\n\n\n\n\n\n\n\nTip\n\n\n\nIt is a good programming practice to give your variables names that are descriptive, but not overly long.\n\n\nIf we want to view the value stored in a variable, we have two options: we could simply type the name of the variable, and run the cell:\n\nx\n\n2\n\n\nor we could pass the variable name into a call to the print() function:\n\nprint(x)\n\n2\n\n\n\n\n\n\n\n\nTask 6\n\n\n\n(a) Define a variable called my_variable, and assign it the value 5.\n(b) Now, run the command print(My_variable) (note the capitalization!)\n\n\n\n\n\n\n\n\nTip\n\n\n\nPython is case-sensitive.\n\n\nSometimes it will be necessary to update or re-assign a new value to an existing variable. For example, let’s examine the structure of the following code:\n\nx = 2\nx = x + 3\n\nWhat do you think running x will return? If you said 5, you’d be correct! The key point of this is:\n\n\n\n\n\n\nImportant\n\n\n\nIn variable assignment, Python starts by executing the righthand side of the equality before executing the lefthand side.\n\n\nSo, in code example above, Python first executed x + 3 (which is equivalent to 2 + 3; i.e. 5), and then re-assigned x the value 5."
  },
  {
    "objectID": "Pages/Labs/Lab01_v2/lab01_v2.html#comments",
    "href": "Pages/Labs/Lab01_v2/lab01_v2.html#comments",
    "title": "Lab01",
    "section": "Comments",
    "text": "Comments\nWhen writing large pieces of code, programmers will often utilize comments to annotate their work and help readers understand what their code is doing. In Python there are two types of comments: inline comments and multiline comments. As an example of both, consider the following snippet of code:\n\nx = 1             # define x\ny = 2             # define y\nz = (x + y) ** 2  # define z\ny = z / 3         # redefine y\n\n\"\"\"This code is defines 3 variables,\ncalled 'x', 'y', and 'z'.\"\"\"\n\n\n\n\n\n\n\nTask 7\n\n\n\nGo back and add some descriptive comments to some of your previous code cells. (You don’t need a separate markdown cell indicating you have done so.)"
  },
  {
    "objectID": "Pages/Labs/Lab01_v2/lab01_v2.html#data-types",
    "href": "Pages/Labs/Lab01_v2/lab01_v2.html#data-types",
    "title": "Lab01",
    "section": "Data Types",
    "text": "Data Types\nBefore closing out this lab, we should talk a bit about the quantities we assign to variables- i.e. the different data types in Python.\n\nThe term data type loosely refers to the actual type of a particular quantity (e.g. numerical, character, etc.) The main data types we will encounter in this class are:\n\nfloat: refers to numerical (real-valued) quantities\nint: short for integer; refers to numerical quantities that are integers\nstr: short for string; refers to character- or text-type data (and will always be enclosed in either single quotation marks or double quotation marks)\n\n\n\n\n\n\n\nTask 8\n\n\n\nRun each of the following:\n\ntype(1)\ntype(1.1)\ntype(\"hello\")\n\n\n\nLet’s combine our knowledge of variable assignment with our newfound knowledge of data types!\n\n\n\n\n\n\nTask 9\n\n\n\n(a) Perform the following variable assignments:\n\ncourse = \"PSTAT 5A\"\nnum_sections = 4\nsection_capacity = 25\n\n(c) A new section has been added! Update the variable num_sections to be one more than when you initially defined it above. (Don’t just use num_sections = 5- think about our discussion on updating variables above!)\n(b) Using comments, write down what you think the output of each of the following expressions will be:\n\ntype(course)\ntype(num_sections)\nnum_sections * section_capacity\n\nThen, run each expression in a separate code chunk and comment on the results.\n(c) Create a new variable called course_capacity and assign it the value of the maximum capacity of the course. (Hint: there are only 5 sections, and each section has a maximum capacity of 25. Try to use your already-defined variables as much as possible!)\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe type() function can be used to identify the data type of a particular quantity."
  },
  {
    "objectID": "Pages/Labs/Lab01_v2/lab01_v2.html#final-formatting",
    "href": "Pages/Labs/Lab01_v2/lab01_v2.html#final-formatting",
    "title": "Lab01",
    "section": "Final Formatting",
    "text": "Final Formatting\nIt’s time to start adding the finishing touches to our first lab!\n\n\n\n\n\n\nTask 10\n\n\n\n\nClick on the gear-shaped icon in the top-right of your console:\n\n\n\nScroll down until you see the Notebook Metadata:\n\n\n\nRight after the second-to-last brace (}), add a comma , and then the following code:\n\n\n\"authors\": [\n        {\n            \"name\": \"<YOUR NAME>\"\n        },\n        {\n            \"name\": \"<YOUR NETID>\"\n        }\n    ]\n\nwhere you replace <YOUR NAME> and <YOUR NETID> with your name and NetID, respectively. For example, after performing the above steps, my Notebook Metadata would look like:"
  },
  {
    "objectID": "Pages/Labs/Lab01_v2/lab01_v2.html#what-to-turn-in",
    "href": "Pages/Labs/Lab01_v2/lab01_v2.html#what-to-turn-in",
    "title": "Lab01",
    "section": "What to Turn In",
    "text": "What to Turn In\nCongrats on finishing the first PSTAT 5A Computing Lab! Here’s what you need to submit:\n\nYour downloaded .ipynb file\nYour downloaded .PDF file\n\n(Please consult the video on Canvas showing you how to upload your work to Gradescope). Toward the end of Lab, your TA will show you how to download the above files. You will have until 40 minutes after the end of your Section (i.e. until 30 minutes past the next hour) to turn in your work in order to get credit for the lab. Also, please remember that you need to upload We will be grading labs based on effort, so just turn in what you are able to!"
  },
  {
    "objectID": "Pages/course_staff.html",
    "href": "Pages/course_staff.html",
    "title": "PSTAT 5A: Understanding Data",
    "section": "",
    "text": "Instructor: Ethan P. Marzban\n\n\n\n\n\n\n\n\n\n\n\n\n\nEthan P. Marzban\n\n\nHello! I am currently in the 3rd year of my PhD program here in the PSTAT department, having joined back in 2020 (after having completed my undergraduate degree in Statistics as well). Outside of school I enjoy playing the piano, drinking boba, and talking about cats!\n\n\n\n\n\nEmail:\n\n\nepmarzban@pstat.ucsb.edu\n\n\n\n\nOH:\n\n\nHW Clinic: Tuesdays, 4:30 - 5:30 in ELLSN 2626  OH: Fridays, 12 - 1pm in SH 5607F (Sobel)"
  },
  {
    "objectID": "Pages/course_staff.html#teaching-assistants-tas",
    "href": "Pages/course_staff.html#teaching-assistants-tas",
    "title": "PSTAT 5A: Understanding Data",
    "section": "Teaching Assistants (TAs)",
    "text": "Teaching Assistants (TAs)\n\n\n\n\n\n\n\nTA: Nickolas Thiessen\n\n\n\n\n\n\n\n\n\n\n\n\n\nNickolas Thiessen\n\n\nNickolas is currently a student in the BS/MS program in Actuarial Science, in the PSTAT department.\n\n\n\n\n\nEmail:\n\n\nnickolas@ucsb.edu\n\n\n\n\nOH:\n\n\nFridays, 9 - 11am in Building 434 Rm 113\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTA: Jason Teng\n\n\n\n\n\n\n\n\n\n\n\n\n\nJason Teng\n\n\nJason is currently a student in the Masters Program in the PSTAT department.\n\n\n\n\n\nEmail:\n\n\njteng@ucsb.edu\n\n\n\n\nOH:\n\n\nThursdays, 11am - noon in South Hall 5421 (the StatLab)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTA: Yuan Zhou\n\n\n\n\n\n\n\n\n\n\n\n\n\nYuan Zhou\n\n\nYuan is currently a student in the Masters program in the PSTAT department.\n\n\n\n\n\nEmail:\n\n\nyuan_zhou@ucsb.edu\n\n\n\n\nOH:\n\n\nTuesdays, 10 - 11am in South Hall 5421 (the StatLab)"
  },
  {
    "objectID": "Pages/course_staff.html#undergraduate-learning-assistants-ulas",
    "href": "Pages/course_staff.html#undergraduate-learning-assistants-ulas",
    "title": "PSTAT 5A: Understanding Data",
    "section": "Undergraduate Learning Assistants (ULAs)",
    "text": "Undergraduate Learning Assistants (ULAs)\n\n\n\n\n\n\n\nULA: Catherine Li\n\n\n\n\n\n\n\n\n\n\n\n\n\nCatherine Li\n\n\nMore information coming soon!\n\n\n\n\n\nEmail:\n\n\ncatherine_li@umail.ucsb.edu\n\n\n\n\nOH:\n\n\nM 2 - 4pm, Th 9 - 11am (Zoom)\n\n\n\nStudy Groups:\n\n\nT 3:30 - 4:30pm (ELLSN 2626), Th 3:30 - 5:30 (SH 5421)"
  },
  {
    "objectID": "Pages/syllabus.html",
    "href": "Pages/syllabus.html",
    "title": "PSTAT 5A: Understanding Data",
    "section": "",
    "text": "WELCOME TO PSTAT 5A! I am very excited to introduce to you the wonderful worlds of Statistics and Data Science. As our world becomes ever more saturated with data, the need for data literacy becomes increasingly important. By the end of this course, I hope you will be able to think critically about statistical studies and results, understand how data can be used to simultaneously inform and manipulate, and begin applying your newfound techniques to your future endeavors, all while gaining an introduction to the fields of Statistics and Data Science. I am very much looking forward to a great quarter with all of you!\n— Ethan"
  },
  {
    "objectID": "Pages/syllabus.html#lecture-information",
    "href": "Pages/syllabus.html#lecture-information",
    "title": "PSTAT 5A: Understanding Data",
    "section": "Lecture Information",
    "text": "Lecture Information\n\n\n\n\n\n\nLecture Times and Locations\n\n\n\nTuesdays and Thursdays: 2:00pm - 3:15pm in CHEM 1171"
  },
  {
    "objectID": "Pages/syllabus.html#course-staff",
    "href": "Pages/syllabus.html#course-staff",
    "title": "PSTAT 5A: Understanding Data",
    "section": "Course Staff",
    "text": "Course Staff\n\n\n\n\n\n\n\n\n\n\nInstructor:\nEthan P. Marzban\n\n\n\n\nEmail:\nepmarzban@pstat.ucsb.edu\n\n\nHelp Hours:\n\nHW Clinic: Tuesdays 4:30 - 5:30pm in ELLSN 2626\nOH: Fridays 12-1pm in SH 5607F\n\n\n\n\n\n\n\n\n\n\n\n\nTAs\n\n\n\n\nNickolas Thiessen\n(nickolas@ucsb.edu)\n\n\nJason Teng\n(jteng@ucsb.edu)\n\n\nYuan Zhou\n(yuan_zhou@ucsb.edu)"
  },
  {
    "objectID": "Pages/syllabus.html#course-description",
    "href": "Pages/syllabus.html#course-description",
    "title": "PSTAT 5A: Understanding Data",
    "section": "Course Description",
    "text": "Course Description\nThe official description of this course, from the Course Catalogue, is:\n\nIntroduction to data science. Concepts of statistical thinking. Topics include random variables, sampling distributions, hypothesis testing, correlation and regression. Visualizing, analyzing and interpreting real world data using Python. Computing labs required."
  },
  {
    "objectID": "Pages/syllabus.html#textbooks",
    "href": "Pages/syllabus.html#textbooks",
    "title": "PSTAT 5A: Understanding Data",
    "section": "Textbook(s)",
    "text": "Textbook(s)\nThis quarter, we do not have a required textbook- the lecture slides and lab activities are designed to be self-sufficient. However, the following textbooks are highly recommended:\n\nOpenIntro: Statistics. David Diez, Mine Çetinkaya-Rundel, and Christopher D Barr. (free version, courtesy of the authors, available at https://leanpub.com/os)\nComputational and Inferential Thinking: The Foundations of Data Science. Ani Adhikari and John DeNero. (available at: https://www.inferentialthinking.com)\nStatClass (2nd Edition, Revised). Dawn E. Holmes and Lubella A. Lenaburg"
  },
  {
    "objectID": "Pages/syllabus.html#course-components",
    "href": "Pages/syllabus.html#course-components",
    "title": "PSTAT 5A: Understanding Data",
    "section": "Course Components",
    "text": "Course Components\nThe following are the assignments and metrics that will be used to compute your final grade in this course:\n\nHomework:\nThere will be weekly homework assignments that are each graded out of 10 points, on a combination of correctness and completion: we will select a few parts/problems to be graded out of 8 points on correctness, and you will be awarded 2 points for completing the remainder of the homework. Your lowest homework score will be dropped.\n\nHomework assignments will typically be released on Wednesdays and be due on Tuesdays, with the exception of Exam Weeks when the homework will be due on Monday.\n\n\nQuizzes\nQuizzes will be administered on Wednesdays, During your Lab Section. Make-up quizzes will not be offered; instead, your lowest quiz score will be dropped at the end of the quarter. There are no quizzes in Exam Weeks.\n\n\nExams\nThere are two midterms and a final exam for this class. You are required to take all three exams; failure to do so will result in an automatic grade of “F”, so please ensure you are able to take the exams on the dates listed below.\n\n\n\n\n\n\nExam Dates:\n\n\n\n\nMidterm 1 is scheduled to take place Tuesday, April 25 from 2 - 3:15pm (lecture time)\nMidterm 2 is scheduled to take place Tuesday, May 23 from 2 - 3:15pm (lecture time)\nThe final is scheduled to take place Tuesday, June 13 from 4 - 7pm (as determined by the University)\n\n\n\nThe midterms are scheduled to take place in Embarcadero Hall, and will have assigned seating. The Final Exam will also take place in Embarcadero Hall. Unless stated otherwise, all exams will be cumulative.\n\n\nSchedule of Due Dates\nA tentative schedule of release and due dates can be found here"
  },
  {
    "objectID": "Pages/syllabus.html#grading-scheme",
    "href": "Pages/syllabus.html#grading-scheme",
    "title": "PSTAT 5A: Understanding Data",
    "section": "Grading Scheme",
    "text": "Grading Scheme\nYour final grade will be computed using the following weights:\n\n\n\nHomework:\n10%\n\n\nLabs:\n10%\n\n\nQuizzes:\n10%\n\n\nMidterm 1\n20%\n\n\nMidterm 2\n20%\n\n\nFinal Examination:\n30%\n\n\n\nPlease note that late submissions for any of the above will not be accepted. Instead, I will drop your lowest homework, lab, quiz, and midterm score (you must take all three exams; failure to do so will result in an automatic ‘F’)\nYour final letter grade will be issued according to the following scheme (cutoffs between plusses and minuses will be calculated at the end of the quarter):\n\nA– – A+: 90 – 100%\nB– – B+: 80 – 89.99%\nC– – C+: 70 – 79.99%\nD– – D+ : 60 – 69.99%\nF: 0 – 59.99%\n\nPlease note: I have elected to adopt an uncurved grading scheme to eliminate any sense of “competition” among students; I highly encourage you all to collaborate with and uplift each other. Having said that, I will certainly consider adjusting the cutoffs (naturally, in everyone’s favor) at the end of the quarter if necessary."
  },
  {
    "objectID": "Pages/syllabus.html#academic-integrity",
    "href": "Pages/syllabus.html#academic-integrity",
    "title": "PSTAT 5A: Understanding Data",
    "section": "Academic Integrity",
    "text": "Academic Integrity\nAs a member of the UCSB community, it is expected that you will act with academic integrity. This means, among other things, that the work you submit should be entirely your own and not copied from any external sources. Collaboration on homework assignments is perfectly acceptable (even encouraged) but the work you submit should still be your own; you can’t have someone else write up solutions for you, nor can you consult cites like Chegg, CourseHero, etc. Anyone found guilty of academic misconduct will be reported to the Academic Senate, and will receive at minimum a failing grade on the assignment in question; further actions may also include failing the course, and marks being made on permanent records. Depending on the severity of the infraction, expulsion is also a possibility.\nBasically, don’t cheat- please! If you’re ever struggling with course material, please come talk to me or the TA’s. We are truly here for you, and want only the best for you."
  },
  {
    "objectID": "Pages/syllabus.html#intellectual-property",
    "href": "Pages/syllabus.html#intellectual-property",
    "title": "PSTAT 5A: Understanding Data",
    "section": "Intellectual Property",
    "text": "Intellectual Property\nYou’ve probably seen a clause on other syllabi stating something to the effect of “all material in this course is the intellectual property of myself and may not be shared with anyone outside this class without my explicit written permission.”\nThough this is all true, I will be making most course-related material available on a public GitHub site, which can be accessed here: https://pstat5a.github.io."
  },
  {
    "objectID": "Pages/syllabus.html#disabled-students-program-dsp",
    "href": "Pages/syllabus.html#disabled-students-program-dsp",
    "title": "PSTAT 5A: Understanding Data",
    "section": "Disabled Students Program (DSP)",
    "text": "Disabled Students Program (DSP)\nIf you have a disability, or otherwise require accommodations for the exams and/or quizzes please reach out to the Disabled Students Program (DSP) ASAP to ensure your request(s) for accommodation can be processed. We ask that all requests be logged at least a week in advance, to ensure the system enough time to process. Please note that we cannot grant any requests for accommodations unless they come to us from DSP directly."
  },
  {
    "objectID": "Pages/syllabus.html#technology-needs",
    "href": "Pages/syllabus.html#technology-needs",
    "title": "PSTAT 5A: Understanding Data",
    "section": "Technology Needs",
    "text": "Technology Needs\nAs a part of this course, you will be required to program in Python. Though the Lab Sections take place in specially designed classrooms that come equipped with computers, your homework and quizzes may cover Python-related questions, which means we expect you to have access to a laptop capable of connecting to the internet. If you do not currently possess such a laptop, please check out UCSB’s Basic Needs Resource page on Technology Resources to try and acquire one."
  },
  {
    "objectID": "Pages/syllabus.html#section-switching",
    "href": "Pages/syllabus.html#section-switching",
    "title": "PSTAT 5A: Understanding Data",
    "section": "Section Switching",
    "text": "Section Switching\nAs mentioned above, Sections (both Discussion and Lab) take place in special “Collaborate Classrooms” which are equipped with laptops. There are a fixed number of seats and laptops in these classrooms, meaning we cannot under any circumstance over-enroll sections. Therefore, if you want to switch section unofficially (we do not have the ability to switch your official enrollment through GOLD), please follow the steps at this link. Any requests to switch sections that do not adhere to the guidelines posted at that link will be ignored."
  },
  {
    "objectID": "Pages/syllabus.html#email-policy",
    "href": "Pages/syllabus.html#email-policy",
    "title": "PSTAT 5A: Understanding Data",
    "section": "Email Policy",
    "text": "Email Policy\nIf you need to send an email to me (Ethan), please include “PSTAT 5A” in the subject line of your email. Additionally, please allow up to one business day for a reply (though I will do my best to get back to you ASAP) Please note that I do not answer questions with extensive mathematics/equations over email (as trying to communicate in math over email can be a bit of a technical nightmare) Instead, if you have questions relating to course content I welcome you to either ask over Discord or during one of my Office Hours. Also, should you need to include your TA, please send a single email to both your TA and myself, as opposed to separate emails to the two of us. Thank you!"
  },
  {
    "objectID": "Pages/syllabus.html#disclaimer",
    "href": "Pages/syllabus.html#disclaimer",
    "title": "PSTAT 5A: Understanding Data",
    "section": "Disclaimer",
    "text": "Disclaimer\nThe instructor reserves the right to modify this syllabus if he deems such modifications academically advisable. Such modifications, should they occur, will be announced publicly."
  },
  {
    "objectID": "Pages/hw.html",
    "href": "Pages/hw.html",
    "title": "PSTAT 5A: Understanding Data",
    "section": "",
    "text": "Here you will find all homework assignments and their solutions.\n\nHomework 1: due by 11:59pm on Tuesday, April 11 .pdf Solns \nHomework 2: due by 11:59pm on Tuesday, April 18 .pdf Solns \nHomework 3: due by 11:59pm on MONDAY, APRIL 24 .pdf Solns \nHomework 4: due by 11:59pm on Tuesday, May 2 .pdf Solns \nHomework 5: due by 11:59pm on Wednesday, May 10 .pdf Solns \nHomework 6: due by 11:59pm on Tuesday, May 16 .pdf Solns \nHomework 7: due by 11:59pm on MONDAY, MAY 22 .pdf Solns \nHomework 8: due by 11:59pm on Wednesday, May 31 .pdf Solns \nHomework 9: due by 11:59pm on Wednesday, June 7 .pdf Solns"
  },
  {
    "objectID": "Pages/Lectures/Lecture04/Lec04.html#last-time",
    "href": "Pages/Lectures/Lecture04/Lec04.html#last-time",
    "title": "PSTAT 5A: Lecture 04",
    "section": "Last Time",
    "text": "Last Time\n\nLast time we discussed the basics of probability.\n\nThese included things like: experiments, outcome spaces, events, and probability.\n\nI’d like to impart one additional tool that can help with visualizing the relationship between events: Venn Diagrams\nWe denote the outcome space \\(\\Omega\\) by a large rectangle, and denote events by circles.\nSince events are subsets of \\(\\Omega\\), we draw them inside (physically) the rectangle representing \\(\\Omega\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture04/Lec04.html#summary",
    "href": "Pages/Lectures/Lecture04/Lec04.html#summary",
    "title": "PSTAT 5A: Lecture 04",
    "section": "Summary",
    "text": "Summary\n\n\n\n\n\n \\(A^\\complement\\)  (complement)\n\n\n\n\n\n\n\n\n \\(A \\cap B\\)  (intersection)\n\n\n\n\n\n\n\n\n \\(A \\cup B\\)  (union)"
  },
  {
    "objectID": "Pages/Lectures/Lecture04/Lec04.html#demorgans-laws",
    "href": "Pages/Lectures/Lecture04/Lec04.html#demorgans-laws",
    "title": "PSTAT 5A: Lecture 04",
    "section": "DeMorgan’s Laws",
    "text": "DeMorgan’s Laws\n\nThis can also help give us some intuition on DeMorgan’s Laws as well!\n\nLet’s do this on the chalkboard together.\n\nWe will return to Venn Diagrams periodically throughout this course- for now, I hope they provide a useful tool to help you visualize the set operations we discussed last lecture."
  },
  {
    "objectID": "Pages/Lectures/Lecture04/Lec04.html#leadup",
    "href": "Pages/Lectures/Lecture04/Lec04.html#leadup",
    "title": "PSTAT 5A: Lecture 04",
    "section": "Leadup",
    "text": "Leadup\n\nNow, let’s return to the classical approach to probability.\nAssuming the outcomes in our outcome space \\(\\Omega\\) are equally likely, the classical approach tells us to compute the probability of any event \\(E\\) as \\[ \\mathbb{P}(E) = \\frac{\\text{number of ways $E$ can occur}}{\\text{total number of elements in $\\Omega$}} \\]\nUp until now, we’ve computed both the numerator and the denominator by explicitly listing out the elements contained in the respective sets, and then counting the number of elements.\nThis works decently for small sets, but is highly inefficient for large sets."
  },
  {
    "objectID": "Pages/Lectures/Lecture04/Lec04.html#ice-cream",
    "href": "Pages/Lectures/Lecture04/Lec04.html#ice-cream",
    "title": "PSTAT 5A: Lecture 04",
    "section": "Ice Cream",
    "text": "Ice Cream\n\nBefore diving fully into the principles of counting, let’s examine a simple situation.\nSuppose we are at a small boutique ice cream parlor that offers only 3 flavors (Vanilla, Chocolate, and Matcha), and 2 toppings (sprinkles or coconut).\n\nFurther suppose that an order of ice cream must contain only 1 flavor and 1 topping.\n\nWe can list out the different orders that are possible (i.e. the outcome space of the experiment of ordering an ice cream from this shop) using a tree diagram:"
  },
  {
    "objectID": "Pages/Lectures/Lecture04/Lec04.html#fundamental-principle-of-counting",
    "href": "Pages/Lectures/Lecture04/Lec04.html#fundamental-principle-of-counting",
    "title": "PSTAT 5A: Lecture 04",
    "section": "Fundamental Principle of Counting",
    "text": "Fundamental Principle of Counting\n\n\nThis is no accident!\n\n\n\n\n\n\n\n\n\n\nFundamental Principle of Counting\n\n\n\nIf an experiment consists of \\(k\\) stages, where the \\(i\\)th stage has \\(n_i\\) possible configurations, then the total number of elements in the outcome space is \\[ n_1 \\times n_2 \\times \\cdots \\times n_k \\]\n\n\n\n\n\n\n\nSo, when we obtained our answer of \\(6\\) on the previous slide, we were implicitly using the Fundamental Principle of Counting with 2 stages (picking a flavor, and picking a topping) where the first stage (picking a flavor) had 3 possible configurations (Vanilla, Chocolate, or Matcha) and the second stage (picking a topping) had two possible configurations (sprinkles or coconut)."
  },
  {
    "objectID": "Pages/Lectures/Lecture04/Lec04.html#slot-diagrams",
    "href": "Pages/Lectures/Lecture04/Lec04.html#slot-diagrams",
    "title": "PSTAT 5A: Lecture 04",
    "section": "Slot Diagrams",
    "text": "Slot Diagrams\n\nWhen dealing with the Fundamental Principle of Counting, I find it useful to utilize what are sometimes referred to as slot diagrams.\nHere’s how we use slot diagrams:\n\nFirst put down as many slots as there are stages in our experiment: \\[ \\underline{\\ \\ \\ \\ \\ } \\ \\ \\  \\underline{\\ \\ \\ \\ \\ } \\ \\ \\ \\cdots \\ \\ \\   \\underline{\\ \\ \\ \\ \\ }  \\]\nThen, fill in each slot with the corresponding number of configurations: \\[ \\underline{\\ n_1 \\ } \\ \\ \\  \\underline{\\ n_2 \\ } \\ \\ \\ \\cdots \\ \\ \\  \\underline{\\ n_k \\ }  \\]\nFinally, invoke the Fundamental Principle of Counting to multiply the slots together: \\[ \\underline{\\ n_1 \\ } \\ \\times \\  \\underline{\\ n_2 \\ } \\ \\times \\ \\cdots \\ \\times  \\underline{\\ n_k \\ }\\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture04/Lec04.html#worked-out-example",
    "href": "Pages/Lectures/Lecture04/Lec04.html#worked-out-example",
    "title": "PSTAT 5A: Lecture 04",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\nWorked-Out Example 1\n\n\n\n\nSuppose a (different) ice cream parlor has 32 flavors, 5 toppings, and 3 drizzles. If a “scoop” consists of a flavor, topping, and drizzle, how many scoops can be created?\n\n\n\n\n\n\nThere are 3 stages: picking a flavor, picking a topping, and picking a drizzle. Hence, we draw three slots: \\[ \\underline{\\ \\ \\ \\ \\ } \\ \\ \\ \\underline{\\ \\ \\ \\ \\ } \\ \\ \\ \\underline{\\ \\ \\ \\ \\ } \\]\nThe first stage has 32 configurations, the second has 5, and the third has 3: \\[ \\underline{\\ 32 \\ } \\ \\ \\ \\underline{\\ 5 \\ } \\ \\ \\ \\underline{\\ 3 \\ } \\]\nFinally, we multiply through: \\[ \\underline{\\ 32 \\ } \\ \\times \\ \\underline{\\ 5 \\ } \\ \\times \\ \\underline{\\ 3 \\ } = \\boxed{480} \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture04/Lec04.html#ordering",
    "href": "Pages/Lectures/Lecture04/Lec04.html#ordering",
    "title": "PSTAT 5A: Lecture 04",
    "section": "Ordering",
    "text": "Ordering\n\nHere’s a question: given \\(n\\) tickets (labeled \\(1\\) through \\(n\\)), how many different ways are there to arrange them in a line?\nLet’s answer this using a slot diagram!\nWe can think of \\(n\\) stages, where the first stage corresponds to placing the first ticket down, the second stage corresponds to placing the second ticket down, and so on and so forth. \\[ \\underbrace{ \\underline{\\ \\ \\ \\ \\ } \\ \\ \\  \\underline{\\ \\ \\ \\ \\ } \\ \\ \\ \\cdots \\ \\ \\   \\underline{\\ \\ \\ \\ \\ } \\ \\ \\   \\underline{\\ \\ \\ \\ \\ } }_{\\text{$n$ slots}} \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture04/Lec04.html#factorials",
    "href": "Pages/Lectures/Lecture04/Lec04.html#factorials",
    "title": "PSTAT 5A: Lecture 04",
    "section": "Factorials",
    "text": "Factorials\n\n\n\n\n\n\n\nDefinition\n\n\n\nFor a positive integer \\(n\\), we define \\(n\\) factorial (denoted \\(n!\\)), to be \\[ n! = n \\times (n - 1) \\times (n - 2) \\times \\cdots \\times 2 \\times 1 \\]\n\n\n\n\n\n\nFor example:\n\n\\(3! = 3 \\times 2 \\times 1 = 6\\)\n\\(4! = 4 \\times 3 \\times 2 \\times 1 = 24\\)\n\\(5! = 5 \\times 4 \\times 3 \\times 2 \\times 1 = 120\\)"
  },
  {
    "objectID": "Pages/Lectures/Lecture04/Lec04.html#worked-out-example-2",
    "href": "Pages/Lectures/Lecture04/Lec04.html#worked-out-example-2",
    "title": "PSTAT 5A: Lecture 04",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\nWorked-Out Example 1\n\n\n\n\nSiobhan has 4 shirts in her closet: 2 purple shirts and 2 red shirts. When organizing these shirts in her closet she wants to keep the purple shirts together and the red shirts together, but doesn’t care if the purple group is to the left or the right of the red group. How many ways are there for Siobhan to arrange these shirts in her closet?\n\n\n\n\n\n\nLet’s answer this question two ways: using direct enumeration, and then using counting techniques.\nLabel the two purple shirts \\(P_1\\) and \\(P_2\\) respectively, and label the two red shirts \\(R_1\\) and \\(R_2\\) respectively. Then here are all of the possible reorderings of the shirts:\n\n\n\\[\\begin{align*}\n  \\Omega = \\{ & P_1 P_2 R_1 R_2, \\ P_1 P_2 R_2 R_1, \\ P_2 P_1 R_1 R_2, \\ P_2 P_1 R_2 R_1 , \\\\\n  & R_1 R_2 P_1 P_2, \\ R_2 R_1P_1 P_2, \\ R_1 R_2 P_2 P_1, \\  R_2 R_1P_2 P_1 \\}\n\\end{align*}\\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture04/Lec04.html#does-order-matter",
    "href": "Pages/Lectures/Lecture04/Lec04.html#does-order-matter",
    "title": "PSTAT 5A: Lecture 04",
    "section": "Does Order Matter?",
    "text": "Does Order Matter?\n\nLet’s now consider a slightly more abstract experiment: consider drawing two tickets from a box with tickets labeled \\(A\\) through \\(C\\), not replacing my first ticket after I draw it.\n\nHow many elements are in the outcome space of this experiment?\n\nWell, the answer is…. it depends!\nSpecifically, we need to know: does order matter?\nHere’s what I mean by order mattering: in a license plate, 123ABC and ABC123 are clearly two different license plates, despite the fact that they are comprised of the same letters and numbers!\n\nAn example of a situation in which order does not matter is drawing cards from a deck of cards: whether I get the Ace of Hearts before or after the King of Diamonds doesn’t matter- all that mattes is that I have the Ace of Hearts and the King of Diamonds!\nSpeaking of cards, you’ll discuss playing cards a bit more on HW02."
  },
  {
    "objectID": "Pages/Lectures/Lecture04/Lec04.html#if-order-does-matter",
    "href": "Pages/Lectures/Lecture04/Lec04.html#if-order-does-matter",
    "title": "PSTAT 5A: Lecture 04",
    "section": "If Order Does Matter",
    "text": "If Order Does Matter\n\nLet’s examine what happens when we assume order does matter.\nIn the context of our drawing tickets example, this means that getting \\(A\\) followed by \\(C\\) is different than getting \\(C\\) followed by \\(A\\).\nThen, letting \\((X, Y)\\) denote the outcome ``I drew the ticket labelled \\(X\\) first, then the ticket labelled \\(Y\\) second’’ (for \\(X \\in \\{A, B, C\\}\\) and \\(Y \\in \\{A, B, C\\}\\)), we have \\[\\begin{align*}\n  \\Omega   = \\{ & (A, B), \\ (A, C) \\\\\n      & (B, A), \\ (B, C)  \\\\\n      & (C, A), \\ (C, B) \\}\n\\end{align*}\\]\n\nBy the way, can anyone tell me why I didn’t include outcomes like \\((A, A)\\)?"
  },
  {
    "objectID": "Pages/Lectures/Lecture04/Lec04.html#generalizing",
    "href": "Pages/Lectures/Lecture04/Lec04.html#generalizing",
    "title": "PSTAT 5A: Lecture 04",
    "section": "Generalizing",
    "text": "Generalizing\n\nLet’s generalize to picking \\(k\\) tickets from a total \\(n\\): \\[ \\underline{\\ \\ \\ \\ \\ {\\color{blue} {n}} \\ \\ \\ \\ \\ } \\ {\\times} \\\n  \\underline{\\ \\ \\ \\ \\ {\\color{blue}{n - 1}} \\ \\ \\ \\ \\ } \\ {\\times}  \\\n  \\underline{\\ \\ \\ \\ \\ {\\color{blue}{n - 2}} \\ \\ \\ \\ \\ } \\ {\\times}  \\\n  \\cdots \\ {\\times}  \n  \\underline{\\ \\ \\ \\ \\ {\\color{blue} {n - k + 1}} \\ \\ \\ \\ \\ } \\]\nWe can write this a little more succinctly using factorials: \\[ n \\times (n - 1) \\times \\cdots \\times (n - k + 1) = \\frac{n!}{(n - k)!} \\]\nThis is yet another quantity that arises so often, we give it a name: this time we call it \\(n\\) order \\(k\\), and write \\((n)_k\\)"
  },
  {
    "objectID": "Pages/Lectures/Lecture04/Lec04.html#n-order-k",
    "href": "Pages/Lectures/Lecture04/Lec04.html#n-order-k",
    "title": "PSTAT 5A: Lecture 04",
    "section": "\\(n\\) order \\(k\\)",
    "text": "\\(n\\) order \\(k\\)\n\n\n\n\n\n\n\nDefinition\n\n\n\nFor a positive integer \\(n\\) and another positive integer \\(k\\) that is less than \\(n\\), \\[ (n)_k = \\frac{n!}{(n - k)!}  = n \\times (n - 1) \\times \\cdots \\times (n - k + 1)  \\]\n\n\n\n\n\n\nFor example:\n\n\\((5)_3 = 5 \\times 4 \\times 3 = 60\\)\n\\((6)_2 = 6 \\times 5 = 30\\)\n\\((4)_4 = 4 \\times 3 \\times 2 \\times 1 = 24\\)"
  },
  {
    "objectID": "Pages/Lectures/Lecture04/Lec04.html#worked-out-example-3",
    "href": "Pages/Lectures/Lecture04/Lec04.html#worked-out-example-3",
    "title": "PSTAT 5A: Lecture 04",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\nWorked-Out Example 2\n\n\n\n\nSuppose now that I have 5 tickets, labeled \\(A\\) through \\(E\\), and I now want to draw 3. How many ways are there to do this, assuming order matters?\n\n\n\n\n\n\nBy our work above, the answer is \\((5)_3 = 5 \\times 4 \\times 3 = \\boxed{60}\\).\nDon’t believe me?\n\n\n\\[{\\tiny \\begin{aligned}[t]\n\\Omega  & = \\{ (A, B, C), \\ (A, B, D), \\ (A, B, E), \\ (A, C, B), \\ (A, C, D), \\ (A, C, E), \\ (A, D, B), \\ (A, D, C), \\ (A, D, E), \\ (A, E, B), \\ (A, E, C), \\ (A, E, D), \\\\\n    %\n    & \\hspace{5mm} (B, A, C), \\ (B, A, D), \\ (B, A, E), \\ (B, C, A), \\ (B, C, D), \\ (B, C, E), \\ (B, D, A), \\ (B, D, C), \\ (B, D, E), \\ (B, E, A), \\ (B, E, C), \\ (B, E, D), \\\\\n    %\n    & \\hspace{5mm} (C, A, B), \\ (C, A, D), \\ (C, A, E), \\ (C, B, A), \\ (C, B, D), \\ (C, B, E), \\ (C, D, A), \\ (C, D, B), \\ (C, D, E), \\ (C, E, A), \\ (C, E, B), \\ (C, E, D), \\\\\n    %\n    & \\hspace{5mm} (D, A, B), \\ (D, A, C), \\ (D, A, E), \\ (D, B, A), \\ (D, B, C), \\ (D, B, E), \\ (D, C, A), \\ (D, C, B), \\ (D, C, E), \\ (D, E, A), \\ (D, E, B), \\ (D, E, C), \\\\\n    %\n    & \\hspace{5mm} (E, A, B), \\ (E, A, C), \\ (E, A, D), \\ (E, B, A), \\ (E, B, C), \\ (E, B, D), \\ (E, C, A), \\ (E, C, B), \\ (E, C, D), \\ (E, D, A), \\ (E, D, B), \\ (E, D, C) \\}\n    \\end{aligned}}\\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture04/Lec04.html#order-doesnt-matter",
    "href": "Pages/Lectures/Lecture04/Lec04.html#order-doesnt-matter",
    "title": "PSTAT 5A: Lecture 04",
    "section": "Order Doesn’t Matter",
    "text": "Order Doesn’t Matter\n\nLet’s return to our example of drawing \\(2\\) tickets from a set of tickets labeled \\(A\\) through \\(C\\).\nWe previously saw that if order does matter, there are 6 possible outcomes: \\[\\begin{align*}\n  \\Omega   = \\{ & (A, B), \\ (A, C) \\\\\n      & (B, A), \\ (B, C)  \\\\\n      & (C, A), \\ (C, B) \\}\n\\end{align*}\\]\nIf order doesn’t matter, we actually have fewer outcomes! Specifically:\n\n\\((A, C)\\) and \\((C, A)\\) become equivalent\n\\((A, B)\\) and \\((B, A)\\) become equivalent\n\\((B, C)\\) and \\((C, B)\\) become equivalent\n\nSo, \\(\\Omega\\) becomes \\(\\{(A, B), \\ (A, C), \\ (B, C)\\}\\), so we have only 3 elements in \\(\\Omega\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture04/Lec04.html#dont-worry",
    "href": "Pages/Lectures/Lecture04/Lec04.html#dont-worry",
    "title": "PSTAT 5A: Lecture 04",
    "section": "Don’t Worry!",
    "text": "Don’t Worry!\n\nI know that was a lot of math, pretty quickly.\nDon’t worry! We will be returning to a few of these concepts over the coming weeks.\nMy main goal for today’s lecture was to give you an overview of the types of arguments we make when dealing with counting problems, as well as to give you some tools to answer counting problems."
  },
  {
    "objectID": "Pages/Lectures/Lecture04/Lec04.html#an-exercise",
    "href": "Pages/Lectures/Lecture04/Lec04.html#an-exercise",
    "title": "PSTAT 5A: Lecture 04",
    "section": "An Exercise",
    "text": "An Exercise\n\n\n\n\n\n\nExercise 1\n\n\n\n\nCalifornia state license plates consist of 7 characters: a digit, followed by 3 letters, followed by 3 digits.\n\nSuppose we do not allow repeated letters or digits in a license plate: i.e. A123BCD456 is a valid plate whereas A122BCC345 is not. How many license plates can be created using this scheme?\nRealistically, license plates are allowed to contain repeated letters or digits. Re-answer the question of how many license plates can be created using this scheme.\nUsing the scheme outlined in part (b), what is the probability of picking a random license plate and having it be B131GHA?"
  },
  {
    "objectID": "Pages/Lectures/Lecture04/Lec04.html#lecture-summary-1",
    "href": "Pages/Lectures/Lecture04/Lec04.html#lecture-summary-1",
    "title": "PSTAT 5A: Lecture 04",
    "section": "Lecture Summary",
    "text": "Lecture Summary\n\nWe started off by talking about Venn Diagrams.\nBut, the main topic of today’s lecture was counting, which refers to the tools we use to systematically count the elements in a set without having to list out all of the elements contained in it.\nWe talked briefly about what it means for order to matter (or, consequently, not matter).\n\nThis lead us to the notations \\(n!\\), \\((n)_k\\), and \\(\\binom{n}{k}\\).\n\nNext time, we’ll see how we can use new information to update our beliefs on certain events by way of what are known as conditional probabilities.\n\nWe’ll also be able to work through a few more interesting examples."
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#structure-of-data",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#structure-of-data",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "Structure of Data",
    "text": "Structure of Data\n\nWe started by talking about the structure of data.\nWe were exposed to the notion of a data matrix, which is comprised of a series of observational units (i.e. rows) on a series of variables (i.e. columns)\nFor instance, the palmerpenguins data matrix is:\n\n\n\n\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 2 more variables: sex <fct>, year <int>"
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#structure-of-data-1",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#structure-of-data-1",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "Structure of Data",
    "text": "Structure of Data\n\n\nOf course, the reader is not expected to a priori know what the variables in a dataset represent; as such, most datasets come equipped with a data dictionary that lists out the variables included in the dataset along with a brief description of each.\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nspecies\nThe species of penguin (either Adelie, Chinstrap, or Gentoo)\n\n\nisland\nThe island on which the penguin was found (either Biscoe, Dream, or Torgersen)\n\n\nbill_length_mm\nThe length (millimeters) of the penguin’s bill\n\n\nbill_depth_mm\nThe depth (in millimeters) of the penguin’s bill\n\n\nflipper_length_mm\nThe length (in millimeters) of the penguin’s flipper\n\n\nbody_mass_g\nThe mass (in grams) of the penguin\n\n\nsex\nThe sex of the penguin (either Male or Female)\n\n\nyear\nThe year in which the penguin was observed"
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#classification-of-variables",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#classification-of-variables",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "Classification of Variables",
    "text": "Classification of Variables\n\nWe also saw that variables fall into two main types: numerical and categorical.\n\nRemember that it is not enough to simply check whether our data is comprised of numbers, as categorical data can be encoded using numbers (e.g. months in a year).\nRather, we should check whether it makes interpretable sense to add two elements in our variable (e.g. 1 + 2 is 3, whereas Jan + Feb is not March)."
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#classification-of-variables-1",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#classification-of-variables-1",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "Classification of Variables",
    "text": "Classification of Variables\n\nWithin numerical data, we have a further subdivision into discrete and continuous variables.\n\nThe set of possible values of a discrete variable has jumps, whereas the set possible values of a continuous variable has no jumps.\n\nWithin categorical data, we have a further subdivision into ordinal and nominal variables.\n\nOrdinal variables have a natural ordering (e.g. letter grades, months of the year, etc.) whereas nominal variables do not (e.g. favorite color)"
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#full-classification-scheme",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#full-classification-scheme",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "Full Classification Scheme",
    "text": "Full Classification Scheme\n\n\n\n\n\n\n\ndata_classification\n\n \n\ncluster_main\n\n  \n\ncluster_0\n\n  \n\ncluster_1\n\n  \n\ncluster_2\n\n  \n\ncluster_3\n\n   \n\nData\n\n Variable   \n\nnumerical\n\n Numerical   \n\nData->numerical\n\n    \n\ncategorical\n\n Categorical   \n\nData->categorical\n\n    \n\ncontinuous\n\n Continuous   \n\nnumerical->continuous\n\n    \n\ndiscrete\n\n Discrete   \n\nnumerical->discrete\n\n    \n\nnominal\n\n Nominal   \n\ncategorical->nominal\n\n    \n\nordinal\n\n Ordinal   \n\ncategorical->ordinal"
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#visualization",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#visualization",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "Visualization",
    "text": "Visualization\n\nOnce we have classified a variable as being either numerical or categorical, we can ask ourselves: how can we best visualize this variable?\nFor categorical data, we use a bargraph and for numerical data we use either a histogram or a boxplot."
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#bargraph",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#bargraph",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "Bargraph",
    "text": "Bargraph"
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#histogram",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#histogram",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "Histogram",
    "text": "Histogram\n\n\nRemember the importance of binwidth: demo"
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#boxplot",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#boxplot",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "Boxplot",
    "text": "Boxplot\n\n\nRemember that the whiskers are never allowed to extend beyond 1.5 times the IQR (and recall that the IQR is just the width of the box)."
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#numerical-summaries",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#numerical-summaries",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "Numerical Summaries",
    "text": "Numerical Summaries\n\nWe can also produce numerical summaries of numerical variables.\nMeasures of Central Tendency are different quantities that summarize the “center” of a variable\n\nThere are two main measures of central tendency we discussed: the mean and the median.\n\nThe mean (or arithmetic mean) is a sort of “balancing point”:\n\n\n\n\n\n\n\n\n\n\\[ \\overline{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i \\]"
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#spread",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#spread",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "Spread",
    "text": "Spread\n\nAnother way we could summarize a numerical dataset (i.e. a dataset containing only one variable, one that is numerical) is to describe how “spread out” the values are.\nThe variance is a sort of “average distance of points to the mean”:\n\n\n\n\n\n\n\n\n\n\\[ s_x^2 = \\frac{1}{n - 1} \\sum_{i=1}^{n} (x_i - \\overline{x})^2 \\]\n\n\nThe standard deviation is just the square root of the variance"
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#spread-1",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#spread-1",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "Spread",
    "text": "Spread\n\nThe interquartile range (IQR) is another measure of spread: \\[ \\mathrm{IQR} = Q_3 - Q_1 \\] where \\(Q_1\\) and \\(Q_3\\) denote the first and third quartiles, respectively.\n\nRecall that the \\(p\\)th percentile of a dataset \\(X\\) is the value \\(\\pi_{x, \\ 0.5}\\) such that p% of observations lie to the left of (i.e. are less than) \\(\\pi_{x, \\ 0.5}\\).\n\\(Q_1\\) is the 25th percentile and \\(Q_3\\) is the 75th percentile\n\nThe third measure of spread we discussed is the range: \\[ \\mathrm{range}(X) = \\max\\{x_1, \\cdots, x_n\\} - \\min\\{x_1, \\ \\cdots, \\ x_n\\} \\]"
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#number-summary",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#number-summary",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "5-Number Summary",
    "text": "5-Number Summary\n\nRecall the five number summary, which contains:\n\nThe minimum\nThe first quartile\nThe median\nThe third quartile\nThe maximum\n\nAlso recall how all of these quantities appear on a boxplot!"
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#comparisons-of-variables",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#comparisons-of-variables",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "Comparisons of Variables",
    "text": "Comparisons of Variables\n\nIf we want to compare two variables, there are three cases to consider:\n\nNumerical vs. Numerical\nNumerical vs. Categorical\nCategorical vs. Categorical\n\nWhen comparing two numerical variables, we use a scatterplot\nWhen comparing a numerical variable to a categorical variable, we use a side-by-side boxplot\nWhen comparing two categorical variables, we construct a contingency table"
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#basics-of-probability",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#basics-of-probability",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "Basics of Probability",
    "text": "Basics of Probability\n\nProbability is, in many ways, the language of uncertainty.\nAn experiment is any procedure we can repeat an infinite number of times, where each time we repeat the procedure the same fixed set of “things” can occur\n\nThese “things” are called outcomes\nThe outcome space, denoted \\(\\Omega\\), is the set containing all outcomes associated with a particular experiment.\nEvents are just subset of the outcome space.\n\nWe can express outcome spaces using tables or trees."
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#probability-1",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#probability-1",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "Probability",
    "text": "Probability\n\nProbability is a function that acts on events\n\nNotationally: \\(\\mathbb{P}(E)\\)\n\nThere are two main approaches to computing probabilities:\n\nThe Classical Aproach: if outcomes are equally likely, then for any event \\(E\\) \\[ \\mathbb{P}(E) = \\frac{\\#(E)}{\\#(\\Omega)} \\]\nThe long-run [relative] frequency approach: repeat the experiment an infinite number of times and define \\(\\mathbb{P}(E)\\) to be the proportion of times \\(E\\) occurs"
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#long-run-frequencies-example",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#long-run-frequencies-example",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "Long-Run Frequencies Example",
    "text": "Long-Run Frequencies Example\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nToss\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n\n\n\n\nOutcome\nH\nT\nT\nH\nT\nH\nH\nH\nT\nT\n\n\nRaw freq. of H\n1\n1\n1\n2\n2\n3\n4\n5\n5\n5\n\n\nRel. freq of H\n1/1\n1/2\n1/3\n2/4\n2/5\n3/6\n4/7\n5/8\n5/9\n5/10"
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#set-operations",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#set-operations",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "Set Operations",
    "text": "Set Operations\n\nGiven two events \\(E\\) and \\(F\\), there are several operations we can perform:\n\nComplement: \\(E^\\complement\\); denotes “not \\(E\\)”\nUnion: \\(E \\cup F\\); denotes \\(E\\) or \\(F\\) (or both)\nIntersection: \\(E \\cap F\\); denotes \\(E\\) and \\(F\\)"
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#venn-diagrams",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#venn-diagrams",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "Venn Diagrams",
    "text": "Venn Diagrams\n\n\n\n\n\n \\(A^\\complement\\)  (complement)\n\n\n\n\n\n\n\n\n \\(A \\cap B\\)  (intersection)\n\n\n\n\n\n\n\n\n \\(A \\cup B\\)  (union)"
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#axioms-of-probability",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#axioms-of-probability",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "Axioms of Probability",
    "text": "Axioms of Probability\n\n\\(\\mathbb{P}(E) \\geq 0\\) for any event \\(E\\)\n\\(\\mathbb{P}(\\Omega) = 1\\)\nFor disjoint events \\(E\\) and \\(F\\) (i.e. for \\(E \\cap F = \\varnothing\\)), \\(\\mathbb{P}(E \\cup F) = \\mathbb{P}(E) + \\mathbb{P}(F)\\)"
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#probability-rules",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#probability-rules",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "Probability Rules",
    "text": "Probability Rules\n\nProbability of the Empty Set: \\(\\mathbb{P}(\\varnothing) = 0\\)\nComplement Rule: \\(\\mathbb{P}(E^\\complement) = 1 - \\mathbb{P}(E)\\)\nAddition Rule: \\(\\mathbb{P}(E \\cup F) = \\mathbb{P}(E) + \\mathbb{P}(F) - \\mathbb{P}(E \\cap F)\\)"
  },
  {
    "objectID": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#conditional-probabilities",
    "href": "Pages/Lectures/MT1Rev/mt1_rev_slides.html#conditional-probabilities",
    "title": "PSTAT 5A: Midterm 1 Review",
    "section": "Conditional Probabilities",
    "text": "Conditional Probabilities\n\n\\(\\mathbb{P}(E \\mid F)\\) denotes an “updating” of our beliefs on \\(E\\) in the presence of \\(F\\))\n\nDefinition: \\(\\displaystyle \\mathbb{P}(E \\mid F) = \\frac{\\mathbb{P}(E \\cap F)}{\\mathbb{P}(F)}\\), provided \\(\\mathbb{P}(F) \\neq 0\\)\n\nMultiplication Rule: \\(\\mathbb{P}(E \\cap F) = \\mathbb{P}(E \\mid F) \\cdot \\mathbb{P}(F) = \\mathbb{P}(F \\mid E) \\cdot \\mathbb{P}(E)\\)\nBayes’ Rule: \\(\\displaystyle \\mathbb{P}(E \\mid F) = \\frac{\\mathbb{P}(F \\mid E) \\cdot \\mathbb{P}(E)}{\\mathbb{P}(F)}\\)\n\n\n\n\nIndependence asserts that \\(\\mathbb{P}(E \\mid F) = \\mathbb{P}(E)\\), which in turn implies \\(\\mathbb{P}(F \\mid E) = \\mathbb{P}(F)\\) and \\(\\mathbb{P}(E \\cap F) = \\mathbb{P}(E) \\cdot \\mathbb{P}(F)\\)\n\nNote that \\(\\mathbb{P}(E \\cap F) = \\mathbb{P}(E) \\cdot \\mathbb{P}(F)\\) only when \\(E\\) and \\(F\\) are independent! Otherwise, you have to compute \\(\\mathbb{P}(E \\cap F)\\) using the multiplication rule.\nThe interpretation of independence is that the two events “do not affect each other”"
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#uncertainty",
    "href": "Pages/Lectures/Lecture03/Lec03.html#uncertainty",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Uncertainty",
    "text": "Uncertainty\n\n\nUncertainty surrounds us!\n\n\n\nStatistics is, in many ways, the study of uncertainty.\nProbability is the language of uncertainty; it gives us a way to quantify exactly how this uncertainty factors into our decision making process."
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#experiment",
    "href": "Pages/Lectures/Lecture03/Lec03.html#experiment",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Experiment",
    "text": "Experiment\n\n\nWe begin with the notion of an experiment. In the context of Probability, we have the following definition:\n\n\n\n\n\n\n\n\n\nDefinition\n\n\n\nAn experiment is any procedure that can be repeated an infinite number of times, and each time the procedure is repeated there are a fixed set of things that could occur.\n\n\n\n\n\n\nAn example of an experiment is tossing a coin:\n\nI could go into Storke field and toss a coin an infinite number of times, and each time I toss the coin it will land on either heads or tails."
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#outcome-space",
    "href": "Pages/Lectures/Lecture03/Lec03.html#outcome-space",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Outcome Space",
    "text": "Outcome Space\n\n\nThe things that could occur on each repetition of an experiment are called outcomes.\n\n\n\n\n\n\n\n\n\nDefinition\n\n\n\nThe outcome space of an experiment is the set \\(\\Omega\\) consisting of all outcomes of the experiment.\n\n\n\n\n\n\nFor instance, in the coin tossing example the outcome space is  \\(\\Omega =\\{\\)heads,  tails\\(\\}\\).\nAs an aside: some textbooks/professors refer to the outcome space as the sample space, and use the letter \\(S\\) to denote it.\n\nSo, if you are doing self-study and encounter the term “sample space”, know that it is the same thing as what we are calling the “outcome space”!"
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#worked-out-example",
    "href": "Pages/Lectures/Lecture03/Lec03.html#worked-out-example",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\nLet’s do an example together.\n\n\n\n\n\n\n\n\nWorked-Out Exercise 1\n\n\n\nConsider the experiment of rolling two four-sided dice and recording the faces that appear. What is an appropriate outcome space for this experiment?\n\n\n\n\n\nOn each die roll, we will observe either a \\(1\\), \\(2\\), \\(3\\), or \\(4\\).\nBut, we cannot simply say that our outcome space is \\(\\{1, 2, 3, 4\\}\\) as this does not take into account the fact that we rolled two dice!"
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#now-its-your-turn",
    "href": "Pages/Lectures/Lecture03/Lec03.html#now-its-your-turn",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Now it’s Your Turn!",
    "text": "Now it’s Your Turn!\n\n\n\n\n\n\nExercise 1\n\n\n\nConsider the experiment of tossing a coin, rolling a 4-sided die, and then tossing another coin. What is the outcome space of this experiment?"
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#other-ways-of-describing-outcome-spaces",
    "href": "Pages/Lectures/Lecture03/Lec03.html#other-ways-of-describing-outcome-spaces",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Other Ways of Describing Outcome Spaces",
    "text": "Other Ways of Describing Outcome Spaces\n\nThere are a few other ways we can use to describe the outcome space of an experiment.\nLet’s return to the tossing four dice example from a few slides ago. Another way we could have kept track of the outcomes was by using a table, recording the outcome of the first die roll in the rows and the outcomes of the second in the columns:\n\n\n\n\n\n\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n\n\n1\n\n\n(1, 1)\n\n\n(1, 2)\n\n\n(1, 3)\n\n\n(1, 4)\n\n\n\n\n2\n\n\n(2, 1)\n\n\n(2, 2)\n\n\n(2, 3)\n\n\n(2, 4)\n\n\n\n\n3\n\n\n(3, 1)\n\n\n(3, 2)\n\n\n(3, 3)\n\n\n(3, 4)\n\n\n\n\n4\n\n\n(4, 1)\n\n\n(4, 2)\n\n\n(4, 3)\n\n\n(4, 4)"
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#other-ways-of-describing-outcome-spaces-1",
    "href": "Pages/Lectures/Lecture03/Lec03.html#other-ways-of-describing-outcome-spaces-1",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Other Ways of Describing Outcome Spaces",
    "text": "Other Ways of Describing Outcome Spaces\n\nSo, tables are a good way of keeping track of outcomes.\nBut, they really only work when we have two of something (e.g. two dice, two coins, etc.). What happens if we, for example, toss three coins?\nThis is where tree diagrams can become useful.\n\n\n\n\n\n\n\n\n\ntree_diagram\n\n  \n\nbase\n\no   \n\nH1\n\nH   \n\nbase->H1\n\n    \n\nT1\n\nT   \n\nbase->T1\n\n    \n\nH21\n\nH   \n\nH1->H21\n\n    \n\nT21\n\nT   \n\nH1->T21\n\n    \n\nH22\n\nH   \n\nT1->H22\n\n    \n\nT22\n\nT   \n\nT1->T22\n\n    \n\nH311\n\nH   \n\nH21->H311\n\n    \n\nT311\n\nT   \n\nH21->T311\n\n    \n\nH321\n\nH   \n\nT21->H321\n\n    \n\nT321\n\nT   \n\nT21->T321\n\n    \n\nH312\n\nH   \n\nH22->H312\n\n    \n\nT312\n\nT   \n\nH22->T312\n\n    \n\nH322\n\nH   \n\nT22->H322\n\n    \n\nT322\n\nT   \n\nT22->T322"
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#events",
    "href": "Pages/Lectures/Lecture03/Lec03.html#events",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Events",
    "text": "Events\n\nSometimes, it will be useful to consider quantities that are a bit more complex than single outcomes.\nFor example, consider the experiment of rolling two 4-sided dice. I could ask myself: in how many outcomes does the second die roll result in a higher number than the first?\nThis leads us to the notion of an event.\n\n\n\n\n\n\n\n\nDefinition\n\n\n\nAn event is a subset of the outcome space. In other words, an event is just a set consisting of one or more outcomes."
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#unions-and-intersections",
    "href": "Pages/Lectures/Lecture03/Lec03.html#unions-and-intersections",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Unions and Intersections",
    "text": "Unions and Intersections\n\nRemember how we talked about the union of two sets last week?\nWell, since events are just sets, we can talk about the union of two sets.\n\nIn words, the union corresponds to an “or” statement.\nFor example, let \\(E\\) denote the event “it is raining” and \\(F\\) denote the event “the ground is wet”, then the event \\(E \\cup F\\) would be the event “it is raining or the ground is wet”.\n\nThe intersection of two events (denoted with the \\(\\cap\\) symbol), corresponds to an “and” statement\n\nFor example, if \\(E\\) and \\(F\\) are defined as in the bullet point above, then \\(E \\cap F\\) denotes the event “it is raining and the ground is wet”."
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#complements",
    "href": "Pages/Lectures/Lecture03/Lec03.html#complements",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Complements",
    "text": "Complements\n\nThe complement of an event \\(E\\), denoted \\(E^{\\complement}\\), represents the event “not \\(E\\)”\n\nFor instance, if \\(E\\) again denotes the event “it is raining”, then \\(E^{\\complement}\\) denotes the event “it is not raining”."
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#leadup",
    "href": "Pages/Lectures/Lecture03/Lec03.html#leadup",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Leadup",
    "text": "Leadup\n\nRecall that \\(E \\cap F\\) denotes the event “both \\(E\\) and \\(F\\) occurred.”\nAlso recall that \\(A^{\\complement}\\) denotes “not \\(A\\)”; i.e. “\\(A\\) did not occur”\nAs such, \\((E \\cap F)^{\\complement}\\) denotes the event “it is not the case that both \\(E\\) or \\(F\\) occurred.”\n\nThis means that either \\(E\\) did not occur, or \\(F\\) did not occur (or both).\nMathematically, this is equivalent to \\(E^\\complement \\cup F^\\complement\\).\n\nAs such, it seems we have arrived at the following equality: \\[ (E \\cap F)^{\\complement} = E^\\complement \\cup F^\\complement \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#demorgans-laws",
    "href": "Pages/Lectures/Lecture03/Lec03.html#demorgans-laws",
    "title": "PSTAT 5A: Lecture 03",
    "section": "DeMorgan’s Laws",
    "text": "DeMorgan’s Laws\n\n\nThis is one of what are known as DeMorgan’s Laws.\n\n\n\n\n\n\n\n\n\n\nDeMorgan’s Laws\n\n\n\nGiven two events \\(E\\) and \\(F\\), we have the following:\n\n\\((E \\cap F)^{\\complement} = E^\\complement \\cup F^\\complement\\)\n\\((E \\cup F)^{\\complement} = E^\\complement \\cap F^\\complement\\)\n\n\n\n\n\n\n\n\nWe will not prove these in this class. However, please familiarize yourself with them as they will be incredibly useful!"
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#leadup-1",
    "href": "Pages/Lectures/Lecture03/Lec03.html#leadup-1",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Leadup",
    "text": "Leadup\n\nNow, there is something interesting about the events defined in the previous example.\nThe outcome space of the underlying experiment is \\[ \\Omega = \\{ (H, H), \\ (H, T), \\ (T, H), \\ (T, T)\\} \\] and\n\n\\(A = \\{(H, H), \\ (T, T)\\}\\)\n\\(B = \\{(H, T), \\ (T, H)\\}\\)\n\\(C = \\{(H, H)\\}\\)"
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#probability",
    "href": "Pages/Lectures/Lecture03/Lec03.html#probability",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Probability",
    "text": "Probability\n\nNow, you may note that we have yet to mention the term “probability.”\nTo get a better sense of “probability”, let’s examine how we use the word in everyday speech:\n\n“the chance of rain is 50%”\n“odds of winning big at a Casino is 1%”\n“probability of scoring a 100% on the PSTAT 5A Midterm 1 is 95%”\n\nNotice that “rain”, “winning big at a Casino”, and “scoring 100% on the PSTAT 5A Midterm 1” are all events.\nAs such, “probability” seems to take in an event and spit out a number."
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#probability-1",
    "href": "Pages/Lectures/Lecture03/Lec03.html#probability-1",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Probability",
    "text": "Probability\n\n\nIn other words, we can think of “probability” (or, more accurately, what we refer to as a probability measure) as a function that takes in an event and outputs a number.\n\n\n\nThe symbol we use for a probability measure is \\(\\mathbb{P}\\); i.e. we write \\(\\mathbb{P}(E)\\) to denote “the probability of event \\(E\\)”.\nNow, this doesn’t really tell us how to define \\(\\mathbb{P}(E)\\) for an arbitrary event \\(E\\).\nThere are (roughly) two schools of thought when it comes to defining the probability of an event: the long-run frequency approach, and the classical approach."
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#long-run-frequency-approach",
    "href": "Pages/Lectures/Lecture03/Lec03.html#long-run-frequency-approach",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Long-Run-Frequency Approach",
    "text": "Long-Run-Frequency Approach\n\nThe long-run frequency approach defines the probability of an event \\(E\\) to be the proportion of times \\(E\\) occurs, if the underlying experiment were to be repeated a large number of times.\nTo help us understand the notion of long-run frequencies, let’s go through an example together. Suppose we toss a coin and record whether the outcome lands heads or tails, and further suppose we observe the following tosses:\n\n\n\nH,  T,   T,   H,   T,   H,   H,   H,   T,   T\n\n\n\nTo compute the relative frequency of heads after each toss, we count the number of times we observed heads and divide by the total number of tosses observed."
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#classical-approach",
    "href": "Pages/Lectures/Lecture03/Lec03.html#classical-approach",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Classical Approach",
    "text": "Classical Approach\n\nThe second way to define probabilities is what is known as the classical approach.\nAs an important note: we can only apply the classical approach if we believe all outcomes in our experiment to be equally likely.\n\nExamples of situations in which it is safe to assume equally likely outcomes include: tossing a fair coin some number of times, rolling a fair \\(k\\)-sided die, selecting a card at random from a deck of cards, etc.\n\nIf we make the equally likely outcomes assumption, then the classical approach to probability tells us to define \\(\\mathbb{P}(E)\\) as \\[ \\mathbb{P}(E) = \\frac{\\text{number of ways $E$ can occur}}{\\text{total number of outcomes}} \\]\nSo, for example, if we toss a fair coin once, then the classical approach to probability (which can be used since the coin is fair) states that \\[ \\mathbb{P}(\\texttt{heads}) = \\frac{1}{2} \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#comparisons",
    "href": "Pages/Lectures/Lecture03/Lec03.html#comparisons",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Comparisons",
    "text": "Comparisons\n\nLet’s quickly compare these two approaches to defining the probability of an event.\nThe long-run frequencies definition has the benefit of not requiring the assumption of equally likely outcomes.\n\nHowever, it relies on the (perhaps odd) consideration of considering what happens when we repeat an experiment a large number of times.\n\nThe classical approach does not rely on such considerations, making the definitions it produces perhaps a bit more easily interpretable.\n\nHowever, it crucially requires the assumption of equally likely outcomes."
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#comparisons-1",
    "href": "Pages/Lectures/Lecture03/Lec03.html#comparisons-1",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Comparisons",
    "text": "Comparisons\n\n\nFor the purposes of this class, we won’t be too concerned with defining the probability of an event: in many cases, we will just give you the probability.\n\n\n\nIn situations where we do not provide a probability a priori, there will likely be some key word or phrase that lets you know we are looking for the classical definition.\n\nAgain, important words/phrases to look out for are: fair, at random, etc."
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#axioms-of-probability",
    "href": "Pages/Lectures/Lecture03/Lec03.html#axioms-of-probability",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Axioms of Probability",
    "text": "Axioms of Probability\n\nIt turns out that there are three axioms that a probability measure must satisfy, collectively called the axioms of probability:\n\n\\(\\mathbb{P}(E) \\geq 0\\) for any event \\(E\\)\n\\(\\mathbb{P}(\\Omega) = 1\\)\nFor disjoint events \\(E\\) and \\(F\\), \\(\\mathbb{P}(E \\cup F) = \\mathbb{P}(E) + \\mathbb{P}(F)\\).\n\nIf you are not familiar with the notion of axioms: an axiom is a fundamental “truth” of math, that does not need to be proven.\n\nAxioms are like the building blocks, or base assumptions on which a system of math is predicated."
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#summary",
    "href": "Pages/Lectures/Lecture03/Lec03.html#summary",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Summary",
    "text": "Summary\n\nLet’s quickly summarize the concepts/terms we’ve covered:\n\nExperiment: any procedure that can be repeated an infinite number of times, where each time the procedure is repeated there are a fixed set of outcomes that can occur.\nOutcome Space: the set of all outcomes associated with a particular experiment.\nEvent: a subset of the outcome space\nProbability (measure): a function that takes an event and outputs a number\n\nThese are the basic building blocks of probability.\nWe will now combine them!"
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#notational-reminder",
    "href": "Pages/Lectures/Lecture03/Lec03.html#notational-reminder",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Notational Reminder",
    "text": "Notational Reminder\n\nBefore we go any further, I’d like to stress something:\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIn this class, using proper notation is very important.\n\n\n\n\n\n\n\nFor example, if we have an event \\(E\\) whose probability of occurring is, say, \\(0.5\\), we must write \\(\\mathbb{P}(E) = 0.5\\); it is NOT correct to say \\(E = 0.5\\), or \\(\\mathbb{P} = 0.5\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#the-complement-rule",
    "href": "Pages/Lectures/Lecture03/Lec03.html#the-complement-rule",
    "title": "PSTAT 5A: Lecture 03",
    "section": "The Complement Rule",
    "text": "The Complement Rule\n\nThe first result we will explore is the so-called complement rule.\n\n\n\n\n\n\n\n\n\nThe Complement Rule\n\n\n\nGiven an event \\(E\\), we have \\(\\mathbb{P}(E^\\complement) = 1 - \\mathbb{P}(E)\\)\n\n\n\n\n\n\n\nAs an example: if we roll a fair six-sided die and if \\(E =\\) “rolling a \\(1\\)”, then \\(E^\\complement =\\) “not rolling a \\(1\\)” and \\[\\mathbb{P}(E^\\complement) = 1 - \\mathbb{P}(E) = 1 - 1/6 = \\boxed{5/6}\\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#the-probability-of-the-empty-set",
    "href": "Pages/Lectures/Lecture03/Lec03.html#the-probability-of-the-empty-set",
    "title": "PSTAT 5A: Lecture 03",
    "section": "The Probability of the Empty Set",
    "text": "The Probability of the Empty Set\n\nRecall that the empty set (\\(\\varnothing\\)) is the set containing no elements.\n\n\n\n\n\n\n\n\n\nThe Probability of the Empty Set\n\n\n\n\\(\\mathbb{P}(\\varnothing) = 0\\).\n\n\n\n\n\n\n\nThe “proof” of this is relatively simple: note that \\(\\Omega^\\complement = \\varnothing\\) (the opposite of “everything” is “nothing”); we also know that \\(\\mathbb{P}(\\Omega) = 1\\) so \\[ \\mathbb{P}(\\Omega^\\complement) = \\mathbb{P}(\\varnothing) = 1 - 1 = 0 \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#the-addition-rule",
    "href": "Pages/Lectures/Lecture03/Lec03.html#the-addition-rule",
    "title": "PSTAT 5A: Lecture 03",
    "section": "The Addition Rule",
    "text": "The Addition Rule\n\nThe next result we will explore is the so-called addition rule.\n\n\n\n\n\n\n\n\n\nThe Addition Rule\n\n\n\nGiven events \\(E\\) and \\(F\\), we have \\(\\mathbb{P}(E \\cup F) = \\mathbb{P}(E) + \\mathbb{P}(F) - \\mathbb{P}(E \\cap F)\\)\n\n\n\n\n\n\n\nNote that if \\(E\\) and \\(F\\) are disjoint, then we recover the third axiom of probability!"
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#worked-out-example-1",
    "href": "Pages/Lectures/Lecture03/Lec03.html#worked-out-example-1",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\nWorked-Out Example 2\n\n\n\n\nA recent survey at the Isla Vista Co-Op revealed that 50% of shoppers buy bread, 30% buy jam, and 20% buy both bread and jam.\n\nWhat is the probability that a randomly selected shopper will not purchase jam?\nWhat is the probability that a randomly selected shopper will purchase either bread or jam (or both)?"
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#solution-to-part-a",
    "href": "Pages/Lectures/Lecture03/Lec03.html#solution-to-part-a",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Solution to Part (a)",
    "text": "Solution to Part (a)\n\nLet \\(J\\) denote the event “a randomly selected shopper will purchase jam”.\n\nFrom the problem statement, we have that \\(\\mathbb{P}(J) = 0.3\\)\n\nThe event “a randomly selected shopper will not purchase jam” is given by \\(J^\\complement\\), meaning the quantity we seek is \\(\\mathbb{P}(J^\\complement)\\).\nBy the Complement Rule, we have \\[ \\mathbb{P}(J^\\complement) = 1 - \\mathbb{P}(J) = 1 - 0.3 = \\boxed{0.7 = 70\\%} \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#solution-to-part-b",
    "href": "Pages/Lectures/Lecture03/Lec03.html#solution-to-part-b",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Solution to Part (b)",
    "text": "Solution to Part (b)\n\nLet \\(J\\) be defined as before, and let \\(B\\) denote the event “a randomly selected shopper will purchase bread or jam”\n\nThe first quantity provided in the problem statement tells us that \\(\\mathbb{P}(B) = 0.5\\)\nThe final quantity provided in the problem statement tells us that \\(\\mathbb{P}(B \\cap J) = 0.2\\)\n\nThe event “a randomly selected shopper will purchase either bread or jam” is given by \\(B \\cup J\\), meaning we seek \\(\\mathbb{P}(B \\cup J)\\).\nBy the Addition Rule,\n\n\n\\[\\begin{align*}\n  \\mathbb{P}(B \\cup J)     & = \\mathbb{P}(B) + \\mathbb{P}(J) - \\mathbb{P}(B \\cap J)   \\\\\n  & = 0.5 + 0.3 - 0.2 = \\boxed{0.6 = 60\\%}\n\\end{align*}\\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#general-strategy",
    "href": "Pages/Lectures/Lecture03/Lec03.html#general-strategy",
    "title": "PSTAT 5A: Lecture 03",
    "section": "General Strategy",
    "text": "General Strategy\n\n\n\n\n\n\n\n\nGeneral Strategy for Probability Word Problems\n\n\n\n\nStart by defining events\nNext, translate the information provided to you (through the problem statement) to be in terms of the events you defined above\nThen, identify the quantity you are trying to obtain\nFinally, apply the various probability rules to solve for the desired quantity."
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#time-to-put-everything-together",
    "href": "Pages/Lectures/Lecture03/Lec03.html#time-to-put-everything-together",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Time to Put Everything Together!",
    "text": "Time to Put Everything Together!\n\n\n\n\n\n\nExercise 5\n\n\n\n\nTwo fair six-sided dice are rolled.\n\nWhat is the outcome space of this experiment?\nWhat is the probability that the first die lands on the number 2?\nWhat is the probability that the first die lands on the number 2, or the second die lands on an even number?"
  },
  {
    "objectID": "Pages/Lectures/Lecture03/Lec03.html#lecture-summary-1",
    "href": "Pages/Lectures/Lecture03/Lec03.html#lecture-summary-1",
    "title": "PSTAT 5A: Lecture 03",
    "section": "Lecture Summary",
    "text": "Lecture Summary\n\nToday, we began our introduction to the field of probability.\n\nProbability, loosely speaking, provides us with a way to quantify uncertainty.\n\nWe discussed the notions of experiments, outcomes, outcome spaces, events, and probabilities.\n\nRemember that there are certain tools/diagrams (namely, tables and tree diagrams) that can help us determine the outcome space of an experiment.\n\nWe then discussed three probability rules: the complement rule, the probability of the empty set, and the addition rule.\nNext time, we will start talking about ways to compute the probability of more complex events under the assumption of equally likely outcomes."
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#last-time",
    "href": "Pages/Lectures/Lecture02/Lec02.html#last-time",
    "title": "PSTAT 5A: Lecture 02",
    "section": "Last Time",
    "text": "Last Time\n\nLast time we started discussing how to produce and interpret visual summaries for datasets consisting of only one variable.\n\nWe learned that histograms and boxplots are good visualizers for numerical variables and barplots are good visualizers for categorical data.\n\nBut, as we also saw (in the palmerpenguins dataset), data is usually comprised of several variables.\nA natural question therefore arises: how might we visualize the relationship between multiple variables?"
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#multiple-variables",
    "href": "Pages/Lectures/Lecture02/Lec02.html#multiple-variables",
    "title": "PSTAT 5A: Lecture 02",
    "section": "Multiple Variables",
    "text": "Multiple Variables\n\nPerhaps unsurprisingly, visualizing the relationship between 3 or more variables can be a bit tricky.\nAs such, we will restrict ourselves to comparing only two variables.\nEven if we compare only two variables, three cases arise:\n\nComparing two numerical variables\nComparing one numerical and one categorical variable\nComparing two categorical variables\n\nWe will examine the first two cases above, and save the third for later."
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#two-numerical-variables",
    "href": "Pages/Lectures/Lecture02/Lec02.html#two-numerical-variables",
    "title": "PSTAT 5A: Lecture 02",
    "section": "Two Numerical Variables",
    "text": "Two Numerical Variables\n\nLet’s say we have two variables, and we want to visualize their relationship.\nAs an example, let’s return to the palmerpenguins dataset and compare the bill_length_mm and bill_depth_mm variables. Let’s also restrict ourselves to Gentoo penguins.\n\n\n\n\n   bill_length_mm bill_depth_mm\n 1           46.1          13.2\n 2           50            16.3\n 3           48.7          14.1\n 4           50            15.2\n 5           47.6          14.5\n 6           46.5          13.5\n 7           45.4          14.6\n 8           46.7          15.3\n 9           43.3          13.4\n10           46.8          15.4\n# ℹ 114 more rows"
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#scatterplot",
    "href": "Pages/Lectures/Lecture02/Lec02.html#scatterplot",
    "title": "PSTAT 5A: Lecture 02",
    "section": "Scatterplot",
    "text": "Scatterplot\n\nThis type of visualization is called a scatterplot.\nSpecifically, when comparing two numerical variables of the same length, we generate a scatterplot by plotting each observational unit on a Cartesian coordinate system where the axes are prescribed by the variables in question.\n\n\n\n\n\n\n\n\nResult\n\n\n\nWhen comparing two numerical variables (of the same length), a scatterplot is the best visulization tool."
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#interpreting-scatterplots",
    "href": "Pages/Lectures/Lecture02/Lec02.html#interpreting-scatterplots",
    "title": "PSTAT 5A: Lecture 02",
    "section": "Interpreting Scatterplots",
    "text": "Interpreting Scatterplots\n\n\n\n\nLet’s return to the scatterplot we generated before:\n\n\n\n\n\n\n\n\n\nNotice how as the values of bill_length_mm increase, the corresponding values of bill_depth_mm also increase on average?\n\nThis makes intutive sense: longer bills are probably deeper!"
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#trend",
    "href": "Pages/Lectures/Lecture02/Lec02.html#trend",
    "title": "PSTAT 5A: Lecture 02",
    "section": "Trend",
    "text": "Trend\n\nThis is an example of what we call a trend; specifically, a positive linear trend.\n\nA trend is, loosely speaking, any relationship we observe between the two variables in a scatterplot.\nA trend is said to be linear if a one-unit change in one variable corresponds to a fixed amount of change in the other (we’ll talk about nonlinear trends in a bit)\nA trend is said to be positive (or increasing) if a one-unit increase in one variable corresponds to a one-unit increase in the other."
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#associations",
    "href": "Pages/Lectures/Lecture02/Lec02.html#associations",
    "title": "PSTAT 5A: Lecture 02",
    "section": "Associations",
    "text": "Associations\n\nAnother way to talk about trends is to phrase things in terms of the variables being compared.\n\nFor example, if the scatterplot of two variables displays a positive linear trend, we might say that the two variables have a positive linear association.\nAs a concrete example: bill length and bill depth appear to have a positive linear association, as seen in the scatterplot from a few slides ago."
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#a-numerical-and-a-categorical-variable",
    "href": "Pages/Lectures/Lecture02/Lec02.html#a-numerical-and-a-categorical-variable",
    "title": "PSTAT 5A: Lecture 02",
    "section": "A Numerical and a Categorical Variable",
    "text": "A Numerical and a Categorical Variable\n\nThe final case we will consider today is comparing a numerical variable to a categorical one.\nAs a concrete example, here is a (mock) dataset comprised of the following variables:\n\n\n\n\n\n\n\n\n\nVariable Name\nDescription\n\n\n\n\nstdy_hrs\naverage amount of time (in hrs) a student spent studying for a particular class each week\n\n\nltr_grd\nthe final letter grade (A+, A, A-, etc.) the student received in the class"
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#side-by-side-boxplots",
    "href": "Pages/Lectures/Lecture02/Lec02.html#side-by-side-boxplots",
    "title": "PSTAT 5A: Lecture 02",
    "section": "Side-by-Side Boxplots",
    "text": "Side-by-Side Boxplots\n\n\n\n\n\n\n\nResult\n\n\n\nWhen comparing one numerical and one categorical variable, it is best to visualize their relationship using a side-by-side boxplot.\n\n\n\n\n\n\nThough the notion of trend is slightly different in the context of a side-by-side boxplot, we can still use them to determine relationships.\nFor example, from the plot on the previous slide, we can see that, on average, students who received lower grades tended to study less than those students who received higher grades."
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#causality",
    "href": "Pages/Lectures/Lecture02/Lec02.html#causality",
    "title": "PSTAT 5A: Lecture 02",
    "section": "Causality",
    "text": "Causality\n\nI should make a very important point: identifying trends is not the same thing as identifying causal relationships.\nFor example, the side-by-side boxplot from a few slides ago does not tell us that “studying less causes your grade to decrease”\n\nThere are a lot of other confounding variables that could contribute to the decrease in grade.\n\nWe won’t talk too much about causality in this course, but it is an important thing to be aware of: association is not the same thing as causation!"
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#quantifying-data",
    "href": "Pages/Lectures/Lecture02/Lec02.html#quantifying-data",
    "title": "PSTAT 5A: Lecture 02",
    "section": "Quantifying Data",
    "text": "Quantifying Data\n\nLet’s once again consider a single numerical variable.\nAs a concrete example, we can consider the exam scores variable from the previous slides:\n\n\n\n\n  [1] 88.236 77.348 81.050 74.431 75.083 79.569 74.998 80.099 74.264 83.850\n [11] 89.857 81.427 79.439 84.260 78.565 77.570 78.224 73.780 88.085 79.341\n [21] 80.554 77.317 81.155 83.842 87.051 78.362 81.528 72.148 74.131 78.927\n [31] 75.446 79.791 78.199 90.769 85.640 78.420 83.484 79.045 97.909 86.736\n [41] 73.723 76.973 81.320 79.238 85.803 86.621 85.781 81.844 82.896 80.478\n [51] 75.903 84.565 76.302 83.432 85.448 69.695 81.049 85.575 84.791 82.525\n [61] 78.361 77.803 86.542 84.171 86.103 72.772 78.730 76.189 75.187 79.194\n [71] 77.159 82.048 82.661 84.021 76.008 79.474 79.015 86.992 72.524 76.094\n [81] 78.765 80.623 82.497 75.776 70.614 79.677 81.182 77.943 76.863 85.561\n [91] 89.569 96.695 73.680 77.770 81.584 81.965 78.373 76.295 73.212 79.229\n[101] 87.273 87.364 82.706 83.843 75.864 82.791 82.637 78.685 72.626 69.302\n[111] 93.408 73.189 83.764 77.832 82.803 80.278 94.962 79.616 85.667 82.710\n[121] 86.823 76.656 74.623 71.508 91.131 78.318 81.058 86.239 76.585 85.652\n[131] 77.122 86.036 83.127 83.234 80.746 83.878 75.544 73.780 81.106 85.523"
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#summarizing-data",
    "href": "Pages/Lectures/Lecture02/Lec02.html#summarizing-data",
    "title": "PSTAT 5A: Lecture 02",
    "section": "Summarizing Data",
    "text": "Summarizing Data\n\nThe remainder of today’s lecture will be devoted to finding numerical summaries of a dataset \\(X = \\{x_i\\}_{i=1}^{n}\\).\nThis will lead us to several different summary statistics, which are mathematical quantities that seek to describe different aspects of our data."
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#quantifying-center",
    "href": "Pages/Lectures/Lecture02/Lec02.html#quantifying-center",
    "title": "PSTAT 5A: Lecture 02",
    "section": "Quantifying “Center”",
    "text": "Quantifying “Center”\n\nHere is a very broad question: what is the center of a dataset \\(X = \\{x_i\\}_{i=1}^{n}\\)?\nPerhaps the scores dataset from earlier is a bit too complicated- let’s simplify things and look at the dataset \\[ X = \\{1, 1, 1, 2, 3, 3, 4, 5, 5, 5, 6, 6\\} \\]\nAs a starting point, I can “plot” these points on a number line (to produce what is known as a dotplot):"
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#quantifying-center-1",
    "href": "Pages/Lectures/Lecture02/Lec02.html#quantifying-center-1",
    "title": "PSTAT 5A: Lecture 02",
    "section": "Quantifying “Center”",
    "text": "Quantifying “Center”\n\n\nBack to our question: what is the center of this dataset?\n\n\n\nPerhaps we can think of center as a balancing point. In other words: where should I place a fulcrum to ensure this number line remains balanced?\n\n\n\n\n\n\n\n\n\nWe call this balancing point the arithmetic mean (or just mean, or average, for short), and denote it \\(\\overline{x}\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#the-mean",
    "href": "Pages/Lectures/Lecture02/Lec02.html#the-mean",
    "title": "PSTAT 5A: Lecture 02",
    "section": "The Mean",
    "text": "The Mean\n\n\n\n\n\n\nFormula: The Mean\n\n\n\nGiven a set of data \\(x = \\{x_i\\}_{i=1}^{n}\\), we compute its mean, denoted \\(\\overline{x}\\), using the formula \\[ \\overline{x} = \\frac{1}{n} (x_1 + \\cdots + x_n) \\] which can be equivalently written as \\[ \\overline{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i \\]\n\n\n\n\n\nIn words: we compute the mean by adding up all of the points included in our dataset, and dividing by the total number of points."
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#a-note-on-notation",
    "href": "Pages/Lectures/Lecture02/Lec02.html#a-note-on-notation",
    "title": "PSTAT 5A: Lecture 02",
    "section": "A Note on Notation",
    "text": "A Note on Notation\n\nPerhaps you haven’t seen the notation \\(\\sum_{i=1}^{n} x_i\\) before. Don’t get scared by it! It’s just a shorthand notation for the sum of the points \\(\\{x_1, x_2, \\cdots, x_n\\}\\).\nSo, if it’s easier for you, you can always think of \\((x_1 + \\cdots x_n)\\) in place of \\(\\sum_{i=1}^{n} x_i\\).\nHaving said that, I will often use this notation (called sigma notation, as the symbol \\(\\sum\\) is the capital Greek letter “sigma”) as it saves quite a bit of time in the long run.\n\nI also urge you to familiarze yourself with \\(\\Sigma\\) notation, as I’m sure you will encounter it even beyond this course!"
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#time-for-an-exercise",
    "href": "Pages/Lectures/Lecture02/Lec02.html#time-for-an-exercise",
    "title": "PSTAT 5A: Lecture 02",
    "section": "Time for an Exercise!",
    "text": "Time for an Exercise!\n\n\n\n\n\n\nExercise 1\n\n\n\nCompute the mean of the set \\(B = \\{-1, 0, 1, 1, 2, 4\\}\\). Discuss with your neighbors!"
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#another-one",
    "href": "Pages/Lectures/Lecture02/Lec02.html#another-one",
    "title": "PSTAT 5A: Lecture 02",
    "section": "Another One!",
    "text": "Another One!\n\n\n\n\n\n\nExercise 2\n\n\n\nA collection of \\(n = 42\\) exam scores have an average 50%. Two additional scores of 100% are reported. How does the average of scores change with the addition of these two new scores?"
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#range",
    "href": "Pages/Lectures/Lecture02/Lec02.html#range",
    "title": "PSTAT 5A: Lecture 02",
    "section": "Range",
    "text": "Range\n\nAnother way we can summarize a dataset \\(X = \\{x_i\\}_{i=1}^{n}\\) is to describe how spread out it is.\nOne idea on how we can capture the spread is to say: how far apart is the smallest value from the largest value?\nIndeed, this statistic has a name: the range.\n\n\n\n\n\n\n\n\nFormula: Range\n\n\n\nGiven a set of numbers \\(X = \\{x_1, x_2, \\cdots, x_n\\}\\), we compute the range of \\(X\\) as: \\[ \\mathrm{range}(X) = \\max\\{x_1, \\cdots, x_n\\} - \\min\\{x_1, \\ \\cdots, \\ x_n\\} \\] i.e. the largest value minus the smallest value."
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#leadup",
    "href": "Pages/Lectures/Lecture02/Lec02.html#leadup",
    "title": "PSTAT 5A: Lecture 02",
    "section": "Leadup",
    "text": "Leadup\n\nNow, there is another way to think about spread: suppose we look at the average distance of points from their mean.\nMore specifically: define \\(d_i := x_i - \\overline{x}\\) to be the deviation of the \\(i\\)th point from the mean:"
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#the-variance",
    "href": "Pages/Lectures/Lecture02/Lec02.html#the-variance",
    "title": "PSTAT 5A: Lecture 02",
    "section": "The Variance",
    "text": "The Variance\n\n\nThis is what we call the variance of the set \\(X\\), denoted by \\(s_x^2\\).\n\n\n\n\n\n\n\n\nFormula: Variance, and Standard Deviation\n\n\n\nGiven a set of data \\(X = \\{x_i\\}_{i=1}^{n}\\), we compute the variance of \\(X\\) by \\[ s_x^2 = \\frac{1}{n - 1} \\sum_{i=1}^{n} (x_i - \\overline{x})^2 \\] We define the standard deviation, denoted by \\(s_x\\), to be \\(\\sqrt{s_X^2}\\); i.e. \\[ s_x := \\sqrt{\\frac{1}{n - 1} \\sum_{i=1}^{n} (x_i - \\overline{x})^2} \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#time-for-an-exercise-1",
    "href": "Pages/Lectures/Lecture02/Lec02.html#time-for-an-exercise-1",
    "title": "PSTAT 5A: Lecture 02",
    "section": "Time for an Exercise!",
    "text": "Time for an Exercise!\n\n\n\n\n\n\nExercise 3\n\n\n\nFor the set \\(X = \\{1, 2, 3, 4, 5\\}\\), compute \\(s_x\\). Discuss with your neighbor!"
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#iqr",
    "href": "Pages/Lectures/Lecture02/Lec02.html#iqr",
    "title": "PSTAT 5A: Lecture 02",
    "section": "IQR",
    "text": "IQR\n\nThere is yet another way to quantify the spread of a dataset, and that is what is known as the Interquartile Range (IQR, for short).\n\n\n\n\n\n\n\n\nFormula: The IQR\n\n\n\nGiven a set of data \\(x = \\{x_i\\}_{i=1}^{n}\\), we compute its interquartile range using the formula \\[ \\mathrm{IQR} = Q_3 - Q_1 \\] where \\(Q_1\\) and \\(Q_3\\) denote the first and third quartiles, respectively.\n\n\n\n\n\n\nIn other words, the IQR is just the width of the box in a boxplot!"
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#five-number-summary",
    "href": "Pages/Lectures/Lecture02/Lec02.html#five-number-summary",
    "title": "PSTAT 5A: Lecture 02",
    "section": "Five Number Summary",
    "text": "Five Number Summary\n\nSpeaking of boxplots, there is a set of numbers that occurs frequently when summarizing numerical data, collectively called the five number summary. The elements of the five number summary are:\n\nThe minimum\nThe first quartile\nThe median\nThe third quartile\nThe maximum"
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#time-for-an-exercise-2",
    "href": "Pages/Lectures/Lecture02/Lec02.html#time-for-an-exercise-2",
    "title": "PSTAT 5A: Lecture 02",
    "section": "Time for an Exercise!",
    "text": "Time for an Exercise!\n\n\n\n\n\n\nExercise 4\n\n\n\nConsider a dataset \\(X\\) that has boxplot given by:\n\n\n\n\n\nProvide the five number summary, along with the IQR. Discuss with your Neighbors!"
  },
  {
    "objectID": "Pages/Lectures/Lecture02/Lec02.html#summary",
    "href": "Pages/Lectures/Lecture02/Lec02.html#summary",
    "title": "PSTAT 5A: Lecture 02",
    "section": "Summary",
    "text": "Summary\n\nWe started off by finishing our discussion of data visualizing, identifying ways to visualize the relationship between two variables.\n\nSuch visualizations included: scatterplots and side-by-side boxplots.\nWe also discussed notions of trend.\n\nNext, we discussed various numerical summaries of data.\n\nThese included measures of central tendency (like the mean or the median), along with measures of spread (like the variance, standard deviation, or IQR).\nWe were also introduced to the five number summary, which is closely related to boxplots.\n\nNext time we’ll begin our discussion on Probability, which, as we will see, provides a rigorous way of quantifying uncertainty."
  },
  {
    "objectID": "Pages/Lectures/Lecture05/Lec05.html#leadup",
    "href": "Pages/Lectures/Lecture05/Lec05.html#leadup",
    "title": "PSTAT 5A: Lecture 05",
    "section": "Leadup",
    "text": "Leadup\n\nRemember how, back in Week 1, we discussed ways to compare two variables?\nAt the time, we only considered comparing two numerical variables and comparing one numerical and one categorical variable.\nWhat about comparing two categorical variables?\nAs a concrete example, let’s return to…"
  },
  {
    "objectID": "Pages/Lectures/Lecture05/Lec05.html#penguins-revisited",
    "href": "Pages/Lectures/Lecture05/Lec05.html#penguins-revisited",
    "title": "PSTAT 5A: Lecture 05",
    "section": "Penguins, Revisited",
    "text": "Penguins, Revisited\n\n\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 2 more variables: sex <fct>, year <int>"
  },
  {
    "objectID": "Pages/Lectures/Lecture05/Lec05.html#penguins-subsetted",
    "href": "Pages/Lectures/Lecture05/Lec05.html#penguins-subsetted",
    "title": "PSTAT 5A: Lecture 05",
    "section": "Penguins, Subsetted",
    "text": "Penguins, Subsetted\n\n\n   species island   \n 1 Adelie  Torgersen\n 2 Adelie  Torgersen\n 3 Adelie  Torgersen\n 4 Adelie  Torgersen\n 5 Adelie  Torgersen\n 6 Adelie  Torgersen\n 7 Adelie  Torgersen\n 8 Adelie  Torgersen\n 9 Adelie  Torgersen\n10 Adelie  Torgersen\n# ℹ 334 more rows"
  },
  {
    "objectID": "Pages/Lectures/Lecture05/Lec05.html#some-questions",
    "href": "Pages/Lectures/Lecture05/Lec05.html#some-questions",
    "title": "PSTAT 5A: Lecture 05",
    "section": "Some Questions",
    "text": "Some Questions\n\nHere are some questions we could ask:\n\nHow many Adelie penguins were found on Biscoe island?\nWere any Gentoo penguins found on Torgersen island?\nWhat proportion of Chinstrap penguins were found on Dream island?\n\nThese sorts of questions are very nicely answered by way of what is known as a contingency table:\n\n\n\n\n           \n            Biscoe Dream Torgersen\n  Adelie        44    56        52\n  Chinstrap      0    68         0\n  Gentoo       124     0         0\n\n\n\n\nThus, the answers to the questions above are: “44”, “no”, and “100%”, respectively."
  },
  {
    "objectID": "Pages/Lectures/Lecture05/Lec05.html#why-bring-this-up-now",
    "href": "Pages/Lectures/Lecture05/Lec05.html#why-bring-this-up-now",
    "title": "PSTAT 5A: Lecture 05",
    "section": "Why Bring This Up Now?",
    "text": "Why Bring This Up Now?\n\nYou may be asking yourselves: “why bring this up now? Weren’t we talking about probability?”\nLet’s re-examine the third question we asked on the previous slide: What proportion of Chinstrap penguins were found on Dream island?\nWhat we really did when we answered this was to first restrict ourselves to the row of the contingency table corresponding to Chinstrap penguins, tally up the entries in that row, and then divided the number of penguins that were both Chinstrap and found on Dream Island by the row total."
  },
  {
    "objectID": "Pages/Lectures/Lecture05/Lec05.html#conditional-probability",
    "href": "Pages/Lectures/Lecture05/Lec05.html#conditional-probability",
    "title": "PSTAT 5A: Lecture 05",
    "section": "Conditional Probability",
    "text": "Conditional Probability\n\nThis leads us to the main topic of today’s lecture: conditional probabilities.\n\n\n\n\n\n\n\n\nDefinition\n\n\n\nIf \\(E\\) and \\(F\\) are two events with \\(\\mathbb{P}(E) \\neq 0\\), then we define the probability of \\(E\\) given \\(F\\), notated \\(\\mathbb{P}(E \\mid F)\\), to be \\[ \\mathbb{P}(E \\mid F) = \\frac{\\mathbb{P}(E \\cap F)}{\\mathbb{P}(F)} \\] If \\(\\mathbb{P}(F) = 0\\), then \\(\\mathbb{P}(E \\mid F)\\) is not defined."
  },
  {
    "objectID": "Pages/Lectures/Lecture05/Lec05.html#interpretation",
    "href": "Pages/Lectures/Lecture05/Lec05.html#interpretation",
    "title": "PSTAT 5A: Lecture 05",
    "section": "Interpretation",
    "text": "Interpretation\n\n\\(\\mathbb{P}(E \\mid F)\\) essentially gives us the proportion of \\(F\\) that is explained by \\(E\\).\n\nAs such, another way to think about conditional probabilities is as an “if-then” statement: if \\(F\\) has occurred, what is the probability that \\(E\\) also occurs?\n\nIf we adopt the classical approach to probability, we have \\[\\begin{align*}\n\\mathbb{P}(E \\mid F)    & = \\frac{\\mathbb{P}(E \\cap F)}{\\mathbb{P}(F)}    \\\\\n  & = \\frac{\\left( \\frac{\\#(E \\cap F)}{\\#(\\Omega)} \\right)}{\\left( \\frac{\\#(F)}{\\#(\\Omega)} \\right)} = \\frac{\\#(E \\cap F)}{\\#(F)}\n\\end{align*}\\]\nThis is also why contingency tables are so useful in the context of probability- the numerator above will be an entry in the table, and the denominator will be either a row-sum or a column-sum (depending on how the table was constructed)."
  },
  {
    "objectID": "Pages/Lectures/Lecture05/Lec05.html#worked-out-example",
    "href": "Pages/Lectures/Lecture05/Lec05.html#worked-out-example",
    "title": "PSTAT 5A: Lecture 05",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\nWorked-Out Exercise 1\n\n\n\n75 UCSB students were surveyed about whether they like pineapple on pizza or not. In addition to their pineapple preference, their standing was also recorded.\n\n\n         Standing\nPineapple Freshman Junior Senior Sophomore\n      No        15      6      1        18\n      Yes       10      7      5        13\n\n\nA student is to be randomly selected. If the student is a Freshman, what is the probability that they like pineapple on pizza?\n\n\n\n\n\nAs always, we begin by defining events and notation. Let \\(P =\\) “the student likes pineapple on pizza” and \\(F =\\) “the student is a freshman”. We then seek \\(\\mathbb{P}(P \\mid F)\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture05/Lec05.html#multiplication-rule",
    "href": "Pages/Lectures/Lecture05/Lec05.html#multiplication-rule",
    "title": "PSTAT 5A: Lecture 05",
    "section": "Multiplication Rule",
    "text": "Multiplication Rule\n\nRecall that. provided \\(\\mathbb{P}(F) \\neq 0\\) \\[ \\mathbb{P}(E \\mid F) = \\frac{\\mathbb{P}(E \\cap F)}{\\mathbb{P}(F)} \\]\nWe can multiply both sides of this equation by \\(\\mathbb{P}(F)\\) to obtain the so-called multiplication rule:\n\n\n\n\n\n\n\n\nThe Multiplication Rule\n\n\n\n\\(\\mathbb{P}(E \\cap F) = \\mathbb{P}(E \\mid F) \\cdot \\mathbb{P}(F)\\), for any events \\(E\\) and \\(F\\) with \\(\\mathbb{P}(F) \\neq 0\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture05/Lec05.html#leadup-1",
    "href": "Pages/Lectures/Lecture05/Lec05.html#leadup-1",
    "title": "PSTAT 5A: Lecture 05",
    "section": "Leadup",
    "text": "Leadup\n\nNote that “\\(E\\) and \\(F\\)” is the same as “\\(F\\) and \\(E\\)”.\nThat is: \\(\\mathbb{P}(E \\cap F) = \\mathbb{P}(F \\cap E)\\).\nSo, if we interchange the place of \\(E\\) and \\(F\\) in the multiplication rule, we obtain \\[ \\mathbb{P}(E \\cap F) = \\mathbb{P}(F \\cap E) = \\mathbb{P}(F \\mid E) \\cdot \\mathbb{P}(E) \\]\nThat is to say, we have \\[\\begin{align*}\n\\mathbb{P}(E \\cap F)    & = \\mathbb{P}(E \\mid F) \\cdot \\mathbb{P}(F) = \\mathbb{P}(F \\mid E) \\cdot \\mathbb{P}(E)\n\\end{align*}\\]\nDividing the last equation by \\(\\mathbb{P}(F)\\) yields an important result:"
  },
  {
    "objectID": "Pages/Lectures/Lecture05/Lec05.html#bayes-rule",
    "href": "Pages/Lectures/Lecture05/Lec05.html#bayes-rule",
    "title": "PSTAT 5A: Lecture 05",
    "section": "Bayes’ Rule",
    "text": "Bayes’ Rule\n\n\n\n\n\n\n\nBaeys’ Rule\n\n\n\n\\[ \\mathbb{P}(E \\mid F) = \\frac{\\mathbb{P}(F \\mid E) \\cdot \\mathbb{P}(E)}{\\mathbb{P}(F)} \\] for events \\(E\\) and \\(F\\) with \\(\\mathbb{P}(E) \\neq 0\\) and \\(\\mathbb{P}(F) \\neq 0\\).\n\n\n\n\n\n\nIn a sense, Bayes’ Rule gives us a way to “reverse the order of a conditional”"
  },
  {
    "objectID": "Pages/Lectures/Lecture05/Lec05.html#worked-out-example-1",
    "href": "Pages/Lectures/Lecture05/Lec05.html#worked-out-example-1",
    "title": "PSTAT 5A: Lecture 05",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\nAs an illustration, let’s return to our pineapple-on-pizza contingency table:\n\n\n\n\n         Standing\nPineapple Freshman Junior Senior Sophomore\n      No        15      6      1        18\n      Yes       10      7      5        13\n\n\n\n\nLetting \\(P\\) and \\(F\\) be defined as before, let’s compute \\(\\mathbb{P}(P \\mid F)\\) using Bayes’ Rule.\nWe need to first compute \\(\\mathbb{P}(F \\mid P)\\), which we see to be \\[ \\mathbb{P}(F \\mid P) = \\frac{10}{10 + 7 + 5 + 13} = \\frac{10}{35} \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture05/Lec05.html#hang-in-there",
    "href": "Pages/Lectures/Lecture05/Lec05.html#hang-in-there",
    "title": "PSTAT 5A: Lecture 05",
    "section": "Hang In There!",
    "text": "Hang In There!\n\nAt the moment, it may not seem obvious why Bayes’ Rule is helpful.\n\nIt seems like it just makes more work!\n\nBut, rest assured, we will see a very practical application of Bayes’ Rule in a few slides.\nBefore we do, there’s just one more concept we need to discuss."
  },
  {
    "objectID": "Pages/Lectures/Lecture05/Lec05.html#leadup-2",
    "href": "Pages/Lectures/Lecture05/Lec05.html#leadup-2",
    "title": "PSTAT 5A: Lecture 05",
    "section": "Leadup",
    "text": "Leadup\n\nConsider an event \\(F\\), and another event \\(E\\).\nIf \\(F\\) happened, it could have happened along with \\(E\\) or it could have happened along with not-\\(E\\).\nThat is, \\[ F = (F \\cap E) \\cup (F \\cap E^\\complement)\\]\nNow, let’s take the probability of both sides. Since the events on the RHS are disjoint, the probability on the RHS just becomes a sum of probabilities: \\[ \\mathbb{P}(F) = \\mathbb{P}(F \\cap E) + \\mathbb{P}(F \\cap E^\\complement) \\]\nFinally, we apply the Multiplication Rule to the probabilities on the RHS to obtain \\[ \\mathbb{P}(F) = \\mathbb{P}(F \\mid E) \\cdot \\mathbb{P}(E) + \\mathbb{P}(F \\mid E^\\complement) \\cdot \\mathbb{P}(E^\\complement) \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture05/Lec05.html#law-of-total-probability",
    "href": "Pages/Lectures/Lecture05/Lec05.html#law-of-total-probability",
    "title": "PSTAT 5A: Lecture 05",
    "section": "Law of Total Probability",
    "text": "Law of Total Probability\n\n\n\n\n\n\n\nThe Law of Total Probability\n\n\n\nGiven two events \\(E\\) and \\(F\\) with \\(\\mathbb{P}(E) \\neq 0\\) and \\(\\mathbb{P}(F) \\neq 0\\), we have \\[ \\mathbb{P}(F) = \\mathbb{P}(F \\mid E) \\cdot \\mathbb{P}(E) + \\mathbb{P}(F \\mid E^\\complement) \\cdot \\mathbb{P}(E^\\complement) \\]\n\n\n\n\n\n\nThis is often useful in the context of a Bayes’ Rule problem."
  },
  {
    "objectID": "Pages/Lectures/Lecture05/Lec05.html#worked-out-example-2",
    "href": "Pages/Lectures/Lecture05/Lec05.html#worked-out-example-2",
    "title": "PSTAT 5A: Lecture 05",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\nAlright, let’s get down to business and tackle a slightly more real-world problem.\n\n\n\n\n\n\n\n\nWorked-Out Exercise 1\n\n\n\nIt is known that a particular disease affects 5% of the population. There exists a test for this disease, but it is not perfect: there is a 10% chance it will return a “negative” result for a person who is actually infected, and there is a 8% chance it will return a “positive” result for a person who is actually healthy.\n\nArasha has taken a test for the disease, and it has indicated a “positive” result. What is the probability that Arasha actually has the disease?"
  },
  {
    "objectID": "Pages/Lectures/Lecture05/Lec05.html#step-1-define-events",
    "href": "Pages/Lectures/Lecture05/Lec05.html#step-1-define-events",
    "title": "PSTAT 5A: Lecture 05",
    "section": "Step 1: Define Events",
    "text": "Step 1: Define Events\n\nAs always, we start by defining events.\nLet + denote “the test returns a positive result” and let \\(D\\) denote “Arasha actually has the disease.”\nFirst of all, note that we are not interested in simply finding \\(\\mathbb{P}(D)\\); rather, we are interested in finding \\(\\mathbb{P}(D \\mid +)\\).\n\nThis is because Arasha has already been tested and received a positive result; this is information we need to incorporate into our beliefs!"
  },
  {
    "objectID": "Pages/Lectures/Lecture05/Lec05.html#step-2-translate-the-information",
    "href": "Pages/Lectures/Lecture05/Lec05.html#step-2-translate-the-information",
    "title": "PSTAT 5A: Lecture 05",
    "section": "Step 2: Translate the Information",
    "text": "Step 2: Translate the Information\n\nWith our events from Step 1, we now turn our attention to translating the information provided in the problem.\nSince there is a \\(10\\%\\) chance that the test returns a negative result given that a person actually has the disease, we have \\[ \\mathbb{P}(+^\\complement \\mid D) = 0.1 \\]\nAdditionally, we are told that there is an \\(8\\%\\) chance that the test returns a positive result given that a person does not have disease, we have \\[ \\mathbb{P}(+ \\mid D^\\complement) = 0.08 \\]\nFinally, we are told that 5% of the population has the disease; hence, \\[ \\mathbb{P}(D) = 0.05 \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture05/Lec05.html#step-3-translate-the-information",
    "href": "Pages/Lectures/Lecture05/Lec05.html#step-3-translate-the-information",
    "title": "PSTAT 5A: Lecture 05",
    "section": "Step 3: Translate the Information",
    "text": "Step 3: Translate the Information\n\nBut wait- there’s more!\nGiven that a person has the disease, they will either test positive or test negative.\n\nWhat that means is that \\[ \\mathbb{P}(+ \\mid D) = 1 - \\mathbb{P}(+^\\complement \\mid D) = 1 - 0.1 = 0.9 \\]\nThink of this as a modified complement rule\n\nSimilarly, \\[ \\mathbb{P}(+^\\complement \\mid D^\\complement) = 1 - \\mathbb{P}(+ \\mid D^\\complement) = 1 - 0.08 = 0.92 \\]\nAdditionally, \\[ \\mathbb{P}(D^\\complement) =  1 - \\mathbb{P}(D) = 1 - 0.05 = 0.95 \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture05/Lec05.html#step-2-translate-the-information-1",
    "href": "Pages/Lectures/Lecture05/Lec05.html#step-2-translate-the-information-1",
    "title": "PSTAT 5A: Lecture 05",
    "section": "Step 2: Translate the Information",
    "text": "Step 2: Translate the Information\n\nSo, here’s a summary of everything we know, just from the problem statement: \\[\\begin{align*}\n\\mathbb{P}(+ \\mid D) = 0.9    & \\hspace{15mm} \\mathbb{P}(+^\\complement \\mid D) = 0.1    \\\\\n\\mathbb{P}(+ \\mid D^\\complement) = 0.08   & \\hspace{15mm}  \\mathbb{P}(P^\\complement \\mid D^\\complement) = 0.92   \\\\\n\\mathbb{P}(D) = 0.05    & \\hspace{15mm}  \\mathbb{P}(D^\\complement) = 0.95\n\\end{align*}\\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture05/Lec05.html#step-3-solve-the-problem",
    "href": "Pages/Lectures/Lecture05/Lec05.html#step-3-solve-the-problem",
    "title": "PSTAT 5A: Lecture 05",
    "section": "Step 3: Solve the Problem",
    "text": "Step 3: Solve the Problem\n\nNow we are in a position to begin solving the problem.\nRecall that we seek \\(\\mathbb{P}(D \\mid +)\\).\nBut, we only have information on \\(\\mathbb{P}(+ \\mid D)\\).\nAny ideas what rule/tool we should use?\n\nThat’s right; Bayes’ Rule!\n\nWe use Bayes’ Rule to write \\[ \\mathbb{P}(D \\mid +) = \\frac{\\mathbb{P}(+ \\mid D) \\cdot \\mathbb{P}(D)}{\\mathbb{P}(+)} \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture05/Lec05.html#step-3-solve-the-problem-1",
    "href": "Pages/Lectures/Lecture05/Lec05.html#step-3-solve-the-problem-1",
    "title": "PSTAT 5A: Lecture 05",
    "section": "Step 3: Solve the Problem",
    "text": "Step 3: Solve the Problem\n\nDo we have \\(\\mathbb{P}(+)?\\)\n\nNo…\nBut how can we get it?\nYup- Law of Total Probability!\n\nWe use the Law of Total Probability to write\n\n\n\\[\\begin{align*}\n  \\mathbb{P}(+)   & = \\mathbb{P}(+ \\mid D) \\cdot \\mathbb{P}(D) + \\mathbb{P}(+ \\mid D^\\complement) \\cdot \\mathbb{P}(D^\\complement)    \\\\\n    & = (0.9) \\cdot (0.05) + (0.08) \\cdot (0.95) = 0.121\n\\end{align*}\\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture05/Lec05.html#step-3-solve-the-problem-2",
    "href": "Pages/Lectures/Lecture05/Lec05.html#step-3-solve-the-problem-2",
    "title": "PSTAT 5A: Lecture 05",
    "section": "Step 3: Solve the Problem",
    "text": "Step 3: Solve the Problem\n\nFinally, putting everything together:\n\n\n\\[\\begin{align*}\n  \\mathbb{P}(D \\mid +)    & = \\frac{\\mathbb{P}(+ \\mid D) \\cdot \\mathbb{P}(D)}{\\mathbb{P}(+)}   \\\\\n    & = \\frac{(0.9) \\cdot (0.05)}{0.121} \\boxed{\\approx 37.19\\%}\n\\end{align*}\\]\n\n\nIf that seems low… you’re right! But, it is actually in line with the problem- the test for the disease is pretty bad, considering how often it gets things wrong. This is why this probability is low- because the test is so bad, we cannot be confident that Arasha actually has the disease, even though she tested positive!"
  },
  {
    "objectID": "Pages/Lectures/Lecture05/Lec05.html#some-terminology",
    "href": "Pages/Lectures/Lecture05/Lec05.html#some-terminology",
    "title": "PSTAT 5A: Lecture 05",
    "section": "Some Terminology",
    "text": "Some Terminology\n\nBy the way, there’s some terminology I’d like to quickly introduce to make our lives easier going forward.\nThe False Positive Rate of a test is the proportion of times it returns a “positive” result, when the truth is actually “negative”.\n\nSo, in the context of epidemiology, the false positive rate of a test is the proportion of times it says someone has a disease when they do not actually have the disease.\n\nAnalogously, the False Negative Rate of a test is the proportion of times it returns a “negative” result, when the truth is actually “positive”.\n\nSo, in the context of epidemiology, the false negative rate of a test is the proportion of times it says someone does not have a disease when they do actually have the disease."
  },
  {
    "objectID": "Pages/Lectures/Lecture05/Lec05.html#a-preview-of-hw",
    "href": "Pages/Lectures/Lecture05/Lec05.html#a-preview-of-hw",
    "title": "PSTAT 5A: Lecture 05",
    "section": "A Preview of HW",
    "text": "A Preview of HW\n\nThere is one very important topic which I decided to put on your homework, so as to not make today’s lecture denser than it already is.\n\nIt is called “Independence”, and is a crucial part of probability!\n\nSpeaking of the next homework- please remember that there will be a Homework 3 released tomorrow (Wednesday 4/19) and will be due MONDAY by 11:59pm.\n\nIt will contain some review problems as well!"
  },
  {
    "objectID": "Pages/Lectures/Lecture05/Lec05.html#some-problems-to-think-on",
    "href": "Pages/Lectures/Lecture05/Lec05.html#some-problems-to-think-on",
    "title": "PSTAT 5A: Lecture 05",
    "section": "Some Problems To Think On:",
    "text": "Some Problems To Think On:\n\n\n\n\n\n\nExercise 1\n\n\n\n\nA recent survey interviewed several UCSB students about their pets. The following data was collected:\n\n\n             Animal\nAdopted       Bunny Cat Dog Hamster\n  Adopted         3   5   8       4\n  Not Adopted     1   5   7       7\n\n\n\nIf a pet is to be selected at random, what is the probability that it is either a cat or adopted?\nA pet is selected at random: what is the probability that it is an adopted dog?\nA pet is selected at random: if it is a dog, what is the probability that it was adopted?"
  },
  {
    "objectID": "Pages/Lectures/Lecture18/Lec18.html#scatterplots-and-trends",
    "href": "Pages/Lectures/Lecture18/Lec18.html#scatterplots-and-trends",
    "title": "PSTAT 5A: Lecture 18",
    "section": "Scatterplots and Trends",
    "text": "Scatterplots and Trends\n\nRecall, from Lecture 2, that the best type of plot to visualize the relationship between to numerical variables is a scatterplot.\nBased on the scatterplot, we can determine whether or not the two variables have an association (a.k.a. trend) or not.\nAssociations can be positive or negative, and linear or nonlinear (or, not present at all)"
  },
  {
    "objectID": "Pages/Lectures/Lecture18/Lec18.html#no-relationship",
    "href": "Pages/Lectures/Lecture18/Lec18.html#no-relationship",
    "title": "PSTAT 5A: Lecture 18",
    "section": "No Relationship",
    "text": "No Relationship\n\nSometimes, two variables will have no relationship at all:"
  },
  {
    "objectID": "Pages/Lectures/Lecture18/Lec18.html#strength-of-a-relationship",
    "href": "Pages/Lectures/Lecture18/Lec18.html#strength-of-a-relationship",
    "title": "PSTAT 5A: Lecture 18",
    "section": "Strength of a Relationship",
    "text": "Strength of a Relationship\n\nThere is another thing to be aware of.\nFor example, consider the following two scatterplots:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBoth scatterplots display a positive linear trend. However, the relationship between Y2 and X2 seems to be “stronger” than the relationship between Y1 and X1, does it not?"
  },
  {
    "objectID": "Pages/Lectures/Lecture18/Lec18.html#correlation-coefficient",
    "href": "Pages/Lectures/Lecture18/Lec18.html#correlation-coefficient",
    "title": "PSTAT 5A: Lecture 18",
    "section": "Correlation Coefficient",
    "text": "Correlation Coefficient\n\nUltimately, we would like to develop a mathematical metric to quantify not only the relationship between two variables, but also the strength of the relationship between these two variables.\nThis quantity is referred to as the correlation coefficient.\nNow, it turns out there are actually a few different correlation coefficients out there. The one we will use in this class (and one of the metrics that is very widely used by statisticians) is called Pearson’s Correlation Coefficient, or often just Pearson’s r (as we use the letter r to denote it.)"
  },
  {
    "objectID": "Pages/Lectures/Lecture18/Lec18.html#pearsons-r",
    "href": "Pages/Lectures/Lecture18/Lec18.html#pearsons-r",
    "title": "PSTAT 5A: Lecture 18",
    "section": "Pearson’s r",
    "text": "Pearson’s r\n\nGiven two sets \\(X = \\{y_i\\}_{i=1}^{n}\\) and \\(Y = \\{y_i\\}_{i=1}^{n}\\) (note that we require the two sets to have the same number of elements!), we compute r using the formula \\[ r = \\frac{1}{n - 1} \\sum_{i=1}^{n} \\left( \\frac{x_i - \\overline{x}}{s_X} \\right) \\left( \\frac{y_i - \\overline{y}}{s_Y} \\right)  \\] where:\n\n\\(\\overline{x}\\) and \\(\\overline{y}\\) denote the sample means of \\(X\\) and \\(Y\\), respectively\n\\(s_X\\) and \\(s_Y\\) denote the sample standard deviations of \\(X\\) and \\(Y\\), respectively."
  },
  {
    "objectID": "Pages/Lectures/Lecture18/Lec18.html#example",
    "href": "Pages/Lectures/Lecture18/Lec18.html#example",
    "title": "PSTAT 5A: Lecture 18",
    "section": "Example",
    "text": "Example\n\nI find it useful to sometimes consider extreme cases, and ensure that the math matches up with our intuition.\nFor example, consider the sets \\(X = \\{1, 2, 3\\}\\) and \\(Y = \\{1, 2, 3\\}\\).\nFrom a scatterplot, I think we would all agree that \\(X\\) and \\(Y\\) have a positive linear relationship, and that the relationship is very strong!"
  },
  {
    "objectID": "Pages/Lectures/Lecture18/Lec18.html#example-1",
    "href": "Pages/Lectures/Lecture18/Lec18.html#example-1",
    "title": "PSTAT 5A: Lecture 18",
    "section": "Example",
    "text": "Example\n\nIndeed, \\(\\overline{x} = 2 = \\overline{y}\\) and \\(s_X = 1 = s_Y\\), meaning \\[\\begin{align*}\nr   & = \\frac{1}{3 - 1} \\left[ \\left( \\frac{1 - 2}{1} \\right) \\left( \\frac{1 - 2}{1} \\right)  + \\left( \\frac{2 - 2}{1} \\right) \\left( \\frac{2 - 2}{1} \\right)  \\right.   \\\\\n  & \\hspace{45mm} \\left. +  \\left( \\frac{3 - 2}{1} \\right) \\left( \\frac{3 - 2}{1} \\right)  \\right] \\\\\n  & = \\frac{1}{2} \\left[ 1 + 0 + 1 \\right] = \\boxed{1}\n\\end{align*}\\]\nIt turns out, r will always be between \\(-1\\) and \\(1\\), inclusive, regardless of what two sets we are comparing!"
  },
  {
    "objectID": "Pages/Lectures/Lecture18/Lec18.html#interpretation",
    "href": "Pages/Lectures/Lecture18/Lec18.html#interpretation",
    "title": "PSTAT 5A: Lecture 18",
    "section": "Interpretation",
    "text": "Interpretation\n\nSo, here is how we interpret the value of r.\n\nThe sign of r (i.e. whether it is positive or negative) indicates whether or not the linear association between the two variables is positive or negative.\nThe magnitude of r indicates how strong the linear relationship between the two variables is, with magnitudes close to \\(1\\) or \\(-1\\) indicating very strong linear relationships.\nAn r value of 0 indicates no linear relationship between the variables."
  },
  {
    "objectID": "Pages/Lectures/Lecture18/Lec18.html#important-distinction",
    "href": "Pages/Lectures/Lecture18/Lec18.html#important-distinction",
    "title": "PSTAT 5A: Lecture 18",
    "section": "Important Distinction",
    "text": "Important Distinction\n\nNow, something that is very important to mention is that r only quantifies linear relationships- it is very bad at quantifying nonlinear relationships.\nFor example, consider the following scatterplot:"
  },
  {
    "objectID": "Pages/Lectures/Lecture18/Lec18.html#important-distinction-1",
    "href": "Pages/Lectures/Lecture18/Lec18.html#important-distinction-1",
    "title": "PSTAT 5A: Lecture 18",
    "section": "Important Distinction",
    "text": "Important Distinction\n\nI think we would all agree that Y and X have a fairly strong relationship.\nHowever, the correlation between Y and X is actually only 0.1953333!\nSo, again- r should only be used as a determination of the strength of linear trends, not nonlinear trends."
  },
  {
    "objectID": "Pages/Lectures/Lecture18/Lec18.html#your-turn",
    "href": "Pages/Lectures/Lecture18/Lec18.html#your-turn",
    "title": "PSTAT 5A: Lecture 18",
    "section": "Your Turn!",
    "text": "Your Turn!"
  },
  {
    "objectID": "Pages/Lectures/Lecture18/Lec18.html#your-turn-1",
    "href": "Pages/Lectures/Lecture18/Lec18.html#your-turn-1",
    "title": "PSTAT 5A: Lecture 18",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\n\n\n\n\n\nExercise 1\n\n\n\n\nCompute the correlation between the following two sets of numbers: \\[\\begin{align*}\n  \\boldsymbol{x}    & = \\{-1, \\ 0, \\ 1\\}    \\\\\n  \\boldsymbol{y}    & = \\{1, \\ 2, \\ 0\\}\n\\end{align*}\\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture18/Lec18.html#leadup",
    "href": "Pages/Lectures/Lecture18/Lec18.html#leadup",
    "title": "PSTAT 5A: Lecture 18",
    "section": "Leadup",
    "text": "Leadup\n\nThere is another thing to note about correlation.\nLet’s see this by way of an example: consider the following two scatterplots:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBoth cor(X, Y1) and cor(X, Y2) are equal to 1, despite the fact that a one unit increase in x corresponds to a different unit increase in y1 as opposed to y2."
  },
  {
    "objectID": "Pages/Lectures/Lecture18/Lec18.html#leadup-1",
    "href": "Pages/Lectures/Lecture18/Lec18.html#leadup-1",
    "title": "PSTAT 5A: Lecture 18",
    "section": "Leadup",
    "text": "Leadup\n\nSo, don’t be fooled- the magnitude of r says nothing about how a one-unit increase in x translates to a change in y!\n\nAgain, the magnitude of r only tells us how strongly the two variables are related.\n\nA natural question that arises is then: how can we specify how a change in x translates to a change in y?"
  },
  {
    "objectID": "Pages/Lectures/Lecture18/Lec18.html#leadup-2",
    "href": "Pages/Lectures/Lecture18/Lec18.html#leadup-2",
    "title": "PSTAT 5A: Lecture 18",
    "section": "Leadup",
    "text": "Leadup\n\nTo help ground our discussion, let’s think in terms of height and weight. That is, let x denote height and y denote weight.\nWe would certainly expect some sort of positive association between height and weight (taller people tend to weigh slightly more than shorter people).\nBut, if we were to take a series of observations on height and weight, and plot these observations on a scatterplot, we would not get data that is perfectly linear.\nRather, we can imagine that there does exist some true linear “fit” (or “trend”) between height and weight, but randomness would inject some “error” into our data causing our data to be modeled as something like \\[ \\texttt{weight} = f(\\texttt{height}) + \\texttt{noise} \\] where, in this case, we would expect the function \\(f()\\) to be a linear function."
  },
  {
    "objectID": "Pages/Lectures/Lecture18/Lec18.html#leadup-3",
    "href": "Pages/Lectures/Lecture18/Lec18.html#leadup-3",
    "title": "PSTAT 5A: Lecture 18",
    "section": "Leadup",
    "text": "Leadup"
  },
  {
    "objectID": "Pages/Lectures/Lecture18/Lec18.html#leadup-4",
    "href": "Pages/Lectures/Lecture18/Lec18.html#leadup-4",
    "title": "PSTAT 5A: Lecture 18",
    "section": "Leadup",
    "text": "Leadup\n\n\nHere, the red line represents the true relationship between height and weight, and any deviations from the line are assumed to be due to chance."
  },
  {
    "objectID": "Pages/Lectures/Lecture18/Lec18.html#leadup-5",
    "href": "Pages/Lectures/Lecture18/Lec18.html#leadup-5",
    "title": "PSTAT 5A: Lecture 18",
    "section": "Leadup",
    "text": "Leadup\n\nRecall that a line is specified by an intercept and a slope. Therefore, since we are assuming a linear relationship between height and weight, our model can be expressed as \\[ \\texttt{weight} = \\beta_0 + \\beta_1 \\cdot \\texttt{height} + \\texttt{noise} \\]\nIn this way, we can see that \\(\\beta_0\\) and \\(\\beta_1\\) are effectively population parameters.\nOur next goal will be to find suitable estimators, \\(\\widehat{\\beta_0}\\) and \\(\\widehat{\\beta_1}\\), of \\(\\beta_0\\) and \\(\\beta_1\\), respectively."
  },
  {
    "objectID": "Pages/Lectures/Lecture18/Lec18.html#model",
    "href": "Pages/Lectures/Lecture18/Lec18.html#model",
    "title": "PSTAT 5A: Lecture 18",
    "section": "Model",
    "text": "Model\n\nIn general, the goal of regression is to quantify the relationship between two variables, x and y.\nWe call y the response variable and x the explanatory variable.\n\nSo, for example, in our height and weight example from above, weight was the response variable and height was the explanatory variable.\n\nOur model (assuming a linear relationship between x and y), is then \\[ \\texttt{y} = \\beta_0 + \\beta_1 \\cdot \\texttt{x} + \\texttt{Noise} \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture18/Lec18.html#goals",
    "href": "Pages/Lectures/Lecture18/Lec18.html#goals",
    "title": "PSTAT 5A: Lecture 18",
    "section": "Goals",
    "text": "Goals\n\nHere’s a visual way of thinking about what I said on the previous slide. Consider the following scatterplot:"
  },
  {
    "objectID": "Pages/Lectures/Lecture18/Lec18.html#goals-1",
    "href": "Pages/Lectures/Lecture18/Lec18.html#goals-1",
    "title": "PSTAT 5A: Lecture 18",
    "section": "Goals",
    "text": "Goals\n\n\nWe are assuming that there exists some true linear relationship (i.e. some “fit”) between Y and X. But, because of natural variability due to randomness, we cannot figure out exactly what the true relationship is."
  },
  {
    "objectID": "Pages/Lectures/Lecture18/Lec18.html#goals-2",
    "href": "Pages/Lectures/Lecture18/Lec18.html#goals-2",
    "title": "PSTAT 5A: Lecture 18",
    "section": "Goals",
    "text": "Goals\n\n\nFinding the “best” estimate of the fit is, therefore, akin to finding the line that “best” fits the data."
  },
  {
    "objectID": "Pages/Lectures/Lecture18/Lec18.html#line-of-best-fit",
    "href": "Pages/Lectures/Lecture18/Lec18.html#line-of-best-fit",
    "title": "PSTAT 5A: Lecture 18",
    "section": "Line of Best Fit",
    "text": "Line of Best Fit\n\nNow, if we are to find the line that best fits the data, we first need to quantify what we mean by “best”.\nHere is one idea: consider minimizing the average distance from the datapoints to the line.\nAs a measure of “average distance from the points to the line”, we will use the so-called residual sum of squares (often abbreviated as RSS)."
  },
  {
    "objectID": "Pages/Lectures/Lecture18/Lec18.html#residuals",
    "href": "Pages/Lectures/Lecture18/Lec18.html#residuals",
    "title": "PSTAT 5A: Lecture 18",
    "section": "Residuals",
    "text": "Residuals\n\nThe ith residual is defined to be the quantity \\(e_i\\) below:\n\n\n\n\n\nRSS is then just \\(\\displaystyle \\mathrm{RSS} = \\sum_{i=1}^{n} e_i^2\\)"
  },
  {
    "objectID": "Pages/Lectures/Lecture18/Lec18.html#results",
    "href": "Pages/Lectures/Lecture18/Lec18.html#results",
    "title": "PSTAT 5A: Lecture 18",
    "section": "Results",
    "text": "Results\n\nIt turns out, using a bit of Calculus, the estimators we seek (i.e. the ones that minimize the RSS) are \\[\\begin{align*}\n\\widehat{\\beta_1}   & = \\frac{\\sum\\limits_{i=1}^{n} (x_i - \\overline{x})(y_i - \\overline{y})}{\\sum\\limits_{i=1}^{n} (x_i - \\overline{x})^2}    \\\\\n\\widehat{\\beta_0}   & = \\overline{y} - \\widehat{\\beta_1} \\overline{x}\n\\end{align*}\\]\nThese are what are known as the ordinary least squares estimators of \\(\\beta_0\\) and \\(\\beta_1\\), and the line \\(\\widehat{\\beta_0} + \\widehat{\\beta_1} x\\) is called the least-squares regression line.\nPerhaps an example may illustrate what I am talking about."
  },
  {
    "objectID": "Pages/Lectures/Lecture18/Lec18.html#example-2",
    "href": "Pages/Lectures/Lecture18/Lec18.html#example-2",
    "title": "PSTAT 5A: Lecture 18",
    "section": "Example",
    "text": "Example"
  },
  {
    "objectID": "Pages/Lectures/Lecture18/Lec18.html#example-3",
    "href": "Pages/Lectures/Lecture18/Lec18.html#example-3",
    "title": "PSTAT 5A: Lecture 18",
    "section": "Example",
    "text": "Example\n\n\n\\(\\widehat{\\beta_0} =\\) -0.2056061; \\(\\widehat{\\beta_1} =\\) -2.1049432.\nI.e. the equation of the line in blue is -0.2056061 + -2.1049432 * x."
  },
  {
    "objectID": "Pages/Lectures/Lecture18/Lec18.html#fitted-values",
    "href": "Pages/Lectures/Lecture18/Lec18.html#fitted-values",
    "title": "PSTAT 5A: Lecture 18",
    "section": "Fitted Values",
    "text": "Fitted Values\n\nLet’s return to our cartoon picture of OLS regression:"
  },
  {
    "objectID": "Pages/Lectures/Lecture18/Lec18.html#fitted-values-1",
    "href": "Pages/Lectures/Lecture18/Lec18.html#fitted-values-1",
    "title": "PSTAT 5A: Lecture 18",
    "section": "Fitted Values",
    "text": "Fitted Values\n\nNotice that each point in our dataset (i.e. the blue points) have a corresponding point on the OLS regression line:"
  },
  {
    "objectID": "Pages/Lectures/Lecture18/Lec18.html#fitted-values-2",
    "href": "Pages/Lectures/Lecture18/Lec18.html#fitted-values-2",
    "title": "PSTAT 5A: Lecture 18",
    "section": "Fitted Values",
    "text": "Fitted Values\n\nThese points are referred to as fitted values; the y-values of the fitted values are denoted as \\(\\widehat{y}_i\\).\nIn this way, the OLS regression line is commonly written as a relationship between the fitted values and the x-values: \\[ \\widehat{y} = \\widehat{\\beta_0} + \\widehat{\\beta_1} x \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture18/Lec18.html#back-to-height-and-weight",
    "href": "Pages/Lectures/Lecture18/Lec18.html#back-to-height-and-weight",
    "title": "PSTAT 5A: Lecture 18",
    "section": "Back to height and weight",
    "text": "Back to height and weight\n\nBefore we work through the math once, let’s apply this technique to the height and weight data from before."
  },
  {
    "objectID": "Pages/Lectures/Lecture18/Lec18.html#back-to-height-and-weight-1",
    "href": "Pages/Lectures/Lecture18/Lec18.html#back-to-height-and-weight-1",
    "title": "PSTAT 5A: Lecture 18",
    "section": "Back to height and weight",
    "text": "Back to height and weight\n\nUsing a computer software, the OLS regression line can be found to be:\n\n\n\n\n\n\n\n\n\n\n\n\nSpecifically, \\(\\widehat{\\beta_0} =\\) 3.366744 and \\(\\widehat{\\beta_1} =\\) 0.9790114\n\n\n\n\nWe will return to the notion of fitted values in a bit."
  },
  {
    "objectID": "Pages/Lectures/Lecture18/Lec18.html#back-to-height-and-weight-2",
    "href": "Pages/Lectures/Lecture18/Lec18.html#back-to-height-and-weight-2",
    "title": "PSTAT 5A: Lecture 18",
    "section": "Back to height and weight",
    "text": "Back to height and weight\n\nA quick note:\nThough there was no way to know this, the true \\(\\beta_1\\) was actually \\(1.0\\). Again, this is just to demonstrate that the OLS estimate \\(\\widehat{\\beta_1}\\) is just that- an estimate!"
  },
  {
    "objectID": "Pages/Lectures/Lecture18/Lec18.html#worked-out-example",
    "href": "Pages/Lectures/Lecture18/Lec18.html#worked-out-example",
    "title": "PSTAT 5A: Lecture 18",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\nAlright, let’s work through a computation by hand once.\nSuppose we have the variables \\[\\begin{align*}\n\\boldsymbol{x}    & = \\{3, \\ 7, \\ 8\\}   \\\\\n\\boldsymbol{y}    & = \\{20, \\ 14, \\ 17\\}\n\\end{align*}\\] and suppose we wish to construct the least-squares regression line when regressing \\(\\boldsymbol{y}\\) onto \\(\\boldsymbol{x}\\).\nFirst, we compute \\[\\begin{align*}\n\\overline{x}    & = 6   \\\\\n\\overline{y}    & = 17\n\\end{align*}\\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture18/Lec18.html#worked-out-example-1",
    "href": "Pages/Lectures/Lecture18/Lec18.html#worked-out-example-1",
    "title": "PSTAT 5A: Lecture 18",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\nNext, we compute \\[\\begin{align*}\n\\sum_{i=1}^{n} (x_i - \\overline{x})^2   & = (3 - 6)^2 + (7 - 6)^2 + (8 - 6)^2 = 14   \\\\\n\\sum_{i=1}^{n} (y_i - \\overline{y})^2   & = (20 - 17)^2 + (14 - 17)^2 + (17 - 17)^2 = 18\n\\end{align*}\\]\nAdditionally, \\[\\begin{align*}\n\\sum_{i=1}^{n} (x_i - \\overline{x})(y_i - \\overline{y})   & = (3 - 6)(20 - 17) + (7 - 6)(14 - 17)    \\\\[-7mm]\n  & \\hspace{10mm} + (8 - 6)(17 - 17)    \\\\[5mm]\n  & = -12\n\\end{align*}\\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture18/Lec18.html#worked-out-example-2",
    "href": "Pages/Lectures/Lecture18/Lec18.html#worked-out-example-2",
    "title": "PSTAT 5A: Lecture 18",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\nTherefore, \\[ \\widehat{\\beta_1} = \\frac{\\sum_{i=1}^{n} (x_i - \\overline{x})(y_i - \\overline{y})}{\\sum_{i=1}^{n} (x_i - \\overline{x})^2}   = \\frac{-12}{14} = - \\frac{6}{7} \\]\nAdditionally, \\[ \\widehat{\\beta_0} = \\overline{y} - \\widehat{\\beta_1} \\overline{x} = 17 - \\left( - \\frac{6}{7} \\right) (6) = \\frac{155}{7} \\]\nThis means that the ordinary least-squares regression line is \\[ \\boxed{\\widehat{y} = \\frac{1}{7} ( 155 - 6 x )} \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture18/Lec18.html#interpreting-the-coefficients",
    "href": "Pages/Lectures/Lecture18/Lec18.html#interpreting-the-coefficients",
    "title": "PSTAT 5A: Lecture 18",
    "section": "Interpreting the Coefficients",
    "text": "Interpreting the Coefficients\n\nAlright, so how do we interpret the OLS regression line? \\[\\widehat{y} = \\widehat{\\beta_0} + \\widehat{\\beta_1} x\\]\nWe can see that a one-unit increase in x corresponds to a \\(\\widehat{\\beta_1}\\)-unit increase in y.\n\nFor example, in our height and weight example we found \\[ \\widehat{\\texttt{weight}} = 3.367 + 0.979 \\cdot \\texttt{height} \\]\nThis means that a one-cm change in height is associated with a (predicted/estimated) 0.979 lbs change in weight."
  },
  {
    "objectID": "Pages/Lectures/Lecture18/Lec18.html#prediction",
    "href": "Pages/Lectures/Lecture18/Lec18.html#prediction",
    "title": "PSTAT 5A: Lecture 18",
    "section": "Prediction",
    "text": "Prediction\n\nWe can also use the OLS regression line to perform prediction.\nTo see how this works, let’s return to our toy example:\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotice that we do not have an x-observation of 5. As such, we don’t know what the y-value corresponding to an x-value of 5 is."
  },
  {
    "objectID": "Pages/Lectures/Lecture18/Lec18.html#prediction-1",
    "href": "Pages/Lectures/Lecture18/Lec18.html#prediction-1",
    "title": "PSTAT 5A: Lecture 18",
    "section": "Prediction",
    "text": "Prediction\n\nHowever, we do have a decent guess as to what the y-value corresponding to an x-value of 5 is- the corresponding fitted value!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[ \\widehat{y}_5 = \\frac{1}{7} (155 - 6 \\cdot 5) \\approx 17.857 \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture18/Lec18.html#extrapolation-and-the-dangers-thereof",
    "href": "Pages/Lectures/Lecture18/Lec18.html#extrapolation-and-the-dangers-thereof",
    "title": "PSTAT 5A: Lecture 18",
    "section": "Extrapolation, and the Dangers Thereof",
    "text": "Extrapolation, and the Dangers Thereof\n\nLet’s look at another toy dataset:\n\n\n\n\n\n\n\n\n\n\n\n\n\nLooks pretty linear, right?"
  },
  {
    "objectID": "Pages/Lectures/Lecture18/Lec18.html#extrapolation-and-the-dangers-thereof-1",
    "href": "Pages/Lectures/Lecture18/Lec18.html#extrapolation-and-the-dangers-thereof-1",
    "title": "PSTAT 5A: Lecture 18",
    "section": "Extrapolation, and the Dangers Thereof",
    "text": "Extrapolation, and the Dangers Thereof\n\nSay we want to predict the corresponding y value of an x value of, 40.\nFollowing our steps from before, we would just find the fitted value corresponding to x = 40:"
  },
  {
    "objectID": "Pages/Lectures/Lecture18/Lec18.html#extrapolation-and-the-dangers-thereof-2",
    "href": "Pages/Lectures/Lecture18/Lec18.html#extrapolation-and-the-dangers-thereof-2",
    "title": "PSTAT 5A: Lecture 18",
    "section": "Extrapolation, and the Dangers Thereof",
    "text": "Extrapolation, and the Dangers Thereof\n\nHere’s the kicker: the true fit was actually NOT linear!\n\nSpecifically, I used a quadratic relationship between x and y to generate the data.\nWhen you zoom in close enough, parabolas look linear!\n\nNow, we wouldn’t have had any way of knowing this.\nThis is why it is a bad idea to try to extrapolate too far.\n\nExtrapolation is the name we give to trying to apply a model estimate to values that are very far outside the realm of the original data.\nHow far is “very far”? Statisticians disagree on this front. For the purposes of this class, just use your best judgment."
  },
  {
    "objectID": "Pages/Lectures/Lecture18/Lec18.html#one-final-connection",
    "href": "Pages/Lectures/Lecture18/Lec18.html#one-final-connection",
    "title": "PSTAT 5A: Lecture 18",
    "section": "One Final Connection",
    "text": "One Final Connection\n\nNow, one final thing I’d like to mention: note that the slope of the OLS regression line is not just the correlation coefficient.\n\nAgain, the magnitude of the correlation coefficient just gives us a measure of how strong the relationship between the two variables is.\n\nThere is, however, a relationship between \\(\\widehat{\\beta_1}\\) and r: it turns out that \\[ \\widehat{\\beta_1} = \\frac{s_Y}{s_X} \\cdot r  \\]\nA question may arise: do we really believe our OLS estimate of the slope?\nRemember that \\(\\widehat{\\beta_1}\\) is just an estimator of \\(\\beta_1\\).\n\nNext time, we’ll talk about how to construct confidence intervals for \\(\\widehat{\\beta_1}\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture18/Lec18.html#your-turn-2",
    "href": "Pages/Lectures/Lecture18/Lec18.html#your-turn-2",
    "title": "PSTAT 5A: Lecture 18",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\n\n\n\n\nExercise 2\n\n\n\n\nAn airline is interested in determining the relationship between flight duration (in minutes) and the net amount of soda consumed (in oz.). Letting x denote flight duration (the explanatory variable) and y denote amount of soda consumed (the response variable), a sample of size 100 yielded the following results: \\[ \\begin{array}{cc}\n  \\displaystyle \\sum_{i=1}^{n} x_i  = 10,\\!211.7;   & \\displaystyle \\sum_{i=1}^{n} (x_i - \\overline{x})^2 =  38,\\!760.68    \\\\\n  \\displaystyle \\sum_{i=1}^{n} y_i  = 14,\\!3995.8;   & \\displaystyle \\sum_{i=1}^{n} (y_i - \\overline{y})^2 =  87.23984   \\\\\n\\displaystyle \\sum_{i=1}^{n} (x_i - \\overline{x})(y_i - \\overline{y}) = 379.945 \\\\\n\\end{array} \\]\n\nFind the equation of the OLS Regression line.\nIf a particular flight has a duration of 110 minutes, how many ounces of soda would we expect to be consumed on the flight?"
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#setup",
    "href": "Pages/Lectures/Lecture20/Lec20.html#setup",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Setup",
    "text": "Setup\n\nConsider a population, governed by some parameter \\(\\theta\\) (e.g. a mean \\(\\mu\\), a variance \\(\\sigma^2\\), a proportion \\(p\\), etc.)\nSuppose we have a null hypothesis that \\(\\theta = \\theta_0\\) (for some specified and fixed value \\(\\theta_0\\)), along with an alternative hypothesis.\nThe goal of hypothesis testing is to use data (in the form of a representative sample taken from the population), and determine whether or not this data leads credence to the null in favor of the alternative.\n\nRecall that there are four main types of alternatives we could adopt: two-sided, lower-tailed, upper-tailed, and simple-vs-simple."
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#testing-a-mean",
    "href": "Pages/Lectures/Lecture20/Lec20.html#testing-a-mean",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Testing a Mean",
    "text": "Testing a Mean\n\nBefore MT2, we discussed the framework of hypothesis testing a population proportion p.\nAfter MT2, we discussed how to perform hypothesis testing on a population mean \\(\\mu\\).\nLet’s, for the moment, consider a two-sided test: \\[ \\left[ \\begin{array}{rr}\nH_0:    & \\mu = \\mu_0   \\\\\nH_A:    & \\mu \\neq \\mu_0\n\\end{array} \\right. \\]\nSince we know that \\(\\overline{X}\\), the sample mean, is a relatively good point estimator of a population mean \\(\\mu\\), we know that our test statistic should involve \\(\\overline{X}\\) in some way."
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#testing-a-mean-1",
    "href": "Pages/Lectures/Lecture20/Lec20.html#testing-a-mean-1",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Testing a Mean",
    "text": "Testing a Mean\n\nSpecifically, we know that our test statistics are usually standardized versions of point estimators. As such, it is tempting to adopt \\[ \\mathrm{TS} = \\frac{\\overline{X} - \\mu_0}{\\sigma / \\sqrt{n}} \\] as, under certain conditions, this follows a standard normal distribution under the null (i.e. when assuming the true population mean \\(\\mu\\) is in fact \\(\\mu_0\\)): \\[ \\mathrm{TS} = \\frac{\\overline{X} - \\mu_0}{\\sigma / \\sqrt{n}} \\stackrel{H_0}{\\sim} \\mathcal{N}(0, \\ 1) \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#testing-a-mean-2",
    "href": "Pages/Lectures/Lecture20/Lec20.html#testing-a-mean-2",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Testing a Mean",
    "text": "Testing a Mean\n\nBut, we won’t always have access to the true population standard deviation \\(\\sigma\\)! Rather, sometimes we only have access to \\(s_X\\), the sample standard deviation.\nThis leads to the following test statistic: \\[ \\mathrm{TS} = \\frac{\\overline{X} - \\mu_0}{s_X / \\sqrt{n}} \\] which now no longer follows the standard normal distribution under the null, but rather a t-distribution with \\(n - 1\\) degrees of freedom: \\[ \\mathrm{TS} = \\frac{\\overline{X} - \\mu_0}{s_X / \\sqrt{n}} \\stackrel{H_0}{\\sim} t_{n - 1} \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#sampling-distribution-of-overlinex",
    "href": "Pages/Lectures/Lecture20/Lec20.html#sampling-distribution-of-overlinex",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Sampling Distribution of \\(\\overline{X}\\)",
    "text": "Sampling Distribution of \\(\\overline{X}\\)\n\n\n\n\n\ngraph TB\n  A[Is the population Normal?  . ] --> |Yes| B{{Use Normal .}}\n  A --> |No| C[Is n >= 30?  .]\n  C --> |Yes| D[sigma or s?  .]\n  C --> |No| E{{cannot proceed   .}}\n  D --> |sigma| F{{Use Normal .}}\n  D --> |s| G{{Use t }}"
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#test-statistic",
    "href": "Pages/Lectures/Lecture20/Lec20.html#test-statistic",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Test Statistic",
    "text": "Test Statistic\n\nSo, to summarize, our test statistic is: \\[ \\mathrm{TS} = \\begin{cases}\n\\displaystyle \\frac{\\overline{X} - \\mu_0}{\\sigma / \\sqrt{n}}    & \\text{if } \\quad  \\begin{array}{rl} \\bullet & \\text{pop. is normal, OR} \\\\ \\bullet & \\text{$n \\geq 30$ AND $\\sigma$ is known} \\end{array} \\quad \\stackrel{H_0}{\\sim} \\mathcal{N}(0, \\ 1) \\\\[5mm]\n\\displaystyle \\frac{\\overline{X} - \\mu_0}{s / \\sqrt{n}}         & \\text{if } \\quad  \\begin{array}{rl} \\bullet & \\text{$n \\geq 30$ AND $\\sigma$ is not known} \\end{array} \\quad \\stackrel{H_0}{\\sim} t_{n - 1}\n\\end{cases} \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#test",
    "href": "Pages/Lectures/Lecture20/Lec20.html#test",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Test",
    "text": "Test\n\nRecall our null and alternative hypotheses: \\[ \\left[ \\begin{array}{rr}\nH_0:    & \\mu = \\mu_0   \\\\\nH_A:    & \\mu \\neq \\mu_0\n\\end{array} \\right. \\]\nIf an observed instance of \\(\\overline{X}\\) is much larger than \\(\\mu_0\\), we are more inclined to believe the alternative over the null.\n\nIn other words, we would reject \\(H_0\\) for large positive values of \\(\\mathrm{TS}\\).\n\nHowever, we would also be more inclined to believe the alternative over the null if an observed instance of \\(\\overline{X}\\) was much smaller than \\(\\mu_0\\).\n\nIn other words, we would reject \\(H_0\\) for large negative values of \\(\\mathrm{TS}\\) as well."
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#test-1",
    "href": "Pages/Lectures/Lecture20/Lec20.html#test-1",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Test",
    "text": "Test\n\nWe combine these two cases using absolute values: \\[ \\texttt{decision}(\\mathrm{TS}) = \\begin{cases} \\texttt{reject } H_0 & \\text{if } |\\mathrm{TS}| > c \\\\ \\texttt{fail to reject } H_0 & \\text{otherwise}\\\\ \\end{cases}  \\] for some critical value \\(c\\).\n\nThe critical value will depend not only on the confidence level, but also the sampling distribution of \\(\\overline{X}\\).\nSpecifically, as we have previously seen, it will be the appropriate percentile (“appropriate” as dictated by the confidence level) of either the \\(\\mathcal{N}(0, \\ 1)\\) distribution or the \\(t_{n - 1}\\) distribution."
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#critical-value",
    "href": "Pages/Lectures/Lecture20/Lec20.html#critical-value",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Critical Value",
    "text": "Critical Value\n\n\nThe critical value is the positive value along the x-axis that makes the blue shaded region equal to \\(\\alpha\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#p-values",
    "href": "Pages/Lectures/Lecture20/Lec20.html#p-values",
    "title": "PSTAT 5A: Lecture 20",
    "section": "p-Values",
    "text": "p-Values\n\nWe also saw how, instead of looking at critical values, we can also look at p-values.\nThe p-value is the probability of observing something as or more extreme (in the directino of the alternative) than what we currently observe.\nAs such, p-values that are smaller than the level of significance lead credence to the alternative over the null; i.e. we reject whenever \\(p < \\alpha\\).\n\nNote this means that the way we compute p-values depends on the type of test (i.e. two-sided, lower-tailed, or upper-tailed) that we are conducting."
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#p-value-lower-tailed-test",
    "href": "Pages/Lectures/Lecture20/Lec20.html#p-value-lower-tailed-test",
    "title": "PSTAT 5A: Lecture 20",
    "section": "p-value; Lower-Tailed Test",
    "text": "p-value; Lower-Tailed Test"
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#p-value-upper-tailed-test",
    "href": "Pages/Lectures/Lecture20/Lec20.html#p-value-upper-tailed-test",
    "title": "PSTAT 5A: Lecture 20",
    "section": "p-value; Upper-Tailed Test",
    "text": "p-value; Upper-Tailed Test"
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#p-value-two-sided-test",
    "href": "Pages/Lectures/Lecture20/Lec20.html#p-value-two-sided-test",
    "title": "PSTAT 5A: Lecture 20",
    "section": "p-value; Two-Sided Test",
    "text": "p-value; Two-Sided Test"
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#worked-out-example",
    "href": "Pages/Lectures/Lecture20/Lec20.html#worked-out-example",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\n\nWorked-Out Example 1\n\n\n\n\nA city official claims that the average monthly rent of a 1 bedroom apartment in GauchoVille is $1.1k. To test this claim, a representative sample of 37 1 bedroom apartments is taken; the average monthly rent of these 37 apartments is found to be $1.21k and the standard deviation of these 37 apartments is found to be 0.34. Assume we are conducting a two-sided test with a 5% level of significance.\n\nDefine the parameter of interest.\nState the null and alternative hypotheses.\nCompute the value of the test statistic.\nAssuming the null is correct, what is the distribution of the test statistic?\nWhat is the critical value of the test?\nConduct the test, and phrase your conclusions in the context of the problem.\nWhat code would we use to compute the p-value? Would we expect this value to be less than or greater than 5%?"
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#solutions",
    "href": "Pages/Lectures/Lecture20/Lec20.html#solutions",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Solutions",
    "text": "Solutions\n\n\\(\\mu =\\) average monthly cost of a 1 bedroom apartment in GauchoVille.\n\n\\[\\left[ \\begin{array}{rr}\n  H_0:    & \\mu = 1.1   \\\\\n  H_A:    & \\mu \\neq 1.1\n\\end{array} \\right. \\]\n\nSince we do not have access to the population standard deviation, we use \\[ \\mathrm{TS} = \\frac{\\overline{X} - \\mu_0}{s / \\sqrt{n}} = \\frac{1.21 - 1.1}{0.34 / \\sqrt{37}} = \\boxed{ 1.97 } \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#solutions-1",
    "href": "Pages/Lectures/Lecture20/Lec20.html#solutions-1",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Solutions",
    "text": "Solutions\n\nWe ask:\n\nIs the population normally distributed (i.e. are housing prices in GauchoVille normally distributed)? No.\nIs our sample size large enough? Yes; \\(n = 37 \\geq 30\\).\nDo we have \\(\\sigma\\) or \\(s\\)? We have \\(s\\).\n\n\n\nTherefore, we use a t-distribution with \\(n - 1 = 37 - 1 = 36\\) degrees of freedom: \\[ \\boxed{\\mathrm{TS} \\stackrel{H_0}{\\sim} t_{36} } \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#solutions-2",
    "href": "Pages/Lectures/Lecture20/Lec20.html#solutions-2",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Solutions",
    "text": "Solutions\n\nFrom the t-table provided on the website (which will also be provided to you during the exam), the critical value is .\n\nSince \\(|\\mathrm{TS}| = |1.97| = 1.97 < 2.03\\), we fail to reject the null:\n\n\n\nAt a 5% level of significance, there was insufficient evidence to reject the null hypothesis that the true monthly cost of a 1-bedroom apartment in GauchoVille is $1.1k in favor of the alternative that the true cost is not $1.1k.\n\n\n\nThe code we would use, after importing scipy.stats, is 2 * scipy.stats.t.cdf(-1.97, 36), which we would expect to be larger than 5% as we failed to reject based on the critical value, and we only reject when p is less than \\(\\alpha\\) (which is 5% for this problem)."
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#two-samples",
    "href": "Pages/Lectures/Lecture20/Lec20.html#two-samples",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Two Samples",
    "text": "Two Samples\n\nThe above discussion was in regards to a single sample, taken from a single population.\nWhat happens if we have two populations, goverend by parameters \\(\\theta_1\\) and \\(\\theta_2\\).\nFor example, suppose we want to compare the average air pollution in Santa Barbara to that in Los Angeles.\nThat is, given two populations (Population 1 and Population 2) with population means \\(\\mu_1\\) and \\(\\mu_2\\), we would like to test some claim involving both \\(\\mu_1\\) and \\(\\mu_2\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#two-samples-1",
    "href": "Pages/Lectures/Lecture20/Lec20.html#two-samples-1",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Two Samples",
    "text": "Two Samples\n\nFor this class, we only ever consider a null of the form \\(H_0: \\mu_1 = \\mu_2\\); i.e. that the two populations have the same average.\nWe do still have two alternative hypotheses available to us:\n\n\\(H_A: \\ \\mu_1 < \\mu_2\\)\n\\(H_A: \\ \\mu_1 > \\mu_2\\)\n\nRemember that the trick is to reparameterize everything to be in terms of a difference of parameters, thereby reducing the two-parameter problem into a one-parameter problem."
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#two-sided",
    "href": "Pages/Lectures/Lecture20/Lec20.html#two-sided",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Two-Sided",
    "text": "Two-Sided\n\nFor example, suppose we are testing the following hypotheses: \\[ \\left[ \\begin{array}{rr}\nH_0:    & \\mu_1 = \\mu_2   \\\\\nH_A:    & \\mu_1 \\neq \\mu_2\n\\end{array} \\right. \\]\nWe can define \\(\\delta = \\mu_2 - \\mu_1\\), and equivalently re-express our hypotheses as \\[ \\left[ \\begin{array}{rr}\nH_0:    & \\delta = 0   \\\\\nH_A:    & \\delta \\neq 0\n\\end{array} \\right. \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#test-statistic-1",
    "href": "Pages/Lectures/Lecture20/Lec20.html#test-statistic-1",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Test Statistic",
    "text": "Test Statistic\n\nNow, we need some sort of test statistic.\nSuppose we have a (representative) sample \\(x = \\{x_i\\}_{i=1}^{n_1}\\) from Population 1 and a (representative) sample \\(y = \\{y_i\\}_{i=1}^{n_2}\\) from Population 2 (note the potentially different sample sizes!)\nWe have an inkling that a decent point estimator for \\(\\delta = \\mu_2 - \\mu_1\\) is \\(\\widehat{\\delta} = \\overline{Y} - \\overline{X}\\).\nOur test statistic will be some standardized form of \\(\\widehat{\\delta}\\), meaning we need to find \\(\\mathbb{E}[\\widehat{\\delta}]\\) and \\(\\mathrm{SD}(\\widehat{\\delta})\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#linear-combinations-of-random-variables",
    "href": "Pages/Lectures/Lecture20/Lec20.html#linear-combinations-of-random-variables",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Linear Combinations of Random Variables",
    "text": "Linear Combinations of Random Variables\n\nOur two main results are:\n\n\\(\\mathbb{E}[aX + bY + c] = a \\cdot \\mathbb{E}[X] + b \\cdot \\mathbb{E}[Y] + c\\)\n\\(\\mathrm{Var}(aX + bY + c) = a^2 \\cdot \\mathrm{Var}(X) + b^2 \\cdot \\mathrm{Var}(Y)\\), for independent random variables \\(X\\) and \\(Y\\).\n\nSince \\(\\mathbb{E}[\\overline{Y}] = \\mu_2\\) and \\(\\mathbb{E}[\\overline{X}] = \\mu_1\\), we have that \\[\\begin{align*}\n\\mathbb{E}[\\widehat{\\delta}]    & = \\mathbb{E}[\\overline{Y} - \\overline{X}]    \\\\\n  & = \\mathbb{E}[\\overline{Y}] - \\mathbb{E}[\\overline{X}]   \\\\\n  & = \\mu_2 - \\mu_1 = \\delta\n\\end{align*}\\] which effectively shows that \\(\\widehat{\\delta}\\) is a “good” point estimator of \\(\\delta\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#linear-combinations-of-random-variables-1",
    "href": "Pages/Lectures/Lecture20/Lec20.html#linear-combinations-of-random-variables-1",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Linear Combinations of Random Variables",
    "text": "Linear Combinations of Random Variables\n\nAdditionally, since \\[ \\mathrm{Var}(\\overline{X}) = \\frac{\\sigma_1^2}{n_1}; \\qquad \\mathrm{Var}(\\overline{Y}) = \\frac{\\sigma_2^2}{n_2} \\] we have \\[\\begin{align*}\n\\mathrm{Var}(\\widehat{\\delta})    & = \\mathrm{Var}(\\overline{Y} - \\overline{X})   \\\\\n  & = \\mathrm{Var}(\\overline{Y}) + \\mathrm{Var}(\\overline{X})   \\\\\n  & = \\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}\n\\end{align*}\\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#test-statistic-2",
    "href": "Pages/Lectures/Lecture20/Lec20.html#test-statistic-2",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Test Statistic",
    "text": "Test Statistic\n\nThis led us to consider the following test statistic: \\[ \\mathrm{TS}_1 = \\frac{\\overline{Y} - \\overline{X}}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}} \\] which, under the null, would follow a standard normal distribution if \\(\\overline{X}\\) and \\(\\overline{Y}\\) both followed a normal distribution.\nHowever, in many situations, we won’t have access to the population variances \\(\\sigma_1^2\\) and \\(\\sigma_2^2\\). Rather, we will only have access to the sample variances \\(s_X^2\\) and \\(s_Y^2\\). Hence, we modify our test statistic to be of the form \\[ \\mathrm{TS} = \\frac{\\overline{Y} - \\overline{X}}{\\sqrt{\\frac{s_X^2}{n_1} + \\frac{s_Y^2}{n_2}}} \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#distribution-of-the-test-statistic",
    "href": "Pages/Lectures/Lecture20/Lec20.html#distribution-of-the-test-statistic",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Distribution of the Test Statistic",
    "text": "Distribution of the Test Statistic\n\nThis statistic is no longer normally distributed under the null.\nIt approximately follows a t distribution with degrees of freedom given by the Satterthwaite Approximation: \\[ \\mathrm{df} = \\mathrm{round}\\left\\{ \\frac{ \\left[ \\left( \\frac{s_X^2}{n_1} \\right) + \\left( \\frac{s_Y^2}{n_2} \\right) \\right]^2 }{ \\frac{\\left( \\frac{s_X^2}{n_1} \\right)^2}{n_1 - 1} + \\frac{\\left( \\frac{s_Y^2}{n_2} \\right)^2}{n_2 - 1} } \\right\\} \\]\nThat is; \\[ \\mathrm{TS} \\stackrel{H_0}{\\sim} t_{\\mathrm{df}}; \\quad \\text{df given by above}\\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#test-2",
    "href": "Pages/Lectures/Lecture20/Lec20.html#test-2",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Test",
    "text": "Test\n\nIf we are conducting a two-sided hypothesis test, then both large positive values and large negative values of our test statistic would lead credence to the null over the alternative.\n\nAs such, our test would reject for large values of \\(|\\mathrm{TS}|\\)\n\nIf instead our alternative took the form \\(\\mu_1 < \\mu_2\\); i.e. that \\(\\delta = \\mu_2 - \\mu_1 > 0\\), our test would reject for large positive values of \\(\\mathrm{TS}\\).\nIf instead our alternative took the form \\(\\mu_1 > \\mu_2\\); i.e. that \\(\\delta = \\mu_2 - \\mu_1 < 0\\), our test would reject for large negative values of \\(\\mathrm{TS}\\).\nAgain, the key is to note that after reparameterizing the problem to be in terms of the difference \\(\\delta = \\mu_2 - \\mu_1\\), the problem becomes a familiar one-parameter problem."
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#worked-out-example-2",
    "href": "Pages/Lectures/Lecture20/Lec20.html#worked-out-example-2",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\n\nWorked-Out Example 2\n\n\n\n\nA renter wants to know which city is cheaper to live in: GauchoVille or Bruin City. Specifically, she would like to test the null hypothesis that the two cities have the same average monthly rent against the alternative that GauchoVille has a higher average monthly rent.\n\nAs such, she takes a representative sample of 32 houses from GauchoVille (which she calls Population 1) and 32 houses from Bruin City (which she calls Population 2), and records the following information about her samples (all values are reported in thousands of dollars):\n\\[\\begin{array}{r|cc}\n                    & \\text{Sample Average}     & \\text{Sample Standard Deviation}    \\\\\n  \\hline\n  \\textbf{GauchoVille}  &     3.2                    & 0.50        \\\\\n  \\textbf{Bruin City}   &     3.5                    & 0.60\n\\end{array}\\]\n\nWrite down the null and alternative hypotheses, taking care to define any relevant parameter(s).\nCompute the value of the test statistic.\nAssuming the null is correct, what is the distribution of the test statistic? Be sure to include any/all relevant parameter(s)! (Assume all independence and normality conditions are met.)\nWhat is the critical value of the test, if we are to use a 5% level of significance?\nConduct the relevant test at a 5% level of significance, and report your conclusions in the context of the problem."
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#solutions-3",
    "href": "Pages/Lectures/Lecture20/Lec20.html#solutions-3",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Solutions",
    "text": "Solutions\n\nLet \\(\\mu_1\\) denote the true average monthly rent in Population 1 (GauchoVille) and let \\(\\mu_2\\) denote the true average monthly rent in Population 2 (Bruin City). Then, the null and alternative hypotheses can be phrased as: \\[ \\left[ \\begin{array}{rr}\n  H_0:    & \\mu_1 = \\mu_2   \\\\\n  H_A:    & \\mu_1 < \\mu_2\n\\end{array} \\right. \\] which, phrased in terms of the difference \\(\\mu_2 - \\mu_1\\), is equivalent to \\[ \\left[ \\begin{array}{rr}\n  H_0:    & \\mu_2 - \\mu_1 = 0   \\\\\n  H_A:    & \\mu_2 - \\mu_1 > 0\n\\end{array} \\right. \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#solutions-4",
    "href": "Pages/Lectures/Lecture20/Lec20.html#solutions-4",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Solutions",
    "text": "Solutions\n\nWe compute\n\n\n\\[\\begin{align*}\n  \\mathrm{TS}   & = \\frac{\\overline{Y} - \\overline{X}}{\\sqrt{\\frac{s_X^2}{n_1} + \\frac{s_Y^2}{n_2}}} \\\\\n    & = \\frac{3.5 - 3.2}{\\sqrt{\\frac{0.5^2}{32} + \\frac{0.6^2}{32}  }} \\approx \\boxed{2.173}\n\\end{align*}\\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#solutions-5",
    "href": "Pages/Lectures/Lecture20/Lec20.html#solutions-5",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Solutions",
    "text": "Solutions\n\nWe know that, under the null, the test statistic (assuming all independence and normality conditions hold) follows a t-distribution with degrees of freedom given by the Satterthwaite Approximation. As such, we should first compute the degrees of freedom:\n\n\n\\[\\begin{align*}\n  \\mathrm{df}   & = \\mathrm{round}\\left\\{ \\frac{ \\left[ \\left( \\frac{s_X^2}{n_1} \\right) + \\left( \\frac{s_Y^2}{n_2} \\right) \\right]^2 }{ \\frac{\\left( \\frac{s_X^2}{n_1} \\right)^2}{n_1 - 1} + \\frac{\\left( \\frac{s_Y^2}{n_2} \\right)^2}{n_2 - 1} } \\right\\} \\\\\n    & = \\mathrm{round}\\left\\{ \\frac{ \\left[ \\left( \\frac{0.5^2}{32} \\right) + \\left( \\frac{0.6^2}{32} \\right) \\right]^2 }{ \\frac{\\left( \\frac{0.5^2}{32} \\right)^2}{32 - 1} + \\frac{\\left( \\frac{0.6^2}{32} \\right)^2}{32 - 1} } \\right\\} \\\\\n    & = \\mathrm{round}\\{60.04737\\} = 60\n\\end{align*}\\]\n\n\nTherefore, \\(\\mathrm{TS} \\stackrel{H_0}{\\sim} t_{60}\\)"
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#solutions-6",
    "href": "Pages/Lectures/Lecture20/Lec20.html#solutions-6",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Solutions",
    "text": "Solutions\n\nRecall that we have an upper-tailed alternative. As such, the critical value will be the \\((1 - 0.05) \\times 100 = 95\\)th percentile of the \\(t_{60}\\) distribution. From our table, we see that this is .\nWe reject when our test statistic is larger than the critical value (again, since we are using an upper-tailed alternative). Since \\(\\mathrm{TS} = 2.173 > 1.67\\), we reject the null:\n\n\n\nAt a 5% level of significance, there was sufficient evidence to reject the null that the average monthly rent in the two cities is the same against the alternative that the average monthly rent in Bruin City is higher than that in GauchoVille.\n\n\n\nQuick aside: do you think it was a valid assumption to make that the “normality conditions” hold?"
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#multiple-populations",
    "href": "Pages/Lectures/Lecture20/Lec20.html#multiple-populations",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Multiple Populations",
    "text": "Multiple Populations\n\nSuppose, instead of comparing two population means, we compare k population means \\(\\mu_1, \\cdots, \\mu_k\\).\nThis is one framework in which ANOVA (Analysis of Variance) is useful.\nGiven \\(k\\) populations, each assumed to be normally distributed, with means \\(\\mu_1, \\cdots, \\mu_k\\), ANOVA tests the following hypotheses: \\[ \\left[ \\begin{array}{rl}\nH_0:    & \\mu_1 = \\mu_2 = \\cdots = \\mu_k    \\\\\nH_A:    & \\text{at least one of the $\\mu_i$'s is different from the others}\n\\end{array} \\right. \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#anova",
    "href": "Pages/Lectures/Lecture20/Lec20.html#anova",
    "title": "PSTAT 5A: Lecture 20",
    "section": "ANOVA",
    "text": "ANOVA\n\nSpecifically, ANOVA utilizes the so-called F-statistic \\[ \\mathrm{F} = \\frac{\\mathrm{MS}_{\\mathrm{G}}}{\\mathrm{MS}_{E}} \\] where \\(\\mathrm{MS}_{\\mathrm{G}}\\), the mean square between groups, can be thought of as a measure of variability between group means, and \\(\\mathrm{MS}_{\\mathrm{E}}\\), the mean squared error, can be thought of as a measure of variability within groups/variability due to chance.\nIf \\(\\mathrm{MS}_{\\mathrm{G}}\\) is much larger than \\(\\mathrm{MS}_{\\mathrm{E}}\\) - i.e. if the variability between groups is much more than what we would expect due to chance alone - we would likely reject the null that all group means were the same.\n\nAs such, ANOVA rejects for values of the F-statistic that are large (i.e. much greater than 1)."
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#anova-1",
    "href": "Pages/Lectures/Lecture20/Lec20.html#anova-1",
    "title": "PSTAT 5A: Lecture 20",
    "section": "ANOVA",
    "text": "ANOVA\n\nAssuming the \\(k\\) populations follow independent normal distributions, the F-statistic follows an F-distribution under the null.\n\nSpecifically, \\(F \\sim F_{k-1, \\ n - k}\\) where \\(n\\) is the total number of observations across all groups.\n\nSince we reject \\(H_0\\) (in favor of \\(H_A\\)) whenever \\(F\\) is large, we always compute p-values in ANOVA using right-tail probabilities:"
  },
  {
    "objectID": "Pages/Lectures/Lecture20/Lec20.html#anova-2",
    "href": "Pages/Lectures/Lecture20/Lec20.html#anova-2",
    "title": "PSTAT 5A: Lecture 20",
    "section": "ANOVA",
    "text": "ANOVA\n\nThe results of an ANOVA are typically displayed by way of an ANOVA Table:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDF\nSum Sq\nMean Sq\nF value\nPr(>F)\n\n\n\n\nBetween Groups\n\\(k - 1\\)\n\\(\\mathrm{SS}_{\\mathrm{G}}\\)\n\\(\\mathrm{MS}_{\\mathrm{G}}\\)\nF\np-value\n\n\nResiduals\n\\(n - k\\)\n\\(\\mathrm{SS}_{\\mathrm{E}}\\)\n\\(\\mathrm{MS}_{\\mathrm{E}}\\)\n\n\n\n\n\n\n\n\n\nYou should familiarize yourself with how these quantities relate to each other; specifically, that \\[ \\mathrm{MS}_{\\mathrm{G}} = \\frac{\\mathrm{SS}_{\\mathrm{G}}}{k - 1} ; \\quad \\mathrm{MS}_{\\mathrm{E}} = \\frac{\\mathrm{SS}_{\\mathrm{E}}}{n - k}; \\quad F = \\frac{\\mathrm{MS}_{\\mathrm{G}}}{\\mathrm{MS}_{\\mathrm{E}}} \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture16/Lec16.html#recap",
    "href": "Pages/Lectures/Lecture16/Lec16.html#recap",
    "title": "PSTAT 5A: Lecture 16",
    "section": "Recap",
    "text": "Recap\n\nLast time, we discussed Hypothesis Testing on population proportions.\nThat is: given a population with true proportion \\(p\\), we took samples and tried to use the data collected in these samples to assess the validity of the claim \\(H_0: \\ p = p_0\\) (for some fixed value \\(p_0\\)) against some alternative hypothesis (two-sided, lower-tailed, or upper-tailed)\nIndeed, we can conduct hypothesis testing on any population parameter.\nOne parameter of interest to statisticians is the population mean \\(\\mu\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture16/Lec16.html#hypothesis-testing-for-the-mean-1",
    "href": "Pages/Lectures/Lecture16/Lec16.html#hypothesis-testing-for-the-mean-1",
    "title": "PSTAT 5A: Lecture 16",
    "section": "Hypothesis Testing for the Mean",
    "text": "Hypothesis Testing for the Mean\n\nThe classification of tests doesn’t change much: we still have two-sided, lower-tailed and upper-tailed tests for the mean.\nMore specifically, if our null is \\(H_0: \\ \\mu = \\mu_0\\) then:\n\nA two-sided alternative is \\(H_A: \\ \\mu \\neq \\mu_0\\)\nA lower-tailed alternative is \\(H_A: \\ \\mu < \\mu_0\\)\nAn upper-tailed alternative is \\(H_A: \\ \\mu > \\mu_0\\)\nA simple-vs-simple alternative is \\(H_A: \\ \\mu = \\mu_1\\) for some \\(\\mu_1 \\neq \\mu_0\\)"
  },
  {
    "objectID": "Pages/Lectures/Lecture16/Lec16.html#hypothesis-testing-for-the-mean-2",
    "href": "Pages/Lectures/Lecture16/Lec16.html#hypothesis-testing-for-the-mean-2",
    "title": "PSTAT 5A: Lecture 16",
    "section": "Hypothesis Testing for the Mean",
    "text": "Hypothesis Testing for the Mean\n\nOur testing procedure will be similar to that of hypothesis tests for a proportion: we compute a test statistic, and then compare this value of the test statistic to a critical value that is determined by both the distribution of the test statistic under the null as well as the significance level \\(\\alpha\\).\nLet’s focus on a two-sided test for now.\nOur test statistic will certainly involve \\(\\overline{X}\\), the sample mean.\n\nSpecifically, it will be some standardized version of \\(\\overline{X}\\).\nHowever, exactly how we standardize will be dependent upon the sampling distribution of \\(\\overline{X}\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture16/Lec16.html#sampling-distribution-of-overlinex",
    "href": "Pages/Lectures/Lecture16/Lec16.html#sampling-distribution-of-overlinex",
    "title": "PSTAT 5A: Lecture 16",
    "section": "Sampling Distribution of \\(\\overline{X}\\)",
    "text": "Sampling Distribution of \\(\\overline{X}\\)\n\n\n\n\n\ngraph TB\n  A[Is the population Normal?  . ] --> |Yes| B{{Use Normal .}}\n  A --> |No| C[Is n >= 30?  .]\n  C --> |Yes| D[sigma or s?  .]\n  C --> |No| E{{cannot proceed   .}}\n  D --> |sigma| F{{Use Normal .}}\n  D --> |s| G{{Use t }}"
  },
  {
    "objectID": "Pages/Lectures/Lecture16/Lec16.html#test-statistic",
    "href": "Pages/Lectures/Lecture16/Lec16.html#test-statistic",
    "title": "PSTAT 5A: Lecture 16",
    "section": "Test Statistic",
    "text": "Test Statistic\n\nWhat this means is that our test statistic itself will take different forms depending on the information that is provided.\nSpecifically, we have \\[ \\mathrm{TS} = \\begin{cases}\n\\displaystyle \\frac{\\overline{X} - \\mu_0}{\\sigma / \\sqrt{n}}    & \\text{if } \\quad  \\begin{array}{rl} \\bullet & \\text{pop. is normal, OR} \\\\ \\bullet & \\text{$n \\geq 30$ AND $\\sigma$ is known} \\end{array} \\quad \\stackrel{H_0}{\\sim} \\mathcal{N}(0, \\ 1) \\\\[5mm]\n\\displaystyle \\frac{\\overline{X} - \\mu_0}{s / \\sqrt{n}}         & \\text{if } \\quad  \\begin{array}{rl} \\bullet & \\text{$n \\geq 30$ AND $\\sigma$ is not known} \\end{array} \\quad \\stackrel{H_0}{\\sim} t_{n - 1}\n\\end{cases} \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture16/Lec16.html#test-statistic-1",
    "href": "Pages/Lectures/Lecture16/Lec16.html#test-statistic-1",
    "title": "PSTAT 5A: Lecture 16",
    "section": "Test Statistic",
    "text": "Test Statistic\n\nIn the two-sided case, our critical value will still be the value \\(c\\) that ensures \\[ \\mathbb{P}(|TS| > c) = \\alpha \\] meaning \\(c\\) will the \\((\\alpha /2) \\times 100\\%\\) percentile, scaled by negative 1, of either the standard normal distribution or the \\(t_{n - 1}\\) distribution.\nOur test in the two-sided case will then take the form \\[ \\texttt{decision}(\\mathrm{TS}) = \\begin{cases} \\texttt{reject } H_0 & \\text{if } |\\mathrm{TS}| > c \\\\ \\texttt{fail to reject } H_0 & \\text{otherwise}\\\\ \\end{cases}  \\]\nLet’s do an example."
  },
  {
    "objectID": "Pages/Lectures/Lecture16/Lec16.html#worked-out-example",
    "href": "Pages/Lectures/Lecture16/Lec16.html#worked-out-example",
    "title": "PSTAT 5A: Lecture 16",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\n\nWorked-Out Example 1\n\n\n\n\nAn article published by the US Census claims that in 2019 the average American took around 27.6 minutes to commute to work. To test this claim, Jenny took a representative sample of 101 Americans in 2019 and observed that these individuals had a combined average commute time of around 20.1 minutes and a standard deviation of around 12.4 minutes.\nUse the information from Jenny’s sample, along with an \\(\\alpha = 0.05\\) level of significance, to test the claim that the average commute time is 27.6 minutes against a two-sided alternative."
  },
  {
    "objectID": "Pages/Lectures/Lecture16/Lec16.html#solutions",
    "href": "Pages/Lectures/Lecture16/Lec16.html#solutions",
    "title": "PSTAT 5A: Lecture 16",
    "section": "Solutions",
    "text": "Solutions\n\nIt’s a good idea to first determine which distribution we need to use.\nIs the population normally distributed?\n\nIn other words, are the commute times of all Americans normally distributed?\nNo; or, at least, there is not enough inforamtion to conclude that they are.\n\nIs our sample large enough?\n\nYes; \\(n = 101 \\geq 30\\)\n\nDo we have \\(\\sigma\\) or \\(s\\)?\n\nWe only have \\(s\\), the standard devaition of commute times of Jenny’s sample."
  },
  {
    "objectID": "Pages/Lectures/Lecture16/Lec16.html#solutions-1",
    "href": "Pages/Lectures/Lecture16/Lec16.html#solutions-1",
    "title": "PSTAT 5A: Lecture 16",
    "section": "Solutions",
    "text": "Solutions\n\nTherefore, what distribution are we going to use?\n\nThe \\(t_{100}\\) distribution.\n\nIn other words, \\[ \\mathrm{TS} = \\frac{\\overline{X} - 27.6}{12.4 / \\sqrt{101}} \\stackrel{H_0}{\\sim} t_{100} \\]\nFrom the \\(t-\\)table, we see that the critical value will be \\(1.98\\).\nAdditionally, the observed value of our test statistic (based on Jenny’s sample) is \\[ \\frac{20.1 - 27.6}{12.4 / \\sqrt{101}} \\approx -6.08 \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture16/Lec16.html#solutions-2",
    "href": "Pages/Lectures/Lecture16/Lec16.html#solutions-2",
    "title": "PSTAT 5A: Lecture 16",
    "section": "Solutions",
    "text": "Solutions\n\nOur test rejects \\(H_0\\) when \\(|\\mathrm{TS}| > c\\).\nIn this case, \\(|\\mathrm{TS}| = |-6.07| = 6.07 > 1.98\\)\nTherefore, we reject the null:\n\n\n\nAt an \\(\\alpha = 0.05\\) level of significance, there was sufficient evidence to reject the Census’ claim that the average commute time of Americans was 27.6 minutes in favor of the alternative that the true average commute time was not 27.6 minutes."
  },
  {
    "objectID": "Pages/Lectures/Lecture16/Lec16.html#one-sided-tests",
    "href": "Pages/Lectures/Lecture16/Lec16.html#one-sided-tests",
    "title": "PSTAT 5A: Lecture 16",
    "section": "One-Sided Tests",
    "text": "One-Sided Tests\n\nAnalogously as with the hypothesis testing for proportions, the lower-tailed test of a population mean takes the form \\[ \\texttt{decision}(\\mathrm{TS}) = \\begin{cases} \\texttt{reject } H_0 & \\text{if } \\mathrm{TS} < c \\\\ \\texttt{fail to reject } H_0 & \\text{otherwise}\\\\ \\end{cases}  \\] and the upper-tailed test of a population mean takes the form \\[ \\texttt{decision}(\\mathrm{TS}) = \\begin{cases} \\texttt{reject } H_0 & \\text{if } \\mathrm{TS} > c \\\\ \\texttt{fail to reject } H_0 & \\text{otherwise}\\\\ \\end{cases}  \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture16/Lec16.html#worked-out-example-2",
    "href": "Pages/Lectures/Lecture16/Lec16.html#worked-out-example-2",
    "title": "PSTAT 5A: Lecture 16",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\n\nWorked-Out Example 2\n\n\n\n\nMcGaucho’s claims that, on average, they are able to serve customers their food in 10 minutes. Dubious of these claims, Markus takes a representative sample of 40 customers and finds that these customers waited an combined average of 15 minutes before receiving their food, with a standard deviation of 10 minutes. Based on this, he decides to perform an upper-tailed test on McGaucho’s claims at an \\(\\alpha = 0.01\\) level of significance. Conduct the test, and state your conclusions."
  },
  {
    "objectID": "Pages/Lectures/Lecture16/Lec16.html#solutions-3",
    "href": "Pages/Lectures/Lecture16/Lec16.html#solutions-3",
    "title": "PSTAT 5A: Lecture 16",
    "section": "Solutions",
    "text": "Solutions\n\nAgain, we should ask ourselves what distribution we are going to use.\nIs the population normally distributed?\n\nIn other words, are the wait times of all customers normally distributed?\nNo; or, at least, there is not enough inforamtion to conclude that they are.\n\nIs our sample large enough?\n\nYes; \\(n = 40 \\geq 30\\)\n\nDo we have \\(\\sigma\\) or \\(s\\)?\n\nWe only have \\(s\\), the standard devaition of commute times of Markus’ sample.\n\nHence, we will use the \\(t_{39}\\) distribution."
  },
  {
    "objectID": "Pages/Lectures/Lecture16/Lec16.html#solutions-4",
    "href": "Pages/Lectures/Lecture16/Lec16.html#solutions-4",
    "title": "PSTAT 5A: Lecture 16",
    "section": "Solutions",
    "text": "Solutions\n\nNow, we are using an upper-tailed alternative, meaning our critical value is the point such that \\((1 - 0.01) = 0.99\\) area lies to the left (or, equivalently, \\(0.01\\) lies to the right).\nThis means our critical value is 2.43, and we reject the null whenever \\(\\mathrm{TS} > 2.43\\).\nThe observed value of our test statistic is \\[ \\frac{15 - 10}{10 / \\sqrt{40}} = 3.162 \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture16/Lec16.html#solutions-5",
    "href": "Pages/Lectures/Lecture16/Lec16.html#solutions-5",
    "title": "PSTAT 5A: Lecture 16",
    "section": "Solutions",
    "text": "Solutions\n\nSince \\(\\mathrm{TS} = 3.162 > 2.43\\), we reject the null:\n\n\n\nAt an \\(\\alpha = 0.01\\) level of significance, there was sufficient evidence to reject McGaucho’s’ claim that the average wait time of customers is 10 minutes in favor of the alternative that the true wait time is longer than 10 minutes."
  },
  {
    "objectID": "Pages/Lectures/Lecture16/Lec16.html#your-turn",
    "href": "Pages/Lectures/Lecture16/Lec16.html#your-turn",
    "title": "PSTAT 5A: Lecture 16",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\n\n\n\n\n\nExercise 1\n\n\n\n\nA university administrator at Gaucho University (GU) claims that GU students get, on average, 8 hours of sleep per night. A sample of 35 students had a combined average of 7.72 hours of sleep and a standard deviation of 3.8 hours. Using an \\(\\alpha = 0.05\\) level of significance, test the administrator’s claims against a…\n\n…two-sided alternative\n…lower-tailed alternative"
  },
  {
    "objectID": "Pages/Lectures/Lecture16/Lec16.html#recap-1",
    "href": "Pages/Lectures/Lecture16/Lec16.html#recap-1",
    "title": "PSTAT 5A: Lecture 16",
    "section": "Recap",
    "text": "Recap\n\nLast time, we discussed Hypothesis Testing on population proportions.\nThat is: given a population with true proportion \\(p\\), we took samples and tried to use the data collected in these samples to assess the validity of the claim \\(H:0: \\ p = p_0\\) (for some fixed value \\(p_0\\)) against some alternative hypothesis (two-sided, lower-tailed, or upper-tailed)\nThe test we constructed last time compared the value of the test statistic to some critical value, and made a decision about rejection based on the comparison between these two values."
  },
  {
    "objectID": "Pages/Lectures/Lecture16/Lec16.html#lower-tailed-example",
    "href": "Pages/Lectures/Lecture16/Lec16.html#lower-tailed-example",
    "title": "PSTAT 5A: Lecture 16",
    "section": "Lower-Tailed Example",
    "text": "Lower-Tailed Example\n\nFor illustrative purposes, let’s consider a lower-tailed test: \\[ \\left[ \\begin{array}{rr}\nH_0:    & p = p_0   \\\\\nH_A:    & p < p_0\n\\end{array} \\right.\\]\nUnder appropriate conditions, our test statistic \\[ \\mathrm{TS} = \\frac{\\widehat{P} - p_0}{\\sqrt{\\frac{p_0(1 - p_0)}{n}}} \\] follows the standard normal distribution under the null."
  },
  {
    "objectID": "Pages/Lectures/Lecture16/Lec16.html#lower-tailed-example-1",
    "href": "Pages/Lectures/Lecture16/Lec16.html#lower-tailed-example-1",
    "title": "PSTAT 5A: Lecture 16",
    "section": "Lower-Tailed Example",
    "text": "Lower-Tailed Example\n\n\nRecall that \\(\\alpha\\), the level of significance, is constructed to be the \\(\\alpha \\times 100\\)th percentile of the standard normal distribution:"
  },
  {
    "objectID": "Pages/Lectures/Lecture16/Lec16.html#reject-or-not",
    "href": "Pages/Lectures/Lecture16/Lec16.html#reject-or-not",
    "title": "PSTAT 5A: Lecture 16",
    "section": "Reject or Not?",
    "text": "Reject or Not?"
  },
  {
    "objectID": "Pages/Lectures/Lecture16/Lec16.html#reject-or-not-1",
    "href": "Pages/Lectures/Lecture16/Lec16.html#reject-or-not-1",
    "title": "PSTAT 5A: Lecture 16",
    "section": "Reject or Not?",
    "text": "Reject or Not?"
  },
  {
    "objectID": "Pages/Lectures/Lecture16/Lec16.html#leadup",
    "href": "Pages/Lectures/Lecture16/Lec16.html#leadup",
    "title": "PSTAT 5A: Lecture 16",
    "section": "Leadup",
    "text": "Leadup\n\nSo, up until now, our decision has been based on comparing the raw value of the test statistic to the critical value.\n\nIf the test statistic is smaller than the critical value, we reject.\nOtherwise, we fail to reject.\n\nAgain, remember the intuition behind why this is: if we observe a value of \\(\\widehat{P}\\) that is much smaller than \\(p_0\\), this leads credence to the claim that \\(p < p_0\\) (i.e. this leads credence to our alternative and away from our null).\n\nValues of \\(\\widehat{p}\\) less than \\(p_0\\) lead to very negative values of \\(\\mathrm{TS}\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture16/Lec16.html#leadup-1",
    "href": "Pages/Lectures/Lecture16/Lec16.html#leadup-1",
    "title": "PSTAT 5A: Lecture 16",
    "section": "Leadup",
    "text": "Leadup\n\nNow, we like to think in terms of areas.\nSo, here’s an idea: let’s translate our reject/fail to reject scheme to be in terms of areas.\nSpecifically, here is what I mean.\nConsider again a situation in which we reject the null:"
  },
  {
    "objectID": "Pages/Lectures/Lecture16/Lec16.html#leadup-2",
    "href": "Pages/Lectures/Lecture16/Lec16.html#leadup-2",
    "title": "PSTAT 5A: Lecture 16",
    "section": "Leadup",
    "text": "Leadup\n\nSo, we can rephrase our decision as follows:\n\nReject \\(H_0\\) if the area to the left of the test statistic is smaller than \\(\\alpha\\).\nOtherwise, fail to reject \\(H_0\\).\n\nNotice that the area to the left of the test statistic, since we are dealing with a lower-tailed test, is equivalent to “the probability of observing something as or more extreme, under the null, as the value we currently observe”.\nThis is an example of what we call a p-value."
  },
  {
    "objectID": "Pages/Lectures/Lecture16/Lec16.html#p-values",
    "href": "Pages/Lectures/Lecture16/Lec16.html#p-values",
    "title": "PSTAT 5A: Lecture 16",
    "section": "p-Values",
    "text": "p-Values\n\nThe textbook defines \\(p-\\)values as:\n\n\n\n[…] the probability of observing data at least as favorable to the alternative hypothesis as our current data set, if the null hypothesis were true.\n\n\n\nAnother way to think about it is this: if the null were true, what is the probability that we would observe something even more extreme (i.e. even “farther away”, in the direction of the alternative) as our current observations?\nNote that exactly how we compute \\(p-\\)values depends heavily on our alternative."
  },
  {
    "objectID": "Pages/Lectures/Lecture16/Lec16.html#the-test-statistic",
    "href": "Pages/Lectures/Lecture16/Lec16.html#the-test-statistic",
    "title": "PSTAT 5A: Lecture 16",
    "section": "The Test Statistic",
    "text": "The Test Statistic"
  },
  {
    "objectID": "Pages/Lectures/Lecture16/Lec16.html#p-value-lower-tailed-test",
    "href": "Pages/Lectures/Lecture16/Lec16.html#p-value-lower-tailed-test",
    "title": "PSTAT 5A: Lecture 16",
    "section": "p-value; Lower-Tailed Test",
    "text": "p-value; Lower-Tailed Test"
  },
  {
    "objectID": "Pages/Lectures/Lecture16/Lec16.html#p-value-upper-tailed-test",
    "href": "Pages/Lectures/Lecture16/Lec16.html#p-value-upper-tailed-test",
    "title": "PSTAT 5A: Lecture 16",
    "section": "p-value; Upper-Tailed Test",
    "text": "p-value; Upper-Tailed Test"
  },
  {
    "objectID": "Pages/Lectures/Lecture16/Lec16.html#p-value-two-sided-test",
    "href": "Pages/Lectures/Lecture16/Lec16.html#p-value-two-sided-test",
    "title": "PSTAT 5A: Lecture 16",
    "section": "p-value; Two-Sided Test",
    "text": "p-value; Two-Sided Test"
  },
  {
    "objectID": "Pages/Lectures/Lecture16/Lec16.html#worked-out-example-3",
    "href": "Pages/Lectures/Lecture16/Lec16.html#worked-out-example-3",
    "title": "PSTAT 5A: Lecture 16",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\n\nWorked-Out Example 3\n\n\n\n\nForbes magazine has claimed that, as of May 2023, 91.7% of US households own a vehicle. To test that claim, we take a representative sample of 500 US households and observe that 89.4% of these households own a vehicle.\n\nCompute the \\(p-\\)value of our observed value of the statistic, assuming we are using a lower-tailed alternative.\nCompute the \\(p-\\)value of our observed value of the statistic, assuming we are using a two-sided alternative."
  },
  {
    "objectID": "Pages/Lectures/Lecture16/Lec16.html#solutions-6",
    "href": "Pages/Lectures/Lecture16/Lec16.html#solutions-6",
    "title": "PSTAT 5A: Lecture 16",
    "section": "Solutions",
    "text": "Solutions\n\nLet’s first compute the observed value of our test statistic: \\[ \\mathrm{TS} = \\frac{0.894 - 0.917}{\\sqrt{\\frac{(0.917) \\cdot (1 - 0.917)}{500}}} = -1.86 \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture16/Lec16.html#solutions-7",
    "href": "Pages/Lectures/Lecture16/Lec16.html#solutions-7",
    "title": "PSTAT 5A: Lecture 16",
    "section": "Solutions",
    "text": "Solutions\n\nIn the lower-tailed case, our p-value is\n\n\n\n\n\n\n\n\n\nWe can compute this in one of two ways."
  },
  {
    "objectID": "Pages/Lectures/Lecture16/Lec16.html#solutions-8",
    "href": "Pages/Lectures/Lecture16/Lec16.html#solutions-8",
    "title": "PSTAT 5A: Lecture 16",
    "section": "Solutions",
    "text": "Solutions\n\nWe could simply use Python:\n\n\n\nimport scipy.stats as sps\nsps.norm.cdf(-1.86)\n\n0.03144276298075271\n\n\n\n\n\n\nOr, we could use our z-table: \\(0.0314\\)\n\n\n\n\nEither way, looks like our p-value in the lower-tailed case is \\(0.0314\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture16/Lec16.html#solutions-9",
    "href": "Pages/Lectures/Lecture16/Lec16.html#solutions-9",
    "title": "PSTAT 5A: Lecture 16",
    "section": "Solutions",
    "text": "Solutions\n\nIn the two-sided case, our p-value is\n\n\n\n\n\n\n\n\n\nSame deal as before: we can compute this using either Python or our table."
  },
  {
    "objectID": "Pages/Lectures/Lecture16/Lec16.html#solutions-10",
    "href": "Pages/Lectures/Lecture16/Lec16.html#solutions-10",
    "title": "PSTAT 5A: Lecture 16",
    "section": "Solutions",
    "text": "Solutions\n\nAlternatively, note that we can utilize the symmetry of the normal distribution in our favor- the area we’re interested in is simply twice the area we found in the lower-tailed case!\nSo, our \\(p-\\)value in the two-sided case is simply \\(2 \\times (0.0314) = 0.0628\\).\nIf you don’t believe me:\n\n\n\nsps.norm.cdf(-1.86) + (1 - sps.norm.cdf(1.86))\n\n0.06288552596150537"
  },
  {
    "objectID": "Pages/Lectures/Lecture16/Lec16.html#p-values-1",
    "href": "Pages/Lectures/Lecture16/Lec16.html#p-values-1",
    "title": "PSTAT 5A: Lecture 16",
    "section": "p-Values",
    "text": "p-Values\n\nOf course, p-values are not restricted to the case of hypothesis testing a proportion; we can absolutely compute \\(p-\\)values in a setting where we are testing a population mean!\n\n\n\n\n\n\n\n\nWorked-Out Example 4\n\n\n\n\nConsider again the setup of Worked-Out Example 2: McGaucho’s claims that, on average, they are able to serve customers their food in 10 minutes. Dubious of these claims, Markus takes a representative sample of 40 customers and finds that these customers waited an combined average of 15 minutes before receiving their food, with a standard deviation of 10 minutes.\n\nCompute the p-value if we were to use a lower-tailed alternative.\nCompute the p-value if we were to use a two-sided alternative."
  },
  {
    "objectID": "Pages/Lectures/Lecture16/Lec16.html#solutions-11",
    "href": "Pages/Lectures/Lecture16/Lec16.html#solutions-11",
    "title": "PSTAT 5A: Lecture 16",
    "section": "Solutions",
    "text": "Solutions\n\nRecall that we ended up needing to use the \\(t_{39}\\) distribution as opposed to the standard normal distribution.\nThis doesn’t affect our computations of the p-value much, except for the fact that now we will need to use Python (as our \\(t-\\)table doesn’t really give us probabilities).\nThe observed value of the test statistic is \\[ \\frac{15 - 10}{10 / \\sqrt{40}} = 3.162 \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture16/Lec16.html#solutions-12",
    "href": "Pages/Lectures/Lecture16/Lec16.html#solutions-12",
    "title": "PSTAT 5A: Lecture 16",
    "section": "Solutions",
    "text": "Solutions\n\nOn a graph, this looks like:"
  },
  {
    "objectID": "Pages/Lectures/Lecture16/Lec16.html#solutions-13",
    "href": "Pages/Lectures/Lecture16/Lec16.html#solutions-13",
    "title": "PSTAT 5A: Lecture 16",
    "section": "Solutions",
    "text": "Solutions\n\nUsing a lower-tailed alternative:\n\n\n\n\n\n\n\n\n\n\nsps.t.cdf(3.162, 39)\n\n0.998485142798897"
  },
  {
    "objectID": "Pages/Lectures/Lecture16/Lec16.html#solutions-14",
    "href": "Pages/Lectures/Lecture16/Lec16.html#solutions-14",
    "title": "PSTAT 5A: Lecture 16",
    "section": "Solutions",
    "text": "Solutions\n\nUsing a two-sided alternative:\n\n\n\n\n\n\n\n\n\n\n2 * sps.t.cdf(-3.162, 39)\n\n0.0030297144022061392\n\n\nor\n\nsps.t.cdf(-3.162, 39) + (1 - sps.t.cdf(3.162, 39))\n\n0.0030297144022060733"
  },
  {
    "objectID": "Pages/Lectures/Lecture16/Lec16.html#what-do-we-do-with-a-p-value",
    "href": "Pages/Lectures/Lecture16/Lec16.html#what-do-we-do-with-a-p-value",
    "title": "PSTAT 5A: Lecture 16",
    "section": "What Do We Do with a p-Value?",
    "text": "What Do We Do with a p-Value?\n\nOkay, so after we’ve computed our p-value… what do we do?\nWe reject when p is small; specifically, we reject when p is less than the level of significance.\nSo, in other words, we now have two ways to conduct hypothesis tests under our belt: using critical values, and using p-values."
  },
  {
    "objectID": "Pages/Lectures/Lecture16/Lec16.html#your-turn-1",
    "href": "Pages/Lectures/Lecture16/Lec16.html#your-turn-1",
    "title": "PSTAT 5A: Lecture 16",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\n\n\n\n\n\nExercise 2\n\n\n\n\nUCSB claims that 40% of its undergraduate student body are first-generation students (as in, the first in their families to earn a Bachelor’s degree). To test this claim, a representative sample of 120 undergraduates is taken and it is found that 43% of these students are first-generation. Suppose we use an \\(\\alpha = 0.05\\) level of significance, and we test the University’s claims against a two-sided alternative.\n\nConduct the test using critical values.\nConduct the test using p-values."
  },
  {
    "objectID": "Pages/Lectures/Lecture16/Lec16.html#your-turn-2",
    "href": "Pages/Lectures/Lecture16/Lec16.html#your-turn-2",
    "title": "PSTAT 5A: Lecture 16",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\n\n\n\n\n\nExercise 3\n\n\n\n\nA botanist claims that the average weight of a chestnut is around 5 grams. To test this claim, a representative sample of 35 chestnuts is taken; these chestnuts have a combined average weight of 4.87 grams and a standard deviation of 1.82 grams. Suppose we use an \\(\\alpha = 0.05\\) level of significance, and a lower-tailed alternative.\n\nConduct the test using critical values.\nConduct the test using p-values."
  },
  {
    "objectID": "Pages/Lectures/Lecture11/Lec11.html#last-time",
    "href": "Pages/Lectures/Lecture11/Lec11.html#last-time",
    "title": "PSTAT 5A: Lecture 11",
    "section": "Last Time",
    "text": "Last Time\n\nLast time we began discussing inference on a proportion.\nWe had a population with proportion \\(p\\), drew representative samples from this population, and used the sample proportion \\(\\widehat{P}\\) (i.e. the proportion observed in the sample) as a proxy for \\(p\\).\nOur main result was the Central Limit Theorem for Proportions which states \\[ \\widehat{P} \\sim \\mathcal{N}\\left( p, \\ \\sqrt{ \\frac{p(1 - p)}{n} } \\right) \\] assuming the success-failure conditions are met:\n\n\\(np \\geq 10\\)\n\\(n(1 - p) \\geq 10\\)"
  },
  {
    "objectID": "Pages/Lectures/Lecture11/Lec11.html#finishing-up-an-example",
    "href": "Pages/Lectures/Lecture11/Lec11.html#finishing-up-an-example",
    "title": "PSTAT 5A: Lecture 11",
    "section": "Finishing up an Example",
    "text": "Finishing up an Example\n\n\n\n\n\n\n\nExercise 2 (from Last Lecture)\n\n\n\n\nAt a certain company, it is known that 65% of employees are from underrepresented minorities (UMs). A representative sample of 80 employees is taken, and the proportion of people from UMs is recorded.\n\nDefine the random variable of interest.\nWhat is the probability that greater than than 50% of people in the sample are from UMs?\nWhat is the probability that the proportion of people in the sample who are from UMs lies within 5% of the true proportion of 65%?"
  },
  {
    "objectID": "Pages/Lectures/Lecture11/Lec11.html#substitution-approximation",
    "href": "Pages/Lectures/Lecture11/Lec11.html#substitution-approximation",
    "title": "PSTAT 5A: Lecture 11",
    "section": "Substitution Approximation",
    "text": "Substitution Approximation\n\nCan anyone point out a potential difficulty with verifying the success-failure conditions?\nThat’s right; they involve the parameter \\(p\\), which is in many cases unknown!\n\nRemember - in the beginning of last lecture, I mentioned that the whole point of performing statistical inference is to try and make claims about a population parameter that is unknowable, or too difficult to determine exactly.\n\nTo remedy this, we often use the so-called substitution approximation to the success-failure conditions:\n\n\\(n \\widehat{p} \\geq 10\\)\n\\(n(1 - \\widehat{p}) \\geq 10\\)\n\nSometimes, we substitute \\(\\widehat{p}\\) into the formula for the standard deviation of \\(\\widehat{P}\\), as the next example illustrates."
  },
  {
    "objectID": "Pages/Lectures/Lecture11/Lec11.html#worked-out-example",
    "href": "Pages/Lectures/Lecture11/Lec11.html#worked-out-example",
    "title": "PSTAT 5A: Lecture 11",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\n\nWorked-Out Example 1\n\n\n\n\nA veterinarian wishes to determine the true proportion of cats that suffer from FIV (Feline Immunodeficiency Virus). To that end, she takes a representative sample of 500 cats and finds that 3.2% of cats in this sample have FIV. What is the probability that the proportion of cats that are FIV-positive in her sample of 500 cats lies within 1 percent of the true proportion of FIV-positive cats?"
  },
  {
    "objectID": "Pages/Lectures/Lecture11/Lec11.html#solution",
    "href": "Pages/Lectures/Lecture11/Lec11.html#solution",
    "title": "PSTAT 5A: Lecture 11",
    "section": "Solution",
    "text": "Solution\n\nLet \\(p\\) denote the true proportion of FIV-positive cats. Let \\(\\widehat{P}\\) denote the proportion of FIV-positive cats in a representative sample of size 500.\n\nDo we know the value of \\(p\\)?\nNo we do not.\n\nWhat we do have is \\(\\widehat{p} = 0.032\\).\n\nTherefore, we use the substitution approximation to the success-failure conditions:\n\\(n \\widehat{p} = (500)(0.032) = 16 \\geq 10 \\ \\checkmark\\)\n\\(n \\widehat{p} (1 - \\widehat{p}) = (500)(1 - 0.032) = 484 \\geq 10 \\ \\checkmark\\)\n\nSince both conditions are met, the CLT tells us \\[ \\widehat{P} \\sim \\mathcal{N}\\left(p, \\ \\sqrt{\\frac{p(1 - p)}{500}} \\right)\\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture11/Lec11.html#solution-1",
    "href": "Pages/Lectures/Lecture11/Lec11.html#solution-1",
    "title": "PSTAT 5A: Lecture 11",
    "section": "Solution",
    "text": "Solution\n\nWe seek \\(\\mathbb{P}(p - 0.01 \\leq \\widehat{P} \\leq p + 0.01)\\).\nOur first step is to write this as \\[ \\mathbb{P}(\\widehat{P} \\leq p + 0.01) - \\mathbb{P}(\\widehat{P} \\leq p - 0.01 ) \\]\nNext, we find the associated \\(z-\\)scores: \\[\\begin{align*}\nz_1   & = \\frac{(p + 0.01) - p}{\\sqrt{\\frac{p(1 - p)}{500}}}  = \\frac{0.01}{\\sqrt{\\frac{p(1 - p)}{500}}}    \\\\\nz_2   & = \\frac{(p + 0.01) - p}{\\sqrt{\\frac{p(1 - p)}{500}}} = - \\frac{0.01}{\\sqrt{\\frac{p(1 - p)}{500}}}  \n\\end{align*}\\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture11/Lec11.html#solution-2",
    "href": "Pages/Lectures/Lecture11/Lec11.html#solution-2",
    "title": "PSTAT 5A: Lecture 11",
    "section": "Solution",
    "text": "Solution\n\nNow, we can apply the substitution approximation to plug in \\(\\widehat{p}\\) in place of \\(p\\) in the denominator of our \\(z-\\)scores to compute \\[\\begin{align*}\nz_{1, \\ \\text{sub}}   & = \\frac{0.01}{\\sqrt{\\frac{(0.032)(1 - (0.032))}{500}}} = \\frac{0.01}{0.00787}  = 1.27  \\\\\nz_{2, \\ \\text{sub}}   & = - \\frac{0.01}{\\sqrt{\\frac{(0.032)(1 - (0.032))}{500}}} = - \\frac{0.01}{0.00787}  = -1.27\n\\end{align*}\\]\nFinally, consulting our standard normal table, we find the answer to be \\[ 0.8980 - 0.1020 = \\boxed{0.796 = 79.6\\%} \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture11/Lec11.html#a-note",
    "href": "Pages/Lectures/Lecture11/Lec11.html#a-note",
    "title": "PSTAT 5A: Lecture 11",
    "section": "A Note",
    "text": "A Note\n\nI’d like to stress that the substitution approximation is just that- an approximation.\nIt is not, for instance, true that \\(\\mathbb{E}[\\widehat{P}] = \\widehat{p}\\); the center of the distribution of \\(\\widehat{P}\\) will always (provided the success-failure conditions are met) be \\(p\\), the true value of the proportion."
  },
  {
    "objectID": "Pages/Lectures/Lecture11/Lec11.html#leadup",
    "href": "Pages/Lectures/Lecture11/Lec11.html#leadup",
    "title": "PSTAT 5A: Lecture 11",
    "section": "Leadup",
    "text": "Leadup\n\nLet’s quickly take stock of what we’ve learned.\nIf we have a population with some unknown population parameter \\(p\\), we can repeatedly take representative samples, compute the sample proportion in each sample, and construct the sampling distribution of \\(\\widehat{P}\\).\nAssuming the success-failure conditions are met, this sampling distribution will be centered at \\(p\\), the true proportion value.\nHence, a natural point estimator of \\(p\\) would be \\(\\widehat{P}\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture11/Lec11.html#leadup-1",
    "href": "Pages/Lectures/Lecture11/Lec11.html#leadup-1",
    "title": "PSTAT 5A: Lecture 11",
    "section": "Leadup",
    "text": "Leadup\n\nHowever, the key assumption in this procedure is our ability to take multiple samples from the population.\nIn many practical situations, this is not feasible.\nSo, here is a new question to consider: given just a single sample from the population, what can we say about \\(p\\)?\nWell, we’ve already seen that it’s risky to simply take \\(\\widehat{p}\\) (i.e. the value of \\(\\widehat{P}\\) that was observed in the sample we took) to be an estimate of \\(p\\), due to the randomness associated with \\(\\widehat{P}\\).\nInstead of looking for point estimates of \\(p\\), what happens if we instead provide intervals we believe may contain \\(p\\)?"
  },
  {
    "objectID": "Pages/Lectures/Lecture11/Lec11.html#leadup-2",
    "href": "Pages/Lectures/Lecture11/Lec11.html#leadup-2",
    "title": "PSTAT 5A: Lecture 11",
    "section": "Leadup",
    "text": "Leadup\n\nLet’s make things a bit more concrete. Since I like cats, let’s go back to our veterinarian example:\n\n\n\nA veterinarian wishes to determine the true proportion of cats that suffer from FIV (Feline Immunodeficiency Virus). To that end, she takes a representative sample of 100 cats and finds that 3.2% of cats in this sample have FIV.\n\n\n\nAgain, it’s risky to say that “the true proportion of FIV-positive cats is 3.2%” based solely on this sample.\nInstead, we are going to start proposing intervals of values that we believe contain \\(p\\).\nNow, clearly the strengths of our beliefs will depend on the interval we provide.\nFor example, I am 100% confident that the true proportion of FIV-positive cats is somewhere in the interval \\((-\\infty, \\infty)\\).\nBut, suppose we instead consider the interval \\((0.030, \\ 0.034)\\); now we can’t really say that we’re 100% certain this interval covers the true value of \\(p\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture11/Lec11.html#confidence-intervals",
    "href": "Pages/Lectures/Lecture11/Lec11.html#confidence-intervals",
    "title": "PSTAT 5A: Lecture 11",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\n\nThis is the basic idea of what are known as confidence intervals.\nI particularly like the analogy our textbook (OpenIntro Statistics) uses:\n\n\n\n[…] Using only a point estimate is like fishing in a murky lake with a spear. We can throw a spear where we saw a fish, but we will probably miss. On the other hand, if we toss a net in that area, we have a good chance of catching the fish. (page 181)\n\n\n\nFor the purposes of this class, we will construct confidence intervals for an arbitrary parameter \\(\\theta\\) (e.g. a population proportion \\(p\\), a population mean \\(\\mu\\), etc.) of the form \\(\\widehat{\\theta} \\pm \\mathrm{m.e.}\\) where \\(\\widehat{\\theta}\\) represents some point estimate of \\(\\theta\\) and \\(\\mathrm{m.e.}\\) represents a margin of error.\nSo, for the veterinarian example, our confidence interval will be of the form \\(\\widehat{p} \\pm \\mathrm{m.e.}\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture11/Lec11.html#confidence-intervals-1",
    "href": "Pages/Lectures/Lecture11/Lec11.html#confidence-intervals-1",
    "title": "PSTAT 5A: Lecture 11",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\n\nBefore constructing a confidence interval, however, we need to specify our confidence level. In other words, we need first have an idea of how confident we want to be that our interval contains the true parameter value.\nFor example, a 95% confidence interval is an interval \\(\\widehat{\\theta} \\pm \\mathrm{m.e.}\\) that we are 95% confident covers the true value of \\(\\theta\\).\nHere’s a question: based on everything we’ve talked about thus far, do you think higher confidence levels correspond to wider or narrower intervals?\n\nThat’s right: the higher our confidence level, the wider our interval will be.\nAs an extreme example, consider again the slightly absurd confidence interval \\((-\\infty, \\ \\infty)\\); this is a 100% confidence interval because we are 100% confident that it covers the true value of the parameter!\n\nSo, therein lies the tradeoff: the more confidence we want, the wider we need to make our intervals and the less informative they become in pinning down the true value of the parameter."
  },
  {
    "objectID": "Pages/Lectures/Lecture11/Lec11.html#confidence-intervals-for-proportions",
    "href": "Pages/Lectures/Lecture11/Lec11.html#confidence-intervals-for-proportions",
    "title": "PSTAT 5A: Lecture 11",
    "section": "Confidence Intervals for Proportions",
    "text": "Confidence Intervals for Proportions\n\nAlright, let’s return to our considerations on population proportions.\nAgain, our confidence interval will take the general form \\(\\widehat{p} \\pm \\mathrm{m.e.}\\).\nIt makes sense that the margin of error should include some information about the variability of \\(\\widehat{P}\\). As such, we take our confidence intervals to be of the form \\[ \\widehat{p} \\pm z_\\alpha \\cdot \\sqrt{ \\frac{p(1 - p)}{n} } \\] where \\(z_{\\alpha}\\) is a constant that depends on our confidence level."
  },
  {
    "objectID": "Pages/Lectures/Lecture11/Lec11.html#confidence-intervals-for-proportions-1",
    "href": "Pages/Lectures/Lecture11/Lec11.html#confidence-intervals-for-proportions-1",
    "title": "PSTAT 5A: Lecture 11",
    "section": "Confidence Intervals for Proportions",
    "text": "Confidence Intervals for Proportions\n\nTo see exactly how this dependency manifests itself, let’s make things a bit more concrete and consider a 95% confidence level. It turns out that this implies \\[ \\mathbb{P}(-z_\\alpha \\leq Z \\leq z_\\alpha) = 0.95 \\] where \\(Z \\sim \\mathcal{N}(0, \\ 1)\\).\n\nI’ll try to post some supplementary material for those of you curious as to why this is- for now, I ask you to just take this fact at face value.\n\nAs always, we draw a picture."
  },
  {
    "objectID": "Pages/Lectures/Lecture11/Lec11.html#confidence-intervals-for-proportions-2",
    "href": "Pages/Lectures/Lecture11/Lec11.html#confidence-intervals-for-proportions-2",
    "title": "PSTAT 5A: Lecture 11",
    "section": "Confidence Intervals for Proportions",
    "text": "Confidence Intervals for Proportions\n\nThe region we will sketch is the area under the standard normal curve between \\(-z_\\alpha\\) and \\(z_\\alpha\\):\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow, here’s the slightly peculiar thing- in this case, we know that the area itself must be 0.95. What we don’t know is exactly where those endpoints are."
  },
  {
    "objectID": "Pages/Lectures/Lecture11/Lec11.html#confidence-intervals-for-proportions-3",
    "href": "Pages/Lectures/Lecture11/Lec11.html#confidence-intervals-for-proportions-3",
    "title": "PSTAT 5A: Lecture 11",
    "section": "Confidence Intervals for Proportions",
    "text": "Confidence Intervals for Proportions\n\nSome of you may have an inkling that a normal table may be helpful…. and it will be!\nTo make clear how a normal table will help, let’s convert our picture to be in terms of tail areas:\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat must be the area of the shaded bit above?\n\nThat’s right: 5% (since the area in the middle is, by construction, 95%)."
  },
  {
    "objectID": "Pages/Lectures/Lecture11/Lec11.html#confidence-intervals-for-proportions-4",
    "href": "Pages/Lectures/Lecture11/Lec11.html#confidence-intervals-for-proportions-4",
    "title": "PSTAT 5A: Lecture 11",
    "section": "Confidence Intervals for Proportions",
    "text": "Confidence Intervals for Proportions\n\nBecause the standard normal density curve is symmetric, the area of any one of the two tails must be (5% / 2) = 2.5%:\n\n\n\n\n\n\n\n\n\n\n\n\n\nSo, what we have shown, is that \\(z_\\alpha\\) must satisfy the condition \\[ \\mathbb{P}(Z \\leq -z_\\alpha) = 0.025 \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture11/Lec11.html#confidence-intervals-for-proportions-5",
    "href": "Pages/Lectures/Lecture11/Lec11.html#confidence-intervals-for-proportions-5",
    "title": "PSTAT 5A: Lecture 11",
    "section": "Confidence Intervals for Proportions",
    "text": "Confidence Intervals for Proportions\n\nAgain, \\(z_\\alpha\\) must satisfy \\[ \\mathbb{P}(Z \\leq -z_\\alpha) = 0.025 \\]\nFrom a normal table, we see that \\[ \\mathbb{P}(Z \\leq -1.96) = 0.025 \\]\nTherefore, we must have \\[ \\mathbb{P}(Z \\leq -z_\\alpha) = \\mathbb{P}(Z \\leq -1.96) \\] that is, \\(-z_\\alpha = -1.96\\) or \\(\\boxed{z_\\alpha = 1.96}\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture11/Lec11.html#confidence-intervals-for-proportions-6",
    "href": "Pages/Lectures/Lecture11/Lec11.html#confidence-intervals-for-proportions-6",
    "title": "PSTAT 5A: Lecture 11",
    "section": "Confidence Intervals for Proportions",
    "text": "Confidence Intervals for Proportions\n\nSo, in conclusion, a 95% confidence interval for a population proportion will take the form \\[ \\widehat{p} \\pm 1.96 \\cdot \\sqrt{ \\frac{p(1 - p)}{n} } \\]\n\n\n\n\n\n\n\n\nExercise 2\n\n\n\n\nUse a similar set of reasoning to show that a 90% confidence interval for a population proportion \\(p\\) takes the form \\[ \\widehat{p} \\pm 1.645 \\cdot \\sqrt{\\frac{p(1 - p)}{n}} \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture11/Lec11.html#quick-aside-percentiles-of-the-standard-normal-distribution",
    "href": "Pages/Lectures/Lecture11/Lec11.html#quick-aside-percentiles-of-the-standard-normal-distribution",
    "title": "PSTAT 5A: Lecture 11",
    "section": "Quick Aside: Percentiles of the Standard Normal Distribution",
    "text": "Quick Aside: Percentiles of the Standard Normal Distribution\n\nAs a quick aside: notice that what we’ve done is actually found various percentiles of the standard normal distribution!\nPercentiles of a distribution are defined much in the same way we defined the percentiles of a list of numbers: the pth percentile of a random variable \\(X\\) is the value \\(\\pi_p\\) such that \\(\\mathbb{P}(X \\leq \\pi_p) = p\\).\nTo find the pth percentile of the standard normal table, here are the steps we use:\n\nFind \\(p\\) in the body of the table\nWhatever \\(z-\\)score that corresponds to the value of \\(p\\) in the table will be the pth percentile\n\n\n\n\n\n\n\n\n\nExercise 3\n\n\n\n\nFind the 4.55th, 83.4th, and 96.41th percentiles of the standard normal distribution."
  },
  {
    "objectID": "Pages/Lectures/Lecture11/Lec11.html#confidence-intervals-for-proportions-7",
    "href": "Pages/Lectures/Lecture11/Lec11.html#confidence-intervals-for-proportions-7",
    "title": "PSTAT 5A: Lecture 11",
    "section": "Confidence Intervals for Proportions",
    "text": "Confidence Intervals for Proportions\n\nHere are some common confidence levels, and their corresponding values of \\(z_\\alpha\\).\n\n\n\n\n\nConfidence Level\nValue of \\(\\boldsymbol{z_\\alpha}\\)\n\n\n\n\n90%\n1.645\n\n\n95%\n1.96\n\n\n99%\n2.58\n\n\n\n\n\nRecall that these \\(z_\\alpha\\)’s are simply corresponding percentiles (scaled by \\(-1\\)) of the standard normal distribution.\nTo find \\(z_\\alpha\\) corresponding to an arbitrary \\(100 \\times (1 - \\alpha)\\) interval, find the \\((\\alpha / 2) \\times 100\\)th percentile of the standard normal distribution and multiply by \\((-1)\\).\n\nE.g. a 95% confidence interval is equivalent to a \\(100 \\times (1 - 0.05)\\%\\) confidence interval, which is why we looked up the \\((0.05)/2 \\times 100 = 2.5\\)th percentile of the standard normal distribution and multiplied by \\(-1\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture11/Lec11.html#confidence-intervals-for-proportions-8",
    "href": "Pages/Lectures/Lecture11/Lec11.html#confidence-intervals-for-proportions-8",
    "title": "PSTAT 5A: Lecture 11",
    "section": "Confidence Intervals for Proportions",
    "text": "Confidence Intervals for Proportions\n\nIn practice: since the value of \\(p\\) is unknown, we typically replace \\(p\\) with \\(\\widehat{p}\\) to obtain an approximate confidence interval: \\[ \\widehat{p} \\pm z_\\alpha \\cdot \\sqrt{ \\frac{\\widehat{p}(1 - \\widehat{p})}{n}} \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture11/Lec11.html#worked-out-example-2",
    "href": "Pages/Lectures/Lecture11/Lec11.html#worked-out-example-2",
    "title": "PSTAT 5A: Lecture 11",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\n\nWorked-Out Example 2\n\n\n\n\nA veterinarian wishes to determine the true proportion of cats that suffer from FIV (Feline Immunodeficiency Virus). To that end, she takes a representative sample of 500 cats and finds that 3.2% of cats in this sample have FIV. Construct a 95% confidence interval for the true poportion of FIV-positive cats.\n\n\n\n\n\n\n\nWe simply plug into our formula from above: \\[ (0.032) \\pm 1.96 \\cdot \\sqrt{ \\frac{(0.032) \\cdot (1 - 0.032)}{500}} = \\boxed{0.032 \\pm 0.0155}\\] or, written out more explicitly, \\[ \\boxed{ [0.0165 \\ , \\ 0.0475] } \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture11/Lec11.html#interpreting-confidence-intervals",
    "href": "Pages/Lectures/Lecture11/Lec11.html#interpreting-confidence-intervals",
    "title": "PSTAT 5A: Lecture 11",
    "section": "Interpreting Confidence Intervals",
    "text": "Interpreting Confidence Intervals\n\nOkay, now that we have an example of a confidence interval under our belt, let’s talk about the correct interpretation of confidence intervals.\nThe following are all correct interpretations of our confidence interval:\n\nWe are 95% confident that the true proportion of FIV-positive cats is between 0.0165 and 0.0475.\nWe are 95% confident that the interval \\([0.0165 \\ , \\ 0.0475]\\) covers the true proportion of FIV-positive cats.\n\nHere is a technically incorrect way of interpreting the confidence interval: there is a 95% probability that the true proportion of FIV-positive cats lies between 0.0165 and 0.0475."
  },
  {
    "objectID": "Pages/Lectures/Lecture11/Lec11.html#interpreting-confidence-intervals-1",
    "href": "Pages/Lectures/Lecture11/Lec11.html#interpreting-confidence-intervals-1",
    "title": "PSTAT 5A: Lecture 11",
    "section": "Interpreting Confidence Intervals",
    "text": "Interpreting Confidence Intervals\n\nWhy is this typically rejected as an interpretation of a confidence interval?\nBecause this phrasing makes it sound as though the true proportion of FIV-positive cats is a random variable!\n\nThe true proportion of FIV positive cats is a fixed, deterministic value \\(p\\).\nWhat is random are the endpoints of our confidence interval!\nThis is why we phrase our interpretation in terms of “coverage”; it is to highlight the fact that the endpoints of our interval are where our uncertainty (i.e. randomness) comes into play.\n\nI grant that the above is a very subtle point. However, Statisticians are quite particular about wording when it comes to interpreting confidence intervals. As such, we will be particular in this class as well!"
  },
  {
    "objectID": "Pages/Lectures/Lecture11/Lec11.html#your-turn",
    "href": "Pages/Lectures/Lecture11/Lec11.html#your-turn",
    "title": "PSTAT 5A: Lecture 11",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\n\n\n\n\n\nExercise 3\n\n\n\n\nAs a film critic, you are interested in determining the true proportion of people that have watched The Mandalorian. You take a representative sample of 100 people, and note that 47% of these people have watched The Mandalorian.\n\nConstruct a 95% confidence interval for the proportion of people that have watched The Mandalorian, and interpret your interval in the context of the problem.\nWhen constructing an 85% confidence interval for the proportion of people that have watched The Mandalorian, would you expect this interval to be wider or shorter than the interval you found in part (a)?\nNow, actually construct an 85% confidence interval for the proportion of people that have watched The Mandalorian and see if this agrees with your answer to part (b)."
  },
  {
    "objectID": "Pages/Lectures/Lecture11/Lec11.html#your-turn-1",
    "href": "Pages/Lectures/Lecture11/Lec11.html#your-turn-1",
    "title": "PSTAT 5A: Lecture 11",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\n\n\n\n\n\nExercise 4\n\n\n\n\nAs a political scientist, Morgan would like to know the true proportion of people in a city that support Candidate A in an upcoming election. To that effect, they take a representative sample of 120 people and determine that 51% of these sampled individuals support Candidate A.\nConstruct an 87% confidence interval for the true proportion of people that support Candidate A."
  },
  {
    "objectID": "Pages/Lectures/Lecture10/Lec10.html#leadup",
    "href": "Pages/Lectures/Lecture10/Lec10.html#leadup",
    "title": "PSTAT 5A: Lecture 10",
    "section": "Leadup",
    "text": "Leadup\n\nLet’s, for the moment, don our Sociology hats and say we’re interested in estimating the average monthly income of US Citizens.\nSurveying every single US Citizen and recording their income is not feasible- doing so would be far too expensive (both in terms of monetary cost as well as temporal cost)\nInstead, a natural idea is to take a sample of some subset of US Citizens, and record the average monthly income of these sampled individuals.\nNow, here’s a question: given two separate samples of, say, 125 US Citizens- do we expect the average income of these two samples to be exactly the same, or slightly different?\n\nProbably slightly different!"
  },
  {
    "objectID": "Pages/Lectures/Lecture10/Lec10.html#leadup-1",
    "href": "Pages/Lectures/Lecture10/Lec10.html#leadup-1",
    "title": "PSTAT 5A: Lecture 10",
    "section": "Leadup",
    "text": "Leadup\n\nSo, it seems we have two things to keep track of:\n\nThe true average monthly income of US Citizens\nThe average monthly income of a sample of US Citizens.\n\nBy the last point on the previous slide, we can see that the second quantity above is a random variable.\n\nThis is an example of a sample statistic, which is basically any quantity that is computed from our data.\n\nThe true average monthly income of US Citizens is a fixed number, which we call a population parameter.\n\nIn general, a population parameter is just a parameter that relates to the population (e.g. mean, median, variance, etc.)"
  },
  {
    "objectID": "Pages/Lectures/Lecture10/Lec10.html#inferential-statistics",
    "href": "Pages/Lectures/Lecture10/Lec10.html#inferential-statistics",
    "title": "PSTAT 5A: Lecture 10",
    "section": "Inferential Statistics",
    "text": "Inferential Statistics\n\nWe consider the population to be some large group we are interested in studying.\n\nThe population is governed by some set of parameters (e.g. mean, median, variance, etc.)\n\nFrom the population we draw a sample (which is random!), and compute sample statistics to try and make inference about the population parameters.\nThis is the structure of inferential statistics."
  },
  {
    "objectID": "Pages/Lectures/Lecture10/Lec10.html#parameter-vs.-statistic",
    "href": "Pages/Lectures/Lecture10/Lec10.html#parameter-vs.-statistic",
    "title": "PSTAT 5A: Lecture 10",
    "section": "Parameter vs. Statistic",
    "text": "Parameter vs. Statistic\n\nIt is extremely important to be able to distinguish what is a population parameter from what a statistic is.\nThere are a couple of ways to do this.\nThe first is to consider the general structure of the situation: if a given quatntity is describing the population, then it must be a population parameter.\n\nIf, instead, it is describing a sample, then it must be a sample statistic.\n\nThe other way to think about this is through randomness: remember that different samples correspond to different observed values of our sample statistic.\n\nSo, imagine asking yourself: “if I took a different sample, would this quantity change?” If so, then it is a sample statistic."
  },
  {
    "objectID": "Pages/Lectures/Lecture10/Lec10.html#worked-out-example",
    "href": "Pages/Lectures/Lecture10/Lec10.html#worked-out-example",
    "title": "PSTAT 5A: Lecture 10",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\n\nWorked-Out Example 1\n\n\n\n\nA veterinarian wishes to determine the true proportion of cats that suffer from FIV (Feline Immunodeficiency Virus). To that end, she takes a representative sample of 100 cats and finds that 3.2% of cats in this sample have FIV.\n\nIdentify the population.\nIdentify the sample.\nIdentify the population parameter of interest.\nIs the value of \\(3.2\\%\\) a population parameter, or an observed instance of a sample statistic?"
  },
  {
    "objectID": "Pages/Lectures/Lecture10/Lec10.html#solution",
    "href": "Pages/Lectures/Lecture10/Lec10.html#solution",
    "title": "PSTAT 5A: Lecture 10",
    "section": "Solution",
    "text": "Solution\n\nthe population is the corpus of all cats in the world, since we the veterinarian seeks to describe the prevalence of FIV among all cats.\nIn this context, the sample is the set of 100 cats the veterinarian examined.\nThe population parameter of interest is \\(p =\\) “the true proportion of cats that suffer from FIV”.\nThe value of 3.2% is a sample statistic, because if the veterinarian had taken a different sample of 100 cats she likely would have observed a different proportion of FIV-positive cats."
  },
  {
    "objectID": "Pages/Lectures/Lecture10/Lec10.html#your-turn",
    "href": "Pages/Lectures/Lecture10/Lec10.html#your-turn",
    "title": "PSTAT 5A: Lecture 10",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\n\n\n\n\n\nExercise 1\n\n\n\n\nA group of (slightly bored) college students would like to determine the true average amount of soda (in liters) in 1-liter soda bottles. To that effect, they purchas 12 different 1-liter soda bottles and find the average amount of soda in these 12 bottles is 0.98L.\n\nIdentify the population.\nIdentify the sample.\nIdentify the population parameter of interest.\nIs the value of 0.98 a population parameter, or an observed instance of a sample statistic?"
  },
  {
    "objectID": "Pages/Lectures/Lecture10/Lec10.html#different-population-parameters",
    "href": "Pages/Lectures/Lecture10/Lec10.html#different-population-parameters",
    "title": "PSTAT 5A: Lecture 10",
    "section": "Different Population Parameters",
    "text": "Different Population Parameters\n\nSo far we’ve seen examples of two different population parameters:\n\nA population proportion\nA population mean\n\nOther population parameters exist! For instance, we could talk about the population median, the population variance, or even the population IQR.\nUp until now, I’ve been pretty vague about what “inferences” mean. This is because “making inferences” is a broad term!\nOne part of making inferences is trying to estimate the value of a population parameter.\n\nThis process is called parameter estimation."
  },
  {
    "objectID": "Pages/Lectures/Lecture10/Lec10.html#parameter-estimation",
    "href": "Pages/Lectures/Lecture10/Lec10.html#parameter-estimation",
    "title": "PSTAT 5A: Lecture 10",
    "section": "Parameter Estimation",
    "text": "Parameter Estimation\n\nBoth Worked-Out Example 1 and Exercise 1 were (implicitly) problems about parmaeter estimation.\n\nIn Worked-Out Example 1, the veterinarian wanted to estimate the true proportion of FIV-positive cats\nIn Exercise 1, the college students wanted to estimate the true average amount of soda in 1L soda bottles.\n\nIn general, we use a sample statistic to estimate a population parameter. Some common estimators of some population parameters are:"
  },
  {
    "objectID": "Pages/Lectures/Lecture10/Lec10.html#a-wrench-in-the-spanner",
    "href": "Pages/Lectures/Lecture10/Lec10.html#a-wrench-in-the-spanner",
    "title": "PSTAT 5A: Lecture 10",
    "section": "A Wrench in the Spanner",
    "text": "A Wrench in the Spanner\n\nNow, can anyone see a potential problem in using a sample statistic to estimate a population parameter?\n\nThat’s right- the sample statistics are random!\nFor example, every time the veterinarian in Worked-Out Example 1 took a new sample of 100 cats she would obtain a different estimate for the true proportion of cats that have FIV.\nSo, we need some way to express the uncertainty that comes from the randomness of these sample statistics.\n\nTo explore this, let’s do a live demo using Python.\n\nIn the demo, I will simulate drawing several samples of 500 cats, recording the proportion of FIV-positive cats, and drawing the distribution of these sample proportions.\n\nBy the way, the distribution of a sample statistic is called the sampling distribution of that statistic."
  },
  {
    "objectID": "Pages/Lectures/Lecture10/Lec10.html#result",
    "href": "Pages/Lectures/Lecture10/Lec10.html#result",
    "title": "PSTAT 5A: Lecture 10",
    "section": "Result",
    "text": "Result\n\nBefore we summarize these results, we should introduce a bit of notation.\n\nWe let \\(\\widehat{P}\\), read “p-hat” denote the random variable that is the proportion of a hypothetical sample.\nWe let \\(\\widehat{p}\\) denote an observed instance of \\(\\widehat{P}\\); i.e. the sample proportion of a particular sample.\n\nWith this notation in mind, we can see that the demo illustrated the following fact: \\(\\widehat{P}\\) is normally distributed!\n\nIt turns out that the expected value of \\(\\widehat{P}\\) is \\(p\\), the true population proportion, and the standard deviation (which, in the context of estimation, is sometimes called the standard error) is \\[ \\sqrt{\\frac{p(1 - p)}{n} } \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture10/Lec10.html#central-limit-theorem-for-proportions",
    "href": "Pages/Lectures/Lecture10/Lec10.html#central-limit-theorem-for-proportions",
    "title": "PSTAT 5A: Lecture 10",
    "section": "Central Limit Theorem for Proportions",
    "text": "Central Limit Theorem for Proportions\n\n\n\n\n\n\nImportant\n\n\n\nIf we have reasonably representative samples taken from a population with true proportion \\(p\\) and let \\(\\widehat{P}\\) denote the sample proportion, then \\[ \\widehat{P} \\sim \\mathcal{N}\\left( p, \\ \\sqrt{ \\frac{p(1 - p)}{n} } \\right) \\] provided that\n\n\n\\(np \\geq 10\\)\n\\(n(1 - p) \\geq 10\\) note that this has been changed from the version presented in lecture last week; please see the corresponding course announcement\n\n\n\n\n\n\n\nThe two conditions above are sometimes referred to as the success-failure conditions, and must be satisfied in order to invoke the Central Limit Theorem for Proportions (CLTP).\n\nWe’ll talk a bit more about “reasonably representative” samples later in the course (time-permitting)."
  },
  {
    "objectID": "Pages/Lectures/Lecture10/Lec10.html#worked-out-example-2",
    "href": "Pages/Lectures/Lecture10/Lec10.html#worked-out-example-2",
    "title": "PSTAT 5A: Lecture 10",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\n\nWorked-Out Example 1\n\n\n\n\nSuppose a recent study has revealed that 87% of Americans are in favor of offering more healthy options at fast-food restaurants. A surveyor takes a representative sample of size 120 Americans, and records the proportion of these Americans that support offering more healthy options at fast-food restaurants.\n\nDefine the random variable of interest.\nWhat is the probability that fewer than 90% of people in the sample support offering more healthy options at fast-food restaurants?\nWhat is the probability that the proportion of people in the sample who support offering more healthy options at fast-food restaurants lies within 2% of the true proportion of 87%?"
  },
  {
    "objectID": "Pages/Lectures/Lecture10/Lec10.html#solutions",
    "href": "Pages/Lectures/Lecture10/Lec10.html#solutions",
    "title": "PSTAT 5A: Lecture 10",
    "section": "Solutions",
    "text": "Solutions\n\nLet \\(\\widehat{P} =\\) the proportion of Americans in the sample of size \\(n = 100\\) that support offering more healthy options at fast-food restaurants."
  },
  {
    "objectID": "Pages/Lectures/Lecture10/Lec10.html#solutions-1",
    "href": "Pages/Lectures/Lecture10/Lec10.html#solutions-1",
    "title": "PSTAT 5A: Lecture 10",
    "section": "Solutions",
    "text": "Solutions\n\nWe would like to utilize the Central Limit Theorem for Proportions. First, we check:\n\n\\(np = (100) \\cdot (0.87) = 87 \\geq 10 \\ \\checkmark\\)\n\\(np(1 - p) = (100) \\cdot (0.87) \\cdot (1 - 0.87) \\approx 11.31 \\geq 10 \\ \\checkmark\\) these are the old conditions; I encourage you to check the new ones on your own!\n\n\nSince both of the success-failure conditions are satisfied, we know that \\[ \\widehat{P} \\sim \\mathcal{N}\\left(0.87, \\ \\sqrt{\\frac{0.87 \\cdot (1 - 0.87)}{100}} \\right) \\sim \\mathcal{N}\\left(0.87, \\ 0.0336 \\right) \\] and so \\[ \\mathbb{P}(\\widehat{P} \\leq 0.9) = \\mathbb{P}\\left( \\frac{\\widehat{P} - 0.87}{0.0336} \\leq \\frac{0.9 - 0.87}{0.0336} \\right) = \\mathbb{P}\\left( \\frac{\\widehat{P} - 0.87}{0.0336} \\leq 0.89 \\right) \\] which equates to around \\(\\boxed{0.8133 = 81.33\\%}\\)"
  },
  {
    "objectID": "Pages/Lectures/Lecture10/Lec10.html#solutions-2",
    "href": "Pages/Lectures/Lecture10/Lec10.html#solutions-2",
    "title": "PSTAT 5A: Lecture 10",
    "section": "Solutions",
    "text": "Solutions\n\nWe seek \\(\\mathbb{P}(0.85 \\leq \\widehat{P} \\leq 0.89)\\).\n\nFirst, we write this as \\(\\mathbb{P}(\\widehat{P} \\leq 0.89) - \\mathbb{P}(\\widehat{P} \\leq 0.85)\\)\nNext, we standardize: \\[\\begin{align*}\nz_1 & = \\frac{0.89 - 0.87}{0.0336} \\approx 0.60   \\\\\nz_2 & = \\frac{0.5 - 0.87}{0.0336} \\approx -0.60\n\\end{align*}\\]\nFinally, we consult a table to see that the desired probability is \\[ 0.7257 - 0.2743 = \\boxed{0.4514 = 45.14\\%} \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture10/Lec10.html#your-turn-1",
    "href": "Pages/Lectures/Lecture10/Lec10.html#your-turn-1",
    "title": "PSTAT 5A: Lecture 10",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\n\n\n\n\n\nExercise 2\n\n\n\n\nAt a certain company, it is known that 65% of employees are from underrepresented minorities (UMs). A representative sample of 80 employees is taken, and the proportion of people from UMs is recorded.\n\nDefine the random variable of interest.\nWhat is the probability that greater than than 50% of people in the sample are from UMs?\nWhat is the probability that the proportion of people in the sample who are from UMs lies within 5% of the true proportion of 65%?\n\n\n\n\n\n\nDue to the retroactive change in the success-failure conditions, I will post the written-out solutions to this exercise before Lecture 11."
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#multiple-populations",
    "href": "Pages/Lectures/Lecture17/Lec17.html#multiple-populations",
    "title": "PSTAT 5A: Lecture 17",
    "section": "Multiple Populations",
    "text": "Multiple Populations\n\nSo far, we’ve talked about constructing confidence intervals and performing hypothesis tests for both population proportions and population means.\nOne crucial thing to note is that everything we’ve done has been in the context of a single population\nSometimes, as Data Scientists, we may want to make claims about the differences between two populations\n\nE.g. Is the average monthly income in Santa Barbara different from the average monthly income in San Francisco?\nE.g. Is the proportion of people who test positive for a disease in one country different than the proportion that test positive in a second country?"
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#two-populations-1",
    "href": "Pages/Lectures/Lecture17/Lec17.html#two-populations-1",
    "title": "PSTAT 5A: Lecture 17",
    "section": "Two Populations",
    "text": "Two Populations\n\nStatistically: we are imagining two populations, Population 1 and Population 2, governed by parameters \\(\\theta_1\\) and \\(\\theta_2\\), respectively, and trying to make claims about the relationship between \\(\\theta_1\\) and \\(\\theta_2\\).\n\nFor example, we could consider two populations with means \\(\\mu_1\\) and \\(\\mu_2\\), respectively, and try to make claims about whether or not \\(\\mu_1\\) and \\(\\mu_2\\) are equal.\n\nThe trick Statisticians use is to think in terms of the difference \\(\\theta_2 - \\theta_1.\\)\n\nFor example, if our null hypothesis is that \\(\\theta_1 = \\theta_2\\), this can be rephrased as \\(H_0: \\ \\theta_2 - \\theta_1 = 0\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#two-populations-2",
    "href": "Pages/Lectures/Lecture17/Lec17.html#two-populations-2",
    "title": "PSTAT 5A: Lecture 17",
    "section": "Two Populations",
    "text": "Two Populations\n\nThe reason we do this is because we have now effectively reduced our two-parameter problem into a one-parameter problem, involving only the parameter \\(\\delta := \\theta_2 - \\theta_1\\).\nNow, we will need a point estimator of \\(\\delta\\).\nIf \\(\\widehat{\\theta}_1\\) and \\(\\widehat{\\theta}_2\\) are point estimators of \\(\\theta_1\\) and \\(\\theta_2\\), respectively, then a natural point estimator of \\(\\delta\\) is \\(\\widehat{\\delta} = \\widehat{\\theta}_2 - \\widehat{\\theta}_1\\).\n\nFor example, a natural point estimator for the difference \\(\\mu_2 - \\mu_1\\) of population means is \\(\\overline{X}_2 - \\overline{X}_1\\), the difference in sample means."
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#two-populations-3",
    "href": "Pages/Lectures/Lecture17/Lec17.html#two-populations-3",
    "title": "PSTAT 5A: Lecture 17",
    "section": "Two Populations",
    "text": "Two Populations\n\nWe will ultimately need access to the sampling distribution of \\(\\widehat{\\delta}\\).\nBefore delving into that, however, we will need a little more probability knowledge; specifically, knowledge on how linear combinations of random variables work."
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#linear-combinations-of-random-variables-1",
    "href": "Pages/Lectures/Lecture17/Lec17.html#linear-combinations-of-random-variables-1",
    "title": "PSTAT 5A: Lecture 17",
    "section": "Linear Combinations of Random Variables",
    "text": "Linear Combinations of Random Variables\n\nRecall, from many weeks ago, that a random variable \\(X\\) is simply some numerical variable that tracks a random outcome of an experiment.\n\nE.g. number of heads in 10 tosses of a fair coin; number of people in a population that test positive for a disease; etc.\n\nA random variable \\(X\\), whether it be discrete or continuous, has an expected value \\(\\mathbb{E}[X]\\) and a variance \\(\\mathrm{Var}(X)\\).\nNow, suppose we have two random variables \\(X\\) and \\(Y\\), and three constants \\(a\\), \\(b\\), and \\(c\\).\nOur task for now is to say as much as we can about the quantity \\(aX + bY + c\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#linear-combinations-of-random-variables-2",
    "href": "Pages/Lectures/Lecture17/Lec17.html#linear-combinations-of-random-variables-2",
    "title": "PSTAT 5A: Lecture 17",
    "section": "Linear Combinations of Random Variables",
    "text": "Linear Combinations of Random Variables\n\n\n\n\n\n\n\nTheorem\n\n\n\n\nGiven two random variables \\(X\\) and \\(Y\\), and constants \\(a, \\ b,\\) and \\(c\\), \\[ \\mathbb{E}[aX + bY + c] = a \\cdot \\mathbb{E}[X] + b \\cdot \\mathbb{E}[Y] + c \\]\n\n\n\n\n\n\n\nYou will prove this in the discrete case on your upcoming homework.\nAs an example: if \\(\\mathbb{E}[X] = 2\\) and \\(\\mathbb{E}[Y] = -1\\), then \\[\\mathbb{E}[2X + 3Y + 1] = 2(2) + 3(-1) + 1 = 2 \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#linear-combinations-of-random-variables-3",
    "href": "Pages/Lectures/Lecture17/Lec17.html#linear-combinations-of-random-variables-3",
    "title": "PSTAT 5A: Lecture 17",
    "section": "Linear Combinations of Random Variables",
    "text": "Linear Combinations of Random Variables\n\n\n\n\n\n\n\nTheorem\n\n\n\n\nGiven two independent random variables \\(X\\) and \\(Y\\), and constants \\(a, \\ b,\\) and \\(c\\), \\[ \\mathrm{Var}(aX + bY + c) = a^2 \\mathrm{Var}(X) + b^2 \\mathrm{Var}(Y) \\]\n\n\n\n\n\n\n\nYou will not be responsible for the proof of this fact.\nAlso, we haven’t explicitly talked about what independence means in the context of random variables; for now, suffice it to say that it works analogously to the concept of independence of events. That is, if the random variables \\(X\\) and \\(Y\\) come from two experiments that don’t have any relation to each otehr, then \\(X\\) and \\(Y\\) will be independent."
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#two-populations-4",
    "href": "Pages/Lectures/Lecture17/Lec17.html#two-populations-4",
    "title": "PSTAT 5A: Lecture 17",
    "section": "Two Populations",
    "text": "Two Populations\n\nAlright, so what does this mean in the context of our two-proportion problem?\nWell, for one thing, we can easily construct a confidence interval for \\((\\theta_2 - \\theta_1)\\) using: \\[ (\\widehat{\\theta}_2 - \\widehat{\\theta}_1) \\pm c \\cdot \\sqrt{\\mathrm{Var}(\\widehat{\\theta}_1) + \\mathrm{Var}(\\widehat{\\theta}_2)} \\] where \\(c\\) is a constant that is determined by both the sampling distribution of \\(\\widehat{\\theta}_2 - \\widehat{\\theta}_1\\) as well as our confidence level.\nBy the way, can anyone tell me why the variances are added, and not subtracted?"
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#two-means",
    "href": "Pages/Lectures/Lecture17/Lec17.html#two-means",
    "title": "PSTAT 5A: Lecture 17",
    "section": "Two Means",
    "text": "Two Means\n\nTo make things more specific, let’s consider comparing two population means.\nSpecifically: imagine we have two populations (which we will call Population 1 and Population 2), governed by population means \\(\\mu_1\\) and \\(\\mu_2\\), respectively.\nFor now, let’s focus a two-sided test, where our hypotheses are \\[\\left[ \\begin{array}{rr}\nH_0:    & \\mu_1 = \\mu_2   \\\\\nH_A:    & \\mu_1 \\neq \\mu_2\n\\end{array} \\right.\\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#two-means-1",
    "href": "Pages/Lectures/Lecture17/Lec17.html#two-means-1",
    "title": "PSTAT 5A: Lecture 17",
    "section": "Two Means",
    "text": "Two Means\n\nAgain, it’s customary to rephrase things to be in terms of differences: \\[\\left[ \\begin{array}{rr}\nH_0:    & \\mu_2 - \\mu_1 = 0   \\\\\nH_A:    & \\mu_2 - \\mu_1 \\neq 0\n\\end{array} \\right.\\]\nNow, we need data!\nSuppose we have a sample \\(X = \\{X_i\\}_{i=1}^{n_1}\\) taken from Population 1 and a sample \\(Y = \\{Y_i\\}_{i=1}^{n_2}\\) taken from Population 2.\n\nNote that we are allowing for different sample sizes, \\(n_1\\) and \\(n_2\\)!\n\nLet’s also assume that, in addition to being representative samples, the two samples are both independent within themselves and independent from each other (i.e. assume the \\(X_i\\)’s and \\(Y_i\\)’s are independent, and that the \\(X\\)’s are independent from the \\(Y\\)’s)"
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#two-means-2",
    "href": "Pages/Lectures/Lecture17/Lec17.html#two-means-2",
    "title": "PSTAT 5A: Lecture 17",
    "section": "Two Means",
    "text": "Two Means\n\nAgain, we are interested in finding a point estimator for \\(\\mu_2 - \\mu_1\\).\nHere’s a question: do we have a natural point estimator for \\(\\mu_2\\)? What about for \\(\\mu_1\\)?\nSo, it seems that a natural point estimator for \\(\\delta = \\mu_2 - \\mu_1\\) is \\[ \\widehat{\\delta} = \\overline{Y} - \\overline{X} \\]\nWhat is the sampling distribution of \\(\\widehat{\\delta}\\)?\nWell, there are a few cases to consider."
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#sampling-distribution-of-widehatdelta",
    "href": "Pages/Lectures/Lecture17/Lec17.html#sampling-distribution-of-widehatdelta",
    "title": "PSTAT 5A: Lecture 17",
    "section": "Sampling Distribution of \\(\\widehat{\\delta}\\)",
    "text": "Sampling Distribution of \\(\\widehat{\\delta}\\)\n\nSuppose that our two populations had known variances \\(\\sigma_1^2\\) and \\(\\sigma_2^2\\), respectively.\nThen, if both \\(\\overline{X}\\) and \\(\\overline{Y}\\) were normally distributed, we could use a fact (from probability theory) that linear combinations of normally distributed random variables are also normally distributed to conclude that \\[ \\widehat{\\delta} \\sim \\mathcal{N}\\left( \\delta, \\ \\sqrt{ \\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2} } \\right) \\]\n\nSee the chalkboard for more details"
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#the-test-statistc",
    "href": "Pages/Lectures/Lecture17/Lec17.html#the-test-statistc",
    "title": "PSTAT 5A: Lecture 17",
    "section": "The Test Statistc",
    "text": "The Test Statistc\n\nIn this case, a natural candidate for our test statistic would be \\[ \\frac{\\widehat{\\delta}}{\\sqrt{ \\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}} =  \\frac{\\overline{Y} - \\overline{X}}{\\sqrt{ \\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}} \\] as, under the null, this would follow a standard normal distribution.\nHowever, there are a few problems with this.\nFor one, it requires both \\(\\overline{X}\\) and \\(\\overline{Y}\\) to be normally distributed, which we know is not always the case.\nAlright, that’s fine though- so long as our sample sizes are large enough, the Central Limit Theorem kicks in and we can be reasonably certain that \\(\\overline{X}\\) and \\(\\overline{Y}\\) will be pretty close to normally distributed."
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#the-test-statistic",
    "href": "Pages/Lectures/Lecture17/Lec17.html#the-test-statistic",
    "title": "PSTAT 5A: Lecture 17",
    "section": "The Test Statistic",
    "text": "The Test Statistic\n\nHowever, the main problem in using this test statistic is that it requires access to the population variances \\(\\sigma_1^2\\) and \\(\\sigma_2^2\\)!\nAny ideas on how to remedy this?\n\nRight; let’s just replace the population variances with their sample analogues: \\[ \\mathrm{TS} = \\frac{\\overline{Y} - \\overline{X}}{\\sqrt{ \\frac{s_X^2}{n_1} + \\frac{s_Y^2}{n_2}}}\\] where \\[\\begin{align*}\ns_X^2   & = \\frac{1}{n_1 - 1} \\sum_{i=1}^{n_1} (X_i - \\overline{X})^2   \\\\\ns_Y^2   & = \\frac{1}{n_2 - 1} \\sum_{i=1}^{n_2} (Y_i - \\overline{Y})^2\n  \\end{align*}\\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#the-test-statistic-1",
    "href": "Pages/Lectures/Lecture17/Lec17.html#the-test-statistic-1",
    "title": "PSTAT 5A: Lecture 17",
    "section": "The Test Statistic",
    "text": "The Test Statistic\n\nAny guesses on what distribution this follows under the null?\nIf you said t….. you’d be wrong! (But pretty close.)\nIt turns out that, under the null (i.e. assuming that \\(\\mu_1 = \\mu_2\\), or, equivalently, that \\(\\delta = \\mu_2 - \\mu_1 = 0\\)), this test statistic approximately follows a t-distribution.\nWhat degrees of freedom?\nThat’s right: \\[ \\mathrm{df} = \\mathrm{round}\\left\\{ \\frac{ \\left[ \\left( \\frac{s_X^2}{n_1} \\right) + \\left( \\frac{s_Y^2}{n_2} \\right) \\right]^2 }{ \\frac{\\left( \\frac{s_X^2}{n_1} \\right)^2}{n_1 - 1} + \\frac{\\left( \\frac{s_Y^2}{n_2} \\right)^2}{n_2 - 1} } \\right\\} \\]\n\nThis is related to what is known as the Satterthwaite Approximation, sometimes called the Welch-Satterthwaite Equation"
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#the-test",
    "href": "Pages/Lectures/Lecture17/Lec17.html#the-test",
    "title": "PSTAT 5A: Lecture 17",
    "section": "The Test",
    "text": "The Test\n\nAlright, so we finally have a test statistic: \\[ \\mathrm{TS} = \\frac{\\overline{Y} - \\overline{X}}{\\sqrt{ \\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}} \\] and its (approximate) distribution under the null: \\[ \\mathrm{TS} \\stackrel{H_0}{\\sim} t_{\\nu} \\] where \\(\\nu\\) is given by the Satterthwaite Approximation.\nRecall our hypotheses: \\[ \\left[ \\begin{array}{rr}\nH_0:    & \\mu_2 - \\mu_1 = 0   \\\\\nH_A:    & \\mu_2 - \\mu_1 \\neq 0\n\\end{array} \\right. \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#the-test-1",
    "href": "Pages/Lectures/Lecture17/Lec17.html#the-test-1",
    "title": "PSTAT 5A: Lecture 17",
    "section": "The Test",
    "text": "The Test\n\nWe can see that large values of \\(|\\mathrm{TS}|\\) lead credence to the alternative over the null; as such, our decision will take the form \\[ \\texttt{decision}(\\mathrm{TS}) = \\begin{cases} \\texttt{reject } H_0 & \\text{if } |\\mathrm{TS}| > c \\\\ \\texttt{fail to reject } H_0 & \\text{otherwise}\\\\ \\end{cases}  \\] where \\(c\\) is the appropriately-selected quantile of the appropriate t-distribution."
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#worked-out-example",
    "href": "Pages/Lectures/Lecture17/Lec17.html#worked-out-example",
    "title": "PSTAT 5A: Lecture 17",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\n\nWorked-Out Example 1\n\n\n\n\nGaucho Gourmande has two locations: one in Goleta and one in Santa Barbara. The owner would like to determine whether the average revenue generated by the two locations are equal or not. To that end, he computes the net revenue generated by the Goleta location over 30 days and also computes the net revenue generated by the Santa Barbara location over 35 days (assume all of the necessary independence conditions hold), and produced the following information:\n\\[\\begin{array}{r|cc}\n                    & \\text{Sample Average}     & \\text{Sample Standard Deviation}    \\\\\n  \\hline\n  \\textbf{Goleta}   &     \\$13                    & \\$3.45        \\\\\n  \\textbf{Santa Barbara}   &     \\$15                    & \\$4.23\n\\end{array}\\]\nTest the owner’s claims at an \\(\\alpha = 0.05\\) level of significance, using a two-sided alternative."
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#solutions",
    "href": "Pages/Lectures/Lecture17/Lec17.html#solutions",
    "title": "PSTAT 5A: Lecture 17",
    "section": "Solutions",
    "text": "Solutions\n\nOur first step should be to figure out what “Population 1” and “Population 2” are in the context of the problem.\nLet “Goleta Location” be Population 1 and “Santa Barbara Location” be Population 2.\n\nIt is perfectly acceptable to swap these two, but just be sure you remain consistent throughout the problem!\nAlso, I will expect you to explicitly write out your definitions of the populations (like above), even if the problem doesn’t explicitly ask you to do so.\n\nIn this way, \\[ \\overline{X} = 13; \\quad s_X = 3.45; \\quad \\overline{Y} = 15; \\quad s_Y = 4.23 \\]\nAdditionally, \\(n_1 = 30\\) and \\(n_2 = 35\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#solutions-1",
    "href": "Pages/Lectures/Lecture17/Lec17.html#solutions-1",
    "title": "PSTAT 5A: Lecture 17",
    "section": "Solutions",
    "text": "Solutions\n\nNow, let’s compute the value of the test statistic. \\[ \\mathrm{TS} =   \\frac{\\overline{Y} - \\overline{X}}{\\sqrt{ \\frac{s_X^2}{n_1} + \\frac{s_Y^2}{n_2}}} = \\frac{15 - 13}{\\sqrt{\\frac{3.45^2}{30}  + \\frac{4.23^2}{35} }} = 2.10 \\]\nWe should next figure out the degrees of freedom: \\[\\begin{align*}\n\\mathrm{df}   &  = \\mathrm{round}\\left\\{ \\frac{ \\left[ \\left( \\frac{s_X^2}{n_1} \\right) + \\left( \\frac{s_Y^2}{n_2} \\right) \\right]^2 }{ \\frac{\\left( \\frac{s_X^2}{n_1} \\right)^2}{n_1 - 1} + \\frac{\\left( \\frac{s_Y^2}{n_2} \\right)^2}{n_2 - 1} } \\right\\}     \\\\\n  & =  \\mathrm{round}\\left\\{ \\frac{ \\left[ \\left( \\frac{3.45^2}{30} \\right) + \\left( \\frac{4.23^2}{35} \\right) \\right]^2 }{ \\frac{\\left( \\frac{3.45^2}{30} \\right)^2}{30 - 1} + \\frac{\\left( \\frac{4.23^2}{35} \\right)^2}{35 - 1} } \\right\\} = 63\n\\end{align*}\\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#solutions-2",
    "href": "Pages/Lectures/Lecture17/Lec17.html#solutions-2",
    "title": "PSTAT 5A: Lecture 17",
    "section": "Solutions",
    "text": "Solutions\n\nAt this point, we could either proceed using critical values or using p-values.\nLet’s use p-values, for practice.\nOur p-value is computed as\n\n\n\nimport scipy.stats as sps\n2*sps.t.cdf(-2.10, 63)\n\n0.03973904581390475\n\n\n\n\nThis is below our level of significance \\(\\alpha = 0.05\\) meaning we would reject the null.\nIf we wanted to instead use critical values:\n\n\n\n-sps.t.ppf(0.05, 63)\n\n1.6694022215079614\n\n\n\n\nThis means our critical value is 1.67; since \\(|\\mathrm{TS}| = |2.10| = 2.10 > 1.67\\), we would again reject at an \\(\\alpha = 0.05\\) level of significance."
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#solutions-3",
    "href": "Pages/Lectures/Lecture17/Lec17.html#solutions-3",
    "title": "PSTAT 5A: Lecture 17",
    "section": "Solutions",
    "text": "Solutions\n\nAt a 5% level of significance, there was sufficient evidence to reject the owner’s claims that the revenue generated by the two locations are equal, in favor of the alternative that the revenue generated by the two locations are not equal."
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#extensions",
    "href": "Pages/Lectures/Lecture17/Lec17.html#extensions",
    "title": "PSTAT 5A: Lecture 17",
    "section": "Extensions",
    "text": "Extensions\n\nUnsurprisingly, we can adapt the above procedure to account for one-sided alternatives as well.\nFor instance, suppose we wish to test \\[ \\left[ \\begin{array}{rr}\nH_0:    & \\mu_1 = \\mu_2   \\\\\nH_A:    & \\mu_1 < \\mu_2\n\\end{array} \\right.\\]\nAgain, we rephrase things as: \\[ \\left[ \\begin{array}{rr}\nH_0:    & \\mu_2 - \\mu_1 = 0   \\\\\nH_A:    & \\mu_2 - \\mu_1 > 0\n\\end{array} \\right.\\] which is now a familiar upper-tailed test on \\(\\delta = \\mu_2 - \\mu_1\\) and \\(\\mu_0 = 0.\\)"
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#extensions-1",
    "href": "Pages/Lectures/Lecture17/Lec17.html#extensions-1",
    "title": "PSTAT 5A: Lecture 17",
    "section": "Extensions",
    "text": "Extensions\n\nSpecifically, we would take the same test statistic (which would still follow the same distribution under the null) and use the decision rule \\[ \\texttt{decision}(\\mathrm{TS}) = \\begin{cases} \\texttt{reject } H_0 & \\text{if } \\mathrm{TS} > c \\\\ \\texttt{fail to reject } H_0 & \\text{otherwise}\\\\ \\end{cases}  \\] where \\(c\\) is the appropriate quantile of the approximate t distribution (with degrees of freedom given by the Satterthwaite Approximation)."
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#leadup",
    "href": "Pages/Lectures/Lecture17/Lec17.html#leadup",
    "title": "PSTAT 5A: Lecture 17",
    "section": "Leadup",
    "text": "Leadup\n\nConsider the following situation: a new drug claims to significantly lower systolic blood pressure.\nTo ensure these claims are validated, a clinical trial collects several volunteers and groups them into four groups: a control group, and three groups which each are administered a different dosage of the drug.\nIf the drug is truly ineffective, we would imagine the average systolic blood pressure of each group to be fairly similar to the average systolic blood pressures of the other groups.\nIn other words, given \\(k\\) groups, each with some population mean \\(\\mu_i\\) (for \\(i = 1, 2, \\cdots, k\\)), we wish to determine whether or not all of the \\(\\mu_i\\)’s are the same."
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#anova",
    "href": "Pages/Lectures/Lecture17/Lec17.html#anova",
    "title": "PSTAT 5A: Lecture 17",
    "section": "ANOVA",
    "text": "ANOVA\n\nThis is the basic setup of Analysis of Variance (often abbreviated as ANOVA).\nGiven \\(k\\) groups, each with mean \\(\\mu_i\\), we wish to test the null hypothesis that all group means are equal (i.e. \\(H_0: \\ \\mu_1 = \\mu_2 = \\cdots = \\mu_k\\)) against the alternative that at least one of the group means differs significantly from the others.\n\n\n\n\n\n\n\n\nCAUTION\n\n\nNote the alternative hypothesis!\n\n\n\n\n\nIt is NOT correct to write the alternative as \\(H_A: \\ \\mu_1 \\neq \\mu_2 \\neq \\cdots \\neq \\mu_k\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#anova-1",
    "href": "Pages/Lectures/Lecture17/Lec17.html#anova-1",
    "title": "PSTAT 5A: Lecture 17",
    "section": "ANOVA",
    "text": "ANOVA\n\nHere is the general idea.\nObservations within each group will have some amount of variability (by virtue of being random observations).\nHowever, the sample means (of the groups) themselves will also have some variability (again, due to the fact that sample means are random).\nThe question ANOVA seeks to answer is: is the variability between sample means greater than what we would expect due to chance alone?\n\nIf so, we may have reason to believe that at least one of the group means differs significantly from the others; i.e. we would have evidence to reject the null."
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#anova-2",
    "href": "Pages/Lectures/Lecture17/Lec17.html#anova-2",
    "title": "PSTAT 5A: Lecture 17",
    "section": "ANOVA",
    "text": "ANOVA\n\nIn practice, ANOVA relies on what is known as the F-statistic.\nThe F-statistic is computed as \\[ F = \\frac{\\mathrm{MS}_\\mathrm{G}}{\\mathrm{MS}_{\\mathrm{E}}} \\]\n\n\\(\\mathrm{MS}_{\\mathrm{G}}\\) can be thought of as a measure of variability between groups; i.e. as a sort of variance of the sample means\n\\(\\mathrm{MS}_{\\mathrm{E}}\\) can be thought of as a measure of variability within groups; i.e. as a sort of variance due to error/randomness.\n\nAs stated on the previous slide, when \\(\\mathrm{MS}_{\\mathrm{G}}\\) is much larger than \\(\\mathrm{MS}_{\\mathrm{E}}\\), i.e. when the variability between groups is much larger than the variability within groups, we would be more likely to reject the null that all group means are equal.\n\nHence, we would reject \\(H_0\\) for large values of \\(F\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#anova-3",
    "href": "Pages/Lectures/Lecture17/Lec17.html#anova-3",
    "title": "PSTAT 5A: Lecture 17",
    "section": "ANOVA",
    "text": "ANOVA\n\nThe formulas for computing \\(\\mathrm{MS}_{\\mathrm{G}}\\) and \\(\\mathrm{MS}_{\\mathrm{E}}\\) are not overly complicated, but can be a bit tedious.\n\nWe will return to them later.\n\nFor now, let’s talk a bit about the sampling distribution of \\(F\\).\nIt turns out that, if we assume observations within each group are normally distributed (which ends up being a very crucial assumption), the statistic \\(F\\) follows what is known as the F-distribution.\nAs such, the critical value of our test is the appropriate percentile of the F-distribution."
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#the-f-distribution",
    "href": "Pages/Lectures/Lecture17/Lec17.html#the-f-distribution",
    "title": "PSTAT 5A: Lecture 17",
    "section": "The F-Distribution",
    "text": "The F-Distribution\n\nThe F-distribution is quite different from the distributions we have encountered thus far.\nFor one thing, it admits only nonnegative values in its state space (i.e. it has state space \\([0, \\infty)\\)).\nAdditionally, it takes two parameters, referred to as the numerator degrees of freedom and the denominator degrees of freedom (sometimes abbreviated as just “degree of freedom 1” and “degree of freedom 2”.)\nTo notate the fact that a random variable \\(X\\) follows the F-distribution with degrees of freedom d1 and d2, respectively, we write \\[ X \\sim F_{\\texttt{d1}, \\ \\texttt{d2}} \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#the-f-distribution-1",
    "href": "Pages/Lectures/Lecture17/Lec17.html#the-f-distribution-1",
    "title": "PSTAT 5A: Lecture 17",
    "section": "The F-Distribution",
    "text": "The F-Distribution"
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#the-test-statistic-2",
    "href": "Pages/Lectures/Lecture17/Lec17.html#the-test-statistic-2",
    "title": "PSTAT 5A: Lecture 17",
    "section": "The Test Statistic",
    "text": "The Test Statistic\n\nRecall that our test statistic in ANOVA is \\[ F = \\frac{\\mathrm{MS}_\\mathrm{G}}{\\mathrm{MS}_{\\mathrm{E}}} \\]\nAs mentioned previously, if we assume normality within groups, then, under the null, \\(F\\) follows the F-distribution with \\(k - 1\\) and \\(n - k\\) degrees of freedom, respectively, where \\(k\\) is the number of groups and \\(n\\) is the combined number of observations: \\[ F = \\frac{\\mathrm{MS}_\\mathrm{G}}{\\mathrm{MS}_{\\mathrm{E}}}  \\stackrel{H_0}{\\sim} F_{k - 1, \\ n - k} \\]\nSince we reject only for large values of \\(F\\), our p-values are always computed as upper-tail probabilities:"
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#p-values-in-anova",
    "href": "Pages/Lectures/Lecture17/Lec17.html#p-values-in-anova",
    "title": "PSTAT 5A: Lecture 17",
    "section": "p-Values in ANOVA",
    "text": "p-Values in ANOVA"
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#anova-tables",
    "href": "Pages/Lectures/Lecture17/Lec17.html#anova-tables",
    "title": "PSTAT 5A: Lecture 17",
    "section": "ANOVA Tables",
    "text": "ANOVA Tables\n\nAs mentioned previously, computing \\(\\mathrm{MS}_{\\mathrm{G}}\\) and \\(\\mathrm{MS}_{\\mathrm{E}}\\) is not particularly challenging, but it can be quite tedious.\nAs such, computer software is usually utilized to carry out an ANOVA.\nOften times, the results of such a computer-generated ANOVA are displayed in what is known as an ANOVA Table.\nI find ANOVA tables to be best described by way of an example."
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#example",
    "href": "Pages/Lectures/Lecture17/Lec17.html#example",
    "title": "PSTAT 5A: Lecture 17",
    "section": "Example",
    "text": "Example\n\n\n\n\n\n\n\nReference Example 1\n\n\n\n\nA state official would like to determine whether or not the average fluoride levels in the water supplies of Cities A, B, and C are the same.\nTo that end, they took a sample of 100 fluoride measurements from city A, 110 from city B, and 100 from city C.\n\n\n\n\n\n\n\nAfter running an ANOVA in a computer software, the following output was produced:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDF\nSum Sq\nMean Sq\nF value\nPr(>F)\n\n\n\n\nBetween Groups\n2\n0.541799\n0.2709\n1.30682497808\n0.272179497817\n\n\nResiduals\n307\n63.6399\n0.207296\n\n\n\n\n\n\n\nLet’s go through this table in more detail."
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#interpreting-an-anova-table",
    "href": "Pages/Lectures/Lecture17/Lec17.html#interpreting-an-anova-table",
    "title": "PSTAT 5A: Lecture 17",
    "section": "Interpreting an ANOVA Table",
    "text": "Interpreting an ANOVA Table\n\n\n\n\n\n\n\n\n\n\n\n\n\nDF\nSum Sq\nMean Sq\nF value\nPr(>F)\n\n\n\n\nBetween Groups\n2\n0.343981\n0.171991\n0.927001041587\n0.396843557892\n\n\nResiduals\n307\n56.9591\n0.185534\n\n\n\n\n\n\n\n\n\nThe DF column gives the degrees of freedom of the resulting F-statistic.\n\nRecall that these are meant to be \\(k - 1\\) and \\(n - k\\) respectively.\n\\(k\\) is the number of groups (i.e. 3, in this example), hence the numerator d.f. of 2.\n\\(n\\) is the total number of observations (i.e. 100 + 110 + 100 = 310, in this example), hence the denominator d.f. of 307 (310 - 3 = 307).\n\nThe rownames (“Between Groups” and “Residuals”) refer to whether the specified entry is in relation to a between group calculation or a within group calculation.\n\nThe reason for calling the second row “Residuals” instead of “Within Group” will become clear next week, after we talk about Regression."
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#interpreting-an-anova-table-1",
    "href": "Pages/Lectures/Lecture17/Lec17.html#interpreting-an-anova-table-1",
    "title": "PSTAT 5A: Lecture 17",
    "section": "Interpreting an ANOVA Table",
    "text": "Interpreting an ANOVA Table\n\n\n\n\n\n\n\n\n\n\n\n\nDF\nSum Sq\nMean Sq\nF value\nPr(>F)\n\n\n\n\nBetween Groups\n2\n0.343981\n0.171991\n0.927001041587\n0.396843557892\n\n\nResiduals\n307\n56.9591\n0.185534\n\n\n\n\n\n\n\n\nThe Sum Sq column is a scaled version of the \\(\\mathrm{MS}_{\\mathrm{G}}\\) and \\(\\mathrm{MS}_{\\mathrm{E}}\\) quantities.\nDon’t worry too much about how those were computed for now.\nThe Mean Sq entries are found by dividing the corresponding Sum Sq entry by the corresponding degree of freedom.\n\nThat is: 0.171991 = 0.343981 / 2\nAnd: 0.185534 = 56.9591 / 307\n\nFinally, the F value is simply the ration of the two Mean Sq values, and represents the value of our test statistic.\n\nThe Pr(>F) is just our p-value."
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#analyzing-the-data",
    "href": "Pages/Lectures/Lecture17/Lec17.html#analyzing-the-data",
    "title": "PSTAT 5A: Lecture 17",
    "section": "Analyzing the Data",
    "text": "Analyzing the Data\n\nMaybe that was a little too opaque.\nIf you’d like, there is the actual data:\n\n\n\n\n                A          B               C\n1     1.765793252  1.0969903    0.8893372313\n2     1.457639487  0.2870172     1.673980678\n3    0.3272998539  0.6808363     1.251111092\n4     1.095808075  1.2379006    0.9022573626\n5     1.410667987  0.9922533    0.7675894536\n6    0.7399572196  0.8899447     1.117615437\n7     1.232755793  0.9316065    0.3737125741\n8     1.154274263  1.3892834      1.67516293\n9     1.102145715  1.3327322     1.296435132\n10    1.012699895  0.7350924     1.607457771\n11    1.316513095  1.1671763     0.816422448\n12    1.701518687  0.8616771      1.45467294\n13   0.6174671647  2.0117302     1.379557127\n14    1.614137039  0.7542573    0.3801190768\n15    1.214315065  0.9248685     1.137032315\n16    1.322568806  0.9104472     1.211800026\n17   0.5316988941  0.8743895    0.8112870512\n18    1.167568439  1.1064417    0.9079229895\n19    1.842268501  1.1592197     1.266283383\n20   0.5600975571  0.4025717     1.052936953\n21   0.1111358597  0.8207547    0.6178776414\n22   0.2283138521  0.5218210     1.336153889\n23    1.233035082  1.5444821     1.172517814\n24    2.292483665  2.0361300     1.182130396\n25    1.661845627  1.6032929     1.113093457\n26    1.936311107  1.1884932     1.272059779\n27    1.149574608  1.4590064    0.9412990966\n28    1.798998189  1.1167241    0.6817587653\n29    0.964376006  1.7126866    0.7361788226\n30    1.406602092  1.3171558     1.567508811\n31   0.9663414057  0.9580213    0.1620266063\n32   0.8253454929  0.5981795     1.313243321\n33    1.166354148  0.7940150    0.4209655028\n34   0.8619289925  1.1755322    0.3460173367\n35     1.75423654  0.5596656    0.6170441472\n36     1.19750664  0.9612827    0.9541098688\n37    1.300104994  1.5859705     1.398728351\n38   0.9311838315  1.0617239     0.305719329\n39    1.728236132  1.8441543     1.823333996\n40    0.734015249  1.6288662     1.593675665\n41    1.430115776  0.9349482     1.010341883\n42   0.9245640543  0.5062758    0.9393893861\n43   0.6302833201  1.0307835    0.6409104121\n44   0.8553313916  0.8576851     1.334618267\n45   0.6977044289  1.2011012     1.100960662\n46   0.9936511806  1.6957632     1.705428937\n47    0.930429877  0.6007584    0.7612029733\n48    1.256084968  0.8945088     1.316574767\n49    1.382576335  1.1829400     1.053627858\n50    1.026289871  1.5039893    0.6888426861\n51    1.087047332  1.2542858     1.926036819\n52    1.244547102  1.2157191     1.195822426\n53   0.8300604643  1.5878198     1.349114526\n54     1.45408001  1.2474061    0.5891445312\n55    1.521112369  0.9551983    0.7448206699\n56    1.201790399  1.0729141       1.1078628\n57    2.297351832  0.7671714    0.8069951795\n58    1.558729469  1.2686659     1.340910022\n59    1.043863764  1.4270002     1.267264358\n60   0.9189097764 -0.1418387    0.7179455626\n61    0.983908872  1.5372595     1.156821193\n62   0.8491355502  1.0273917      1.07181121\n63    1.664392577  1.2343554    0.9961661192\n64   0.7510949848  1.3149496     0.553034481\n65    1.059438908  0.1319587    0.6940211732\n66   0.8353519595  0.5716283     1.164702828\n67    1.623091428  0.9167595     1.526293084\n68   0.3907219854  0.7334437    0.6334109797\n69   0.9187504084  1.1596691     1.617731702\n70    1.039047154  0.8440360      1.15219132\n71    1.259678211  0.3655715    0.4985811837\n72    1.330451451  1.2949189     1.215525468\n73   0.9921050538  1.4900517     1.468598686\n74    1.594536229  1.2757410     1.188977611\n75    1.257376889  0.7648802     2.113607293\n76    2.333825528  0.9456562     1.208900453\n77   0.3458392564  1.2693712     1.471653814\n78    1.410300332  1.2912796     1.074960214\n79   0.5774337312  1.4527445     1.511033358\n80   0.7009955912  0.9322296 -0.008007406026\n81    2.092542296  0.4816478    0.6841148682\n82    1.972407074  0.7784213    0.9060748612\n83   0.1719072597  1.2635539     1.296767106\n84   0.9886131504  1.3475898     1.154749047\n85    1.067076076  1.9400199     1.558352183\n86  0.03414394576  1.0968726      1.23062258\n87    1.075584744  1.1478458     1.163298305\n88    1.296670609  0.9790805     1.004720958\n89    1.208632573  0.8877082     1.143756914\n90   0.1028031151  1.3627101     1.443280618\n91    1.653854117  1.1913910     1.409884981\n92    1.222271988  0.9670882     1.465768561\n93    1.069043985  0.6181210    0.9620196655\n94   0.7230535196  1.0007721    0.7572245096\n95    1.455979508  0.8936071    0.8578699627\n96    1.559134576  1.5520695     1.227264384\n97   0.8589534306  1.4288504     1.795155782\n98    1.144793806  0.6410757      1.17770267\n99    1.513499311  1.5168673     1.288251183\n100    0.12274394  0.8212093     0.323627882\n101            NA  1.1032550              NA\n102            NA  1.5310975              NA\n103            NA  0.6400988              NA\n104            NA  1.1141597              NA\n105            NA  0.5978099              NA\n106            NA  0.2734801              NA\n107            NA  1.4446391              NA\n108            NA  1.0612909              NA\n109            NA  0.7210530              NA\n110            NA  0.4153793              NA"
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#analyzing-the-data-1",
    "href": "Pages/Lectures/Lecture17/Lec17.html#analyzing-the-data-1",
    "title": "PSTAT 5A: Lecture 17",
    "section": "Analyzing the Data",
    "text": "Analyzing the Data\n\nWhoops- maybe that’s too detailed.\nAny ideas on how we might be able to get a better sense of the data that doesn’t involve looking at all those numbers?\nMaybe… something we learned in Week 1?"
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#checking-assumptions",
    "href": "Pages/Lectures/Lecture17/Lec17.html#checking-assumptions",
    "title": "PSTAT 5A: Lecture 17",
    "section": "Checking Assumptions",
    "text": "Checking Assumptions\n\nFinally, I should mention: every good statistician and data scientists starts by checking assumptions.\nOne of the key assumptions in ANOVA is that observations within each group are normally distributed.\nHow can we check that?\n\nThat’s right: QQ-plots!\n\nIn lab, you’ll begin to start talking about how to start statistical analyses.\n\nSpecifically, you will learn about something called Exploratory Data Analysis (EDA), part of which entails producing any diagnostic tools you may need to produce in order to ensure assumptions are being satisfied!"
  },
  {
    "objectID": "Pages/Lectures/Lecture17/Lec17.html#qq-plots-for-the-fluoride-dataset",
    "href": "Pages/Lectures/Lecture17/Lec17.html#qq-plots-for-the-fluoride-dataset",
    "title": "PSTAT 5A: Lecture 17",
    "section": "QQ-Plots for the Fluoride Dataset",
    "text": "QQ-Plots for the Fluoride Dataset"
  },
  {
    "objectID": "Pages/Lectures/Lecture19/Lec19.html#review-of-lecture-18",
    "href": "Pages/Lectures/Lecture19/Lec19.html#review-of-lecture-18",
    "title": "PSTAT 5A: Lecture 19",
    "section": "Review of Lecture 18",
    "text": "Review of Lecture 18\n\nLast time, we started to talk about how to actually model a relationship between two variables.\nSpecifically, given a respone variable y and an explanatory variable x, we typically assume x and y are related through the equation \\[ \\texttt{y} = f(\\texttt{x}) + \\texttt{noise} \\] where \\(f\\) is some function.\nOf particular interest to us in this class is when \\(f\\) takes the form of a linear equation: i.e. when our model is of the form \\[ \\texttt{y} = \\beta_0 + \\beta_1 \\cdot \\texttt{x} + \\texttt{noise} \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture19/Lec19.html#review-of-lecture-18-1",
    "href": "Pages/Lectures/Lecture19/Lec19.html#review-of-lecture-18-1",
    "title": "PSTAT 5A: Lecture 19",
    "section": "Review of Lecture 18",
    "text": "Review of Lecture 18\n\nNow, the noise part of our model makes it impossible to know the true values of \\(\\beta_0\\) and \\(\\beta_1\\).\n\nIn this way, we can think of them as population parameters.\n\nAs such, we sought to find point estimators \\(\\widehat{\\beta_0}\\) and \\(\\widehat{\\beta_1}\\) that best estimate \\(\\beta_0\\) and \\(\\beta_1\\), respectively.\nTo quantify what we mean by “best”, we employed the condition of minimizing the residual sum of squares.\n\nEffectively, this means finding the line \\(\\widehat{\\beta_0} + \\widehat{\\beta_1} \\cdot \\texttt{x}\\) that minimizes the average distance between the points in the dataset and the line."
  },
  {
    "objectID": "Pages/Lectures/Lecture19/Lec19.html#review-of-lecture-18-2",
    "href": "Pages/Lectures/Lecture19/Lec19.html#review-of-lecture-18-2",
    "title": "PSTAT 5A: Lecture 19",
    "section": "Review of Lecture 18",
    "text": "Review of Lecture 18"
  },
  {
    "objectID": "Pages/Lectures/Lecture19/Lec19.html#review-of-lecture-18-3",
    "href": "Pages/Lectures/Lecture19/Lec19.html#review-of-lecture-18-3",
    "title": "PSTAT 5A: Lecture 19",
    "section": "Review of Lecture 18",
    "text": "Review of Lecture 18\n\nSuch estimators (i.e. those that minimize the RSS) are said to be ordinary least squares (OLS) estimates.\n\nThe resulting line \\(\\widehat{\\beta_0} + \\widehat{\\beta_1} \\cdot \\texttt{x}\\) is thus called the OLS Regression Line\n\nIt turns out that the OLS estimates of \\(\\beta_0\\) and \\(\\beta_1\\) are: \\[\\begin{align*}\n\\widehat{\\beta_1}   & = \\frac{\\sum_{i=1}^{n} (x_i - \\overline{x})(y_i - \\overline{y})}{\\sum_{i=1}^{n} (x - \\overline{x})^2} = \\frac{s_Y}{s_X} \\cdot r \\\\\n\\widehat{\\beta_0}   & = \\overline{y} - \\widehat{\\beta_1} \\cdot \\overline{x}\n\\end{align*}\\] where r denotes Pearson’s Correlation Coefficient \\[ r = \\frac{1}{n - 1} \\sum_{i=1}^{n} \\left( \\frac{x_i - \\overline{x}}{s_X} \\right) \\left( \\frac{y_i - \\overline{y}}{s_Y} \\right)  \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture19/Lec19.html#example",
    "href": "Pages/Lectures/Lecture19/Lec19.html#example",
    "title": "PSTAT 5A: Lecture 19",
    "section": "Example",
    "text": "Example\n\n\n\\(\\widehat{\\beta_0} =\\) -2.5884231; \\(\\widehat{\\beta_1} =\\) -1.733778"
  },
  {
    "objectID": "Pages/Lectures/Lecture19/Lec19.html#example-1",
    "href": "Pages/Lectures/Lecture19/Lec19.html#example-1",
    "title": "PSTAT 5A: Lecture 19",
    "section": "Example",
    "text": "Example\n\n\n\\(\\widehat{\\beta_0} =\\) -2.5884231; \\(\\widehat{\\beta_1} =\\) -0.483778"
  },
  {
    "objectID": "Pages/Lectures/Lecture19/Lec19.html#example-2",
    "href": "Pages/Lectures/Lecture19/Lec19.html#example-2",
    "title": "PSTAT 5A: Lecture 19",
    "section": "Example",
    "text": "Example\n\n\n\\(\\widehat{\\beta_0} =\\) -2.5884231; \\(\\widehat{\\beta_1} =\\) 0.266222\nDo we really believe the slope, though?"
  },
  {
    "objectID": "Pages/Lectures/Lecture19/Lec19.html#leadup",
    "href": "Pages/Lectures/Lecture19/Lec19.html#leadup",
    "title": "PSTAT 5A: Lecture 19",
    "section": "Leadup",
    "text": "Leadup\n\nWell, remember- \\(\\widehat{\\beta_1}\\) is just an estimator of \\(\\beta_1\\), the true slope.\nAs such, we should be a little wary of using a single observed instance of \\(\\widehat{\\beta_1}\\) as an estimate for \\(\\beta_1\\).\nWhat do you think would be a better quantity to report?\nThat’s right- a confidence interval!"
  },
  {
    "objectID": "Pages/Lectures/Lecture19/Lec19.html#confidence-intervals-for-the-slope",
    "href": "Pages/Lectures/Lecture19/Lec19.html#confidence-intervals-for-the-slope",
    "title": "PSTAT 5A: Lecture 19",
    "section": "Confidence Intervals for the Slope",
    "text": "Confidence Intervals for the Slope\n\nIt is often desired to report a confidence interval for \\(\\beta_1\\).\nRecall: in general, a confidence interval for a parameter \\(\\theta\\) takes the form \\[ \\widehat{\\theta} \\pm c \\cdot \\mathrm{SD}(\\widehat{\\theta}) \\] where:\n\n\\(\\widehat{\\theta}\\) is a point estimator for \\(\\theta\\)\n\\(c\\) is a constant that depends both on the confidence level as well as the sampling distribution of \\(\\widehat{\\theta}\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture19/Lec19.html#confidence-intervals-for-the-slope-1",
    "href": "Pages/Lectures/Lecture19/Lec19.html#confidence-intervals-for-the-slope-1",
    "title": "PSTAT 5A: Lecture 19",
    "section": "Confidence Intervals for the Slope",
    "text": "Confidence Intervals for the Slope\n\nWe’re going to do the same thing for \\(\\beta_1\\): \\[ \\widehat{\\beta_1} \\pm c \\cdot \\mathrm{SD}\\left( \\widehat{\\beta_1} \\right) \\]\nYou will not be responsible for computing the standard deviation of \\(\\widehat{\\beta_1}\\). On a problem, you will be provided with this information.\nWe should, however, talk about what distribution \\[ \\frac{\\widehat{\\beta_1} - \\beta_1}{\\mathrm{SD}(\\widehat{\\beta_1})} \\] follows, as this will be the distribution whose quantiles we use when constructing our confidence intervals."
  },
  {
    "objectID": "Pages/Lectures/Lecture19/Lec19.html#confidence-intervals-for-the-slope-2",
    "href": "Pages/Lectures/Lecture19/Lec19.html#confidence-intervals-for-the-slope-2",
    "title": "PSTAT 5A: Lecture 19",
    "section": "Confidence Intervals for the Slope",
    "text": "Confidence Intervals for the Slope\n\nIt turns out that, if we assume the noise in our model is normally distributed, then \\[ \\left( \\frac{\\widehat{\\beta_1} - \\beta_1}{\\mathrm{SD}(\\widehat{\\beta_1})} \\right) \\sim t_{n - 2} \\]\n\nDon’t worry too much about why we use \\(n - 2\\) degrees of freedom instead of \\(n - 1\\). The reasoning depends a bit on the way we compute \\(\\mathrm{SD}(\\widehat{\\beta_1})\\), which, again, we will not discuss in this class.\n\nThis allows us to construct confidence intervals for \\(\\beta\\) using the \\(t_{n - 2}\\) distribution."
  },
  {
    "objectID": "Pages/Lectures/Lecture19/Lec19.html#worked-out-example",
    "href": "Pages/Lectures/Lecture19/Lec19.html#worked-out-example",
    "title": "PSTAT 5A: Lecture 19",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\nWorked-Out Example 1\n\n\n\n\nTwo sets \\(\\boldsymbol{x} = \\{x_i\\}_{i=1}^{100}\\) and \\(\\boldsymbol{y} = \\{y_i\\}_{i=1}^{100}\\) have yielded: \\[\\begin{align*}\n  \\sum_{i=1}^{100} x_i = 1009.491    & \\qquad \\sum_{i=1}^{100} y_i = -2009.075  \\\\\n  \\sum_{i=1}^{100} (x_i - \\overline{x})^2 = 796.16 & \\qquad  \\sum_{i=1}^{100} (y_i - \\overline{y})^2 = 6237.68   \\\\\n  \\sum_{i=1}^{100} (x_i - \\overline{x})(y_i - \\overline{y}) = -1380.372\n\\end{align*}\\]\n\nFind the equation of the OLS regression line.\nIt turns out that \\(\\mathrm{Var}(\\widehat{\\beta_1}) = 0.0452\\). Construct a 95% confidence interval for \\(\\beta_1\\), the slope of the true linear relationship between x and y."
  },
  {
    "objectID": "Pages/Lectures/Lecture19/Lec19.html#solutions",
    "href": "Pages/Lectures/Lecture19/Lec19.html#solutions",
    "title": "PSTAT 5A: Lecture 19",
    "section": "Solutions",
    "text": "Solutions\n\nFor part (a), we simply plug into our formulas: \\[\\begin{align*}\n\\widehat{\\beta_1}   & = \\frac{\\sum_{i=1}^{n} (x_i - \\overline{x})(y_i - \\overline{y})}{\\sum_{i=1}^{n} (x_i - \\overline{x})^2} = \\frac{-1380.372}{796.16}  \\approx \\boxed{-1.73}    \\\\\n\\widehat{\\beta_0}   & = \\overline{y} - \\widehat{\\beta_1} \\cdot \\overline{x} = \\left( \\frac{-2009.075}{100} \\right) - (-1.734) \\cdot \\left( \\frac{1009.491}{100} \\right) \\approx \\boxed{-2.627}\n\\end{align*}\\]\nHence, the equation of the OLS regression line is \\[ \\boxed{ \\widehat{y} = -2.627 - 1.734 \\cdot \\texttt{x} } \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture19/Lec19.html#solutions-1",
    "href": "Pages/Lectures/Lecture19/Lec19.html#solutions-1",
    "title": "PSTAT 5A: Lecture 19",
    "section": "Solutions",
    "text": "Solutions\n\nFor part (b), we should first find the appropriate quantile of the appropriate t-distribution.\nHere, \\(n = 100\\) meaning we will use the \\(t_{98}\\) distribution.\nAdditionally, we require a 95% confidence level, meaning the quantile we seek is the \\(97.5\\)th percentile or, equivalently, negative one times the \\(2.5\\)th percentile.\n\n\n\nimport scipy.stats as sps\nsps.t.ppf(0.975, 98)\n\n1.984467454426692\n\n\n\nimport scipy.stats as sps\n-sps.t.ppf(0.025, 98)\n\n1.9844674544266925"
  },
  {
    "objectID": "Pages/Lectures/Lecture19/Lec19.html#solutions-2",
    "href": "Pages/Lectures/Lecture19/Lec19.html#solutions-2",
    "title": "PSTAT 5A: Lecture 19",
    "section": "Solutions",
    "text": "Solutions\n\nTherefore, our confidence interval for \\(\\beta_1\\) takes the form \\[ (-1.734) \\pm 1.98 \\cdot (\\sqrt{0.0452})  = \\boxed{[1.313 \\ ,  2.155]}\\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture19/Lec19.html#a-ci-that-contains-zero",
    "href": "Pages/Lectures/Lecture19/Lec19.html#a-ci-that-contains-zero",
    "title": "PSTAT 5A: Lecture 19",
    "section": "A CI that Contains Zero",
    "text": "A CI that Contains Zero\n\nWhat happens if a confidence interval for \\(\\beta_1\\) includes zero?\nWell, let’s think about what a confidence interval is actually saying.\nA 95% CI \\([a \\ , b]\\) for some parameter \\(\\theta\\) is saying “we are 95% confident that the true value of \\(\\theta\\) lies in the interval \\([a \\ , b]\\).”\nAs such, if a 95% confidence interval for \\(\\beta_1\\), we are effectively saying that the value of \\(\\beta_1\\) could be zero.\nSince \\(\\beta_1\\) represents the slope of the relationship between x and y, this means we are saying that there could potentially be no relationship between x and y at all!"
  },
  {
    "objectID": "Pages/Lectures/Lecture19/Lec19.html#hypothesis-testing-for-the-slope",
    "href": "Pages/Lectures/Lecture19/Lec19.html#hypothesis-testing-for-the-slope",
    "title": "PSTAT 5A: Lecture 19",
    "section": "Hypothesis Testing for the Slope",
    "text": "Hypothesis Testing for the Slope\n\nIndeed, we could make this a bit more rigorous by performing a hypothesis test for \\(\\beta_1\\).\n\nQuick side note: there is in fact a duality between confidence intervals and hypothesis testing. For the sake of time and brevity, however, we will likely not get a chance to discuss this connection this quarter.\n\nSpecifically, we may wish to test \\[ \\left[ \\begin{array}{rr}\nH_0:    & \\beta_1 = 0   \\\\\nH_A:    & \\beta_1 \\neq 0\n\\end{array} \\right.\\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture19/Lec19.html#hypothesis-testing-for-the-slope-1",
    "href": "Pages/Lectures/Lecture19/Lec19.html#hypothesis-testing-for-the-slope-1",
    "title": "PSTAT 5A: Lecture 19",
    "section": "Hypothesis Testing for the Slope",
    "text": "Hypothesis Testing for the Slope\n\nA natural test statistic is \\[ \\frac{\\widehat{\\beta_1}}{\\mathrm{SD}(\\widehat{\\beta})} \\stackrel{H_0}{\\sim} t_{n - 2} \\]\nSince we are considering a two-sided alternative (for now), our test would reject for large values of \\(|\\mathrm{TS}|\\), where the critical value comes from the \\(t_{n - 2}\\) distribution.\n\nOr, we could simply consider p-values!"
  },
  {
    "objectID": "Pages/Lectures/Lecture19/Lec19.html#hypothesis-testing-for-the-slope-2",
    "href": "Pages/Lectures/Lecture19/Lec19.html#hypothesis-testing-for-the-slope-2",
    "title": "PSTAT 5A: Lecture 19",
    "section": "Hypothesis Testing for the Slope",
    "text": "Hypothesis Testing for the Slope\n\nIndeed, many computer softwares (and statistical papers!) report a table resembling the following after running a regression:\n\n\n\n\n\n\nEstimate\nStd. Error\nt-value\nPr(>|t|)\n\n\n\n\nIntercept\n-2.588\n2.327\n-1.112\n0.269\n\n\nSlope\n-1.734\n0.222\n-7.811\n6.41e-12\n\n\n\n\n\nThe first column is the raw estimated value (i.e. \\(\\widehat{\\beta_0}\\) and \\(\\widehat{\\beta_1}\\), respectively)\nThe second column is the standard error (i.e. standard deviation) of the estimator\nThe third column is the test statistic (i.e. the first column divided by the second)\nThe fourth column is the p-value in a two-sided test, testing whether or not the given parameter is actually zero or not."
  },
  {
    "objectID": "Pages/Lectures/Lecture19/Lec19.html#extensions",
    "href": "Pages/Lectures/Lecture19/Lec19.html#extensions",
    "title": "PSTAT 5A: Lecture 19",
    "section": "Extensions",
    "text": "Extensions\n\nRegression truly is one of the workhorses of statistics (the other being Hypothesis Testing).\nIndeed, there is so much one can do with regression!\nFor one, we could consider how a response variable y is related to multiple covariates x1, x2, …, xk.\nWe could also explore nonlinear relationships between y and a single covariate x (or even multiple covariates!)"
  },
  {
    "objectID": "Pages/Lectures/Lecture19/Lec19.html#extensions-1",
    "href": "Pages/Lectures/Lecture19/Lec19.html#extensions-1",
    "title": "PSTAT 5A: Lecture 19",
    "section": "Extensions",
    "text": "Extensions\n\nHere is an interesting project: suppose we have access to a roster of passengers on the Titanic.\n\nFor those who don’t know, the RMS Titanic was a British passenger line that crahsed into an iceberg on April 15, 1912.\n\nTragically, not all passengers survived.\nOne question we may want to ask is: how did various factors (e.g. class, gender, etc.) affect whether or not a given passenger survived?\n\nFor instance, since women and children boarded lifeboats first, it may be plausible surmise that women and children had a higher likelihood of surviving."
  },
  {
    "objectID": "Pages/Lectures/Lecture19/Lec19.html#extensions-2",
    "href": "Pages/Lectures/Lecture19/Lec19.html#extensions-2",
    "title": "PSTAT 5A: Lecture 19",
    "section": "Extensions",
    "text": "Extensions\n\nNotice how here, the response variable (i.e. whether or not someone survived) is a binary variable; that is, it takes only one of two values (survived or did not survive).\nRegression involving a binary response (like with this Titanic project) is called logistic regression and is highly applicable to lots of other projects as well.\n\nBy the way, there is an actual dataset on the Titanic at https://www.kaggle.com/competitions/titanic\n\nThese are all topics that fall under the category of Machine Learning.\n\nI highly encourage everyone to take a look at some Machine Learning resources, either online or by way of PSTAT 131!"
  },
  {
    "objectID": "Pages/Lectures/Lecture19/Lec19.html#where-does-data-come-from-1",
    "href": "Pages/Lectures/Lecture19/Lec19.html#where-does-data-come-from-1",
    "title": "PSTAT 5A: Lecture 19",
    "section": "Where Does Data Come From?",
    "text": "Where Does Data Come From?\n\nThroughout this course, we have been using various pieces of data.\nOne thing we should discuss, as good Data Scientists, is where this data actually came from?\n\nWho collected it? How was the data collected? Who were the subjects included in the data?\n\nWe will now begin to discuss some possible answers to these questions, as well as some practical strategies for collecting data of our own!"
  },
  {
    "objectID": "Pages/Lectures/Lecture19/Lec19.html#the-research-process",
    "href": "Pages/Lectures/Lecture19/Lec19.html#the-research-process",
    "title": "PSTAT 5A: Lecture 19",
    "section": "The Research Process",
    "text": "The Research Process\n\nIndeed, most experiments and studies begin with some sort of question.\n\nFor example: “does this new drug truly reduce blood pressure?”\nOr: “is smoking really linked with higher rates of lung cancer?”\nOr: “what is the average mercury content in swordfish in the Atlantic Ocean?”\nOr: “over the past 3 years, what is the average number of people that have been admitted into the PSTAT major?”\n\nThe next step is something we’ve actually done several times in this class already: identify the population of interest!\n\nCan anyone tell me what the populations associated with the above research questions are?"
  },
  {
    "objectID": "Pages/Lectures/Lecture19/Lec19.html#sampling-procedures",
    "href": "Pages/Lectures/Lecture19/Lec19.html#sampling-procedures",
    "title": "PSTAT 5A: Lecture 19",
    "section": "Sampling Procedures",
    "text": "Sampling Procedures\n\nNow, we need the most crucial piece of all: data!\n\nSpecifically, we need to collect our data.\n\nThis will entail taking a sample (or possibly many samples) from our population.\nThere are many ways to take a sample!\nOne way is to take what is known as a simple random sample (or SRS, for short).\nA simple random sample is akin to assigning a unique number to each person in the population, and then picking some subset of these numbers uniformly at random.\n\nCrucially, in this way, each member of the population has an equal chance of being included in the sample."
  },
  {
    "objectID": "Pages/Lectures/Lecture19/Lec19.html#sampling-procedures-1",
    "href": "Pages/Lectures/Lecture19/Lec19.html#sampling-procedures-1",
    "title": "PSTAT 5A: Lecture 19",
    "section": "Sampling Procedures",
    "text": "Sampling Procedures\n\nNow, Simple Random Samples do have some potential downsides.\nAs an example, let’s consider the following situation: suppose our population is the workplace at some company (which we will call Company X).\nSay we are interested in determining whether or not there is systemic racism present in Company X.\nTo determine whether or not this is the case, we might take a sample and administer a survey (we’ll talk more about surveys in a bit).\nCan anyone tell me a potential problem with this setup if we were to take a Simple Random Sample?\n\nWhat if I tell you, for example, that 80% of people in this company are Caucasian and only 20% are not?"
  },
  {
    "objectID": "Pages/Lectures/Lecture19/Lec19.html#sampling-procedures-2",
    "href": "Pages/Lectures/Lecture19/Lec19.html#sampling-procedures-2",
    "title": "PSTAT 5A: Lecture 19",
    "section": "Sampling Procedures",
    "text": "Sampling Procedures\n\nThat’s right- we run the risk of obtaining a biased sample.\nSaid differently: if we take an SRS of employees at this company, there is a high probability that this sample will contain a disproportionately larger number of Caucasians than non-Caucasians.\n\nThis would almost certainly affect the results of our survey!\n\nOne way to remedy this would be to perform what is known as in which we first divide the population into several strata (i.e. groups), and then take an SRS from each stratum.\n\nThis has the benefit of ensuring a roughly equal number of participants from each stratum, but has the downsides of being very dependent on the strata that are created.\nThat is, sometimes it won’t necessarily be obvious how to divide the population!"
  },
  {
    "objectID": "Pages/Lectures/Lecture19/Lec19.html#sampling-procedures-3",
    "href": "Pages/Lectures/Lecture19/Lec19.html#sampling-procedures-3",
    "title": "PSTAT 5A: Lecture 19",
    "section": "Sampling Procedures",
    "text": "Sampling Procedures\n\nAnother type of sampling is called cluster sampling. Similar to stratified sampling, the population is first divided into several groups (now called clusters). But, instead of taking an SRS from every cluster we instead take an SRS of clusters and then take an SRS from each included cluster\n\nNote, then, that we are not including every cluster in our sample in this way of sampling.\nThis has the benefit of being (potentially) cheaper, but has the (obvious) downside of potentially skewing results due to the lack of certain clusters.\n\nBy the way, this is slightly different than the notion of Cluster Sampling outlined in the textbook; the textbook calls the above scheme “multistage sampling”. However, this term is not widely used, and as such we will simply refer to the above as “Cluster Sampling”, as most statisticians do."
  },
  {
    "objectID": "Pages/Lectures/Lecture19/Lec19.html#example-3",
    "href": "Pages/Lectures/Lecture19/Lec19.html#example-3",
    "title": "PSTAT 5A: Lecture 19",
    "section": "Example",
    "text": "Example\n\nLet’s consider an example from the textbook:\n\n\n\n\n\n\n\n\nExample (credit to OpenIntro Statistics)\n\n\n\n\nSuppose we are interested in estimating the malaria rate in a densely tropical portion of rural Indonesia. We learn that there are 30 villages in that part of the Indonesian jungle, each more or less similar to the next. Our goal is to test 150 individuals for malaria. What sampling method should be employed?\n\n\n\n\n\n\n\nAt surface level, an SRS may seem tempting.\n\nAfter all, the villages are “more or less similar to the next”.\n\nHowever, an SRS will likely contain individuals from all (or certainly most) of the villages.\nThis would require someone to actually visit these villages to collect data, which will be incredibly costly.\nAs such, an SRS is probably not a good idea."
  },
  {
    "objectID": "Pages/Lectures/Lecture19/Lec19.html#example-4",
    "href": "Pages/Lectures/Lecture19/Lec19.html#example-4",
    "title": "PSTAT 5A: Lecture 19",
    "section": "Example",
    "text": "Example\n\nIndeed, cluster sampling seems to be the way to go.\nSpecifically, we could assign each village to its own cluster, then select some number of villages (say, maybe 15 or so) randomly, and then select some number of people (say, 10 or so) from each of the selected villages.\nBecause the villages are “more or less similar”, the people included in our sample would likely not have many obvious differences (in the context of the experiment) and we would believe our sample to be relatively representative.\nAdditionally, the experimenter (or a volunteer) would only need to visit 15 villages instead of all 30.\nOf course, we would need to modify our statistical tools slightly to reflect the fact that our sample was actually collected via a clustering scheme- such modifications (though not too challenging) are outside the scope of this course.\n\nRight now, I’m merely trying to get us to think about the practicalities of data collection!"
  },
  {
    "objectID": "Pages/Lectures/Lecture19/Lec19.html#bias",
    "href": "Pages/Lectures/Lecture19/Lec19.html#bias",
    "title": "PSTAT 5A: Lecture 19",
    "section": "Bias",
    "text": "Bias\n\nEven if we end up with a fairly representative sample, there are ways bias can creep in.\nSuppose we administer a survey to a representative sample of people.\nIt is not guaranteed that everyone in our sample will actually fill out the survey; even if a particular individual does attempt the survey, there is no guarantee they will finish the entirety of the survey.\nThis can lead to gaps in our data with respect to certain demographics, which is itself a form of bias.\nWe call this non-response bias."
  },
  {
    "objectID": "Pages/Lectures/Lecture19/Lec19.html#bias-1",
    "href": "Pages/Lectures/Lecture19/Lec19.html#bias-1",
    "title": "PSTAT 5A: Lecture 19",
    "section": "Bias",
    "text": "Bias\n\nAnother sampling strategy that is prone to bias is known as convenience sampling.\nWe can think of convenience sampling as any sort of sampling scheme where individuals who are easily accessed have a higher chance of being included in the sample.\nFor example, suppose we poll students about their views on the housing crisis in Santa Barbara.\n\nIf we conduct this poll at UCSB, we are more likely to get UCSB students and much less likely to get, say, UCLA students.\nSimilarly, if we conducted our study at UCLA, we would be more likely to include UCLA students than, say, NYU students."
  },
  {
    "objectID": "Pages/Lectures/Lecture19/Lec19.html#bias-2",
    "href": "Pages/Lectures/Lecture19/Lec19.html#bias-2",
    "title": "PSTAT 5A: Lecture 19",
    "section": "Bias",
    "text": "Bias\n\nSo, why would we ever use a convenience sample?\nWell, as the name suggests- it is convenient!\nConvenience samples are often both the easiest to obtain, as well as the cheapest.\nHaving said that, most statisticans agree thate convenience samples are bad. This is due to what is known as the garbage in garbage out philosophy.\nLoosely speaking, the “garbage in garbage out” philosophy states that our results are only as good as our data- even the most sophisticated statistical models will output nonsensical or skewed results if we are feeding them nonsensical or skewed data!"
  },
  {
    "objectID": "Pages/Lectures/Lecture19/Lec19.html#another-distinction",
    "href": "Pages/Lectures/Lecture19/Lec19.html#another-distinction",
    "title": "PSTAT 5A: Lecture 19",
    "section": "Another Distinction",
    "text": "Another Distinction\n\nThere is another distinction we should be aware of: the difference between an observational study and an experiment.\nIn an observational study, no treatment is ever explicitly applied (or withheld).\nThis is in contrast to an experiment, in which researchers assign treatments to cases.\nFor example, suppose a researcher is interested in determining the relationship between cancer rates and tanning beds.\nIn an experiment, the researcher would take some sample of volunteers, split them into groups, and assign one group to tan regularly and another to not use tanning beds at all for the duration of the study.\n\nAt the end of the study, the researcher would collect data and analyze."
  },
  {
    "objectID": "Pages/Lectures/Lecture19/Lec19.html#another-distinction-1",
    "href": "Pages/Lectures/Lecture19/Lec19.html#another-distinction-1",
    "title": "PSTAT 5A: Lecture 19",
    "section": "Another Distinction",
    "text": "Another Distinction\n\nIf instead the researcher were to conduct an observational study, they might take some sample of people who already use tanning beds, analyze cancer rates, and repeat for a sample of people who do not use tanning beds.\n\nNote that in this case, the experimenter has not explicitly assigned a treatment (i.e. using a tanning bed) to either group. In a sense, the groups were formed around the treatment.\n\nNaturally, experiments come with all sorts of ethical considerations.\n\nFor instance, in trying to determine the relationship between cancer rates and smoking, nobody would actually force a group of participants to smoke, just for the purposes of an experiment.\nThis is one “pro” in favor of observational studies- the experimenter is not responsible for forcing a group of people to do (or not do something) they wouldn’t want to."
  },
  {
    "objectID": "Pages/Lectures/Lecture19/Lec19.html#another-distinction-2",
    "href": "Pages/Lectures/Lecture19/Lec19.html#another-distinction-2",
    "title": "PSTAT 5A: Lecture 19",
    "section": "Another Distinction",
    "text": "Another Distinction\n\nOf course, observational studies are not perfect either.\nSpecifically, observational studies cannot be used to identify causal relationships; they can only ever identify associations (or a lack thereof).\nRemember that assocations are not the same things as causation!\n\nSo, just because an observational study indicates a link between smoking and increased lung cancer rates, that is not sufficient justification to say that smoking causes increased lung cancer rates. To make that causal claim, an experiment would need to be conducted."
  },
  {
    "objectID": "Pages/Lectures/Lecture19/Lec19.html#example-5",
    "href": "Pages/Lectures/Lecture19/Lec19.html#example-5",
    "title": "PSTAT 5A: Lecture 19",
    "section": "Example",
    "text": "Example\n\n\n\n\n\n\n\nExample (OpenIntro Statistics, 1.19)\n\n\n\n\nA large college class has 160 students. All 160 students attend the lectures together, but the students are divided into 4 groups, each of 40 students, for lab sections administered by different teaching assistants. The professor wants to conduct a survey about how satisfied the students are with the course, and he believes that the lab section a student is in might affect the student’s overall satisfaction with the course.\n\nWhat type of study is this?\nSuggest a sampling strategy for carrying out this study."
  },
  {
    "objectID": "Pages/Lectures/Lecture19/Lec19.html#solutions-3",
    "href": "Pages/Lectures/Lecture19/Lec19.html#solutions-3",
    "title": "PSTAT 5A: Lecture 19",
    "section": "Solutions",
    "text": "Solutions\n\nBecause treatment has neither been administered nor withheld by the researcher, this is an example of an observational study.\nIn this case, stratified sampling would likely be a good idea, with each lab section assigned to a stratum. This ensures each stratum (lab section) is represented in the sample."
  },
  {
    "objectID": "Pages/Lectures/Lecture19/Lec19.html#longitudinal-vs.-cross-sectional-studies",
    "href": "Pages/Lectures/Lecture19/Lec19.html#longitudinal-vs.-cross-sectional-studies",
    "title": "PSTAT 5A: Lecture 19",
    "section": "Longitudinal vs. Cross-Sectional Studies",
    "text": "Longitudinal vs. Cross-Sectional Studies\n\nSuppose a particular drug claims to significantly reduce blood sugar levels. Consider the following two scenarios:\n\nAn SRS of 100 people is taken from a certain demographic, and divided randomly into two groups: group A and group B. Group A is administered the drug and Group B is not administered the drug. After a week, the researcher collects data on the blood sugar levels of the two groups.\nAn SRS of 50 people is taken from a certain demographic. These 50 people have their blood sugar levels recorded, and are then all administered the drug. A week later, the participants have their blood sugar levels recorded."
  },
  {
    "objectID": "Pages/Lectures/Lecture19/Lec19.html#longitudinal-vs.-cross-sectional-studies-1",
    "href": "Pages/Lectures/Lecture19/Lec19.html#longitudinal-vs.-cross-sectional-studies-1",
    "title": "PSTAT 5A: Lecture 19",
    "section": "Longitudinal vs. Cross-Sectional Studies",
    "text": "Longitudinal vs. Cross-Sectional Studies\n\nNote that both of these situations seemingly end up with the same data: 50 measurements of pre-treatment blood sugar levels, and 50 measurements of post-treatment blood sugar levels.\nHowever, we can see that these two studies are fundamentally different.\nIn the first study, people were divided into two groups (called the treatment and control groups, respectively).\nIn the second, the same 50 individuals were tracked over time.\nThis is an example of the distinction between a cross-sectional study and a longitudinal study."
  },
  {
    "objectID": "Pages/Lectures/Lecture19/Lec19.html#longitudinal-vs.-cross-sectional-studies-2",
    "href": "Pages/Lectures/Lecture19/Lec19.html#longitudinal-vs.-cross-sectional-studies-2",
    "title": "PSTAT 5A: Lecture 19",
    "section": "Longitudinal vs. Cross-Sectional Studies",
    "text": "Longitudinal vs. Cross-Sectional Studies\n\nIn a longitudinal study, the same set of individuals is tracked over time.\nIn a cross-sectional study, individuals are divided into several groups.\nNotice that data in longitudinal studies necessarily possess serial correlation: that is, measurements are correlated!\n\nPre-treatment measurements for, say, John, are very likely correlated with John’s post-treatment measurements since these measurements were still collected from John!"
  },
  {
    "objectID": "Pages/Lectures/Lecture19/Lec19.html#principles-of-experimental-design",
    "href": "Pages/Lectures/Lecture19/Lec19.html#principles-of-experimental-design",
    "title": "PSTAT 5A: Lecture 19",
    "section": "Principles of Experimental Design",
    "text": "Principles of Experimental Design\n\nLet’s quickly return to our distinction between observational studies and experiments.\nSuppose it is decided that we want to conduct an experiment.\nWe then need to think very carefully about how we want to design our experiment.\nThis leads us into our final discussion for this course: Experimental Design."
  },
  {
    "objectID": "Pages/Lectures/Lecture19/Lec19.html#treatment-vs.-control-groups",
    "href": "Pages/Lectures/Lecture19/Lec19.html#treatment-vs.-control-groups",
    "title": "PSTAT 5A: Lecture 19",
    "section": "Treatment vs. Control Groups",
    "text": "Treatment vs. Control Groups\n\nFirst, let me make clear the distinction between treatment and control groups.\nTreatment groups are groups to which one or more treatments is/are administered.\n\nThere can potentially be multiple treatment groups; for example, we could have 4 groups each testing a different medicine\n\nTypically, we always leave at least one group “alone”; i.e. one group to which no treatment is administered. This group is called the control group.\n\nIt is also possible to have multiple control groups!"
  },
  {
    "objectID": "Pages/Lectures/Lecture19/Lec19.html#experimental-design",
    "href": "Pages/Lectures/Lecture19/Lec19.html#experimental-design",
    "title": "PSTAT 5A: Lecture 19",
    "section": "Experimental Design",
    "text": "Experimental Design\n\nExperimental Design loosely refers to the principles of and procedures related to setting up an experiment.\nI won’t go into this in too much detail.\nI do highly encourage you to read Section 1.4 of your textbook, as it provides a very nice summary of some of the main tenants of experimental design.\nRemember - as Data Scientists, it is very important we understand our data as much as possible before performing analyses.\n\nPart of knowing our data is knowing how it was collected, and, indeed, how we might go about collecting our own!"
  },
  {
    "objectID": "Pages/Lectures/Lecture19/Lec19.html#what-now-1",
    "href": "Pages/Lectures/Lecture19/Lec19.html#what-now-1",
    "title": "PSTAT 5A: Lecture 19",
    "section": "What Now?",
    "text": "What Now?\n\nAlright, so that was the last bit of new material I wanted to cover in this class.\nBut…. why did we do all of this?\nWhat was the point of this course?\n\nIf you say “the point was for me to finish my major”, well, then, fair enough!\n\nBut, more fundamentally, this course is designed to try and provide an introduction to Data Science."
  },
  {
    "objectID": "Pages/Lectures/Lecture19/Lec19.html#what-now-2",
    "href": "Pages/Lectures/Lecture19/Lec19.html#what-now-2",
    "title": "PSTAT 5A: Lecture 19",
    "section": "What Now?",
    "text": "What Now?\n\nThe key operating word in this is “introduction-” we only just scratched the surface of the topics we discussed!\nEven within our own PSTAT department, there are lots of different courses that dive deeper into the subjects we discussed.\nPSTAT 120A provides a deeper look at Probability, and some more sophisticated probability tools.\nPSTAT 120B and 120C provide a deeper look at inferential statistics, and how to answer much more interesting and complex problems than those we looked at in this course!\nInterested in Experimental Design? PSTAT 122 is devoted entirely to that!\nWant to learn more about regression (including logistic regression)? Take PSTAT 126 and 131!"
  },
  {
    "objectID": "Pages/Lectures/Lecture19/Lec19.html#story",
    "href": "Pages/Lectures/Lecture19/Lec19.html#story",
    "title": "PSTAT 5A: Lecture 19",
    "section": "Story",
    "text": "Story\n\nNow, I know many of you are graduating this quarter.\n\nCongratulations, by the way!!!\n\nFor those of you who are not, and especially for those of you who aren’t quite sure what you want to do, I’d like to leave you with a story.\nI encountered a student who had come into undergrad not knowing what they wanted to do at all.\nThey had a vague inkling that they might want to do math, but after a quarter switched to Econ, then Physics, and finally wound their way into the equivalent of PSTAT 5A at their undergraduate institution.\nBy the end of the quarter, they were so intrigued to learn more the decided to take the analog of 120A, and then 120B, and then, before they knew it, they had completed a degree in statistics!"
  },
  {
    "objectID": "Pages/Lectures/Lecture19/Lec19.html#story-1",
    "href": "Pages/Lectures/Lecture19/Lec19.html#story-1",
    "title": "PSTAT 5A: Lecture 19",
    "section": "Story",
    "text": "Story\n\nThat student…. was me!\nYears ago, I stumbled into the analog of PSTAT 5A and was so enamored by the field that here I am now, pursuing a PhD in it!\nI truly believe statistics and data science are some of the most useful and applicable fields around.\nWherever there is data, there is the need for a data scientist.\nWhenever there is uncertainty, there is the need for a statistician.\nStatistics and Data Science have far reaching applications in so many fields!"
  },
  {
    "objectID": "Pages/Lectures/Lecture19/Lec19.html#story-2",
    "href": "Pages/Lectures/Lecture19/Lec19.html#story-2",
    "title": "PSTAT 5A: Lecture 19",
    "section": "Story",
    "text": "Story\n\nThere is a famous quote from an extremely influential statistician named John Tukey:\n\n\n\nThe best thing about being a statistician is that you get to play in everyone’s backyard.\n\n\n\nI couldn’t agree more!\n\nThough, I would perhaps update this quote to say “statistician and/or data scientist”\n\nSo, now that you’ve learned the basics…\n\n\n…go out and play!"
  },
  {
    "objectID": "Pages/Lectures/Corr_Reg/corr_reg.html#associations-and-correlations",
    "href": "Pages/Lectures/Corr_Reg/corr_reg.html#associations-and-correlations",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Associations and Correlations",
    "text": "Associations and Correlations\n\nRecall, from Week 1, that a scatterplot is a good way to visualize the relationship between two numerical variables x and y.\nTwo variables can have either a positive or a negative relationship/association, along with a linear or nonlinear one.\n\n“Positive” means a one-unit increase in x translates to an increase in y\n“Negative” means a one-unit increase in x translates to an degrease in y\n“Linear” means the rate of change is fixed (i.e. constant)\n“Nonlinear” means the rate of change depends on x"
  },
  {
    "objectID": "Pages/Lectures/Corr_Reg/corr_reg.html#no-relationship",
    "href": "Pages/Lectures/Corr_Reg/corr_reg.html#no-relationship",
    "title": "PSTAT 5A: Lecture 20",
    "section": "No Relationship",
    "text": "No Relationship\n\nSometimes, two variables will have no relationship at all:"
  },
  {
    "objectID": "Pages/Lectures/Corr_Reg/corr_reg.html#strength-of-a-relationship",
    "href": "Pages/Lectures/Corr_Reg/corr_reg.html#strength-of-a-relationship",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Strength of a Relationship",
    "text": "Strength of a Relationship"
  },
  {
    "objectID": "Pages/Lectures/Corr_Reg/corr_reg.html#pearsons-r",
    "href": "Pages/Lectures/Corr_Reg/corr_reg.html#pearsons-r",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Pearson’s r",
    "text": "Pearson’s r\n\nPearson’s r (or just the correlation coefficient) is a metric used to quantify the strength and direction of a linear relationship between two variables.\nGiven variables x and y (whose elements are denoted using the familiar notation we’ve been using throughout this course), we compute r using \\[ r = \\frac{1}{n - 1} \\sum_{i=1}^{n} \\left( \\frac{x_i - \\overline{x}}{s_X} \\right) \\left( \\frac{y_i - \\overline{y}}{s_Y} \\right)  \\]\nRecall that \\(-1 \\leq r \\leq 1\\) for any two variables x and y.\n\nFurthermore, r will only ever be \\(-1\\) or \\(1\\) exactly when the points in the scatterplot fall perfectly on a line."
  },
  {
    "objectID": "Pages/Lectures/Corr_Reg/corr_reg.html#regression",
    "href": "Pages/Lectures/Corr_Reg/corr_reg.html#regression",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Regression",
    "text": "Regression\n\nWe may also want to model the relationship between x and y.\nSpecifically, given a respone variable y and an explanatory variable x, we typically assume x and y are related through the equation \\[ \\texttt{y} = f(\\texttt{x}) + \\texttt{noise} \\] where \\(f\\) is some function.\n\nBy the way: on a scatterplot, the response variable will always appear on the vertical axis and the explanatory variable will appear on the horizontal axis.\n\nOf particular interest to us in this class is when \\(f\\) takes the form of a linear equation: i.e. when our model is of the form \\[ \\texttt{y} = \\beta_0 + \\beta_1 \\cdot \\texttt{x} + \\texttt{noise} \\]"
  },
  {
    "objectID": "Pages/Lectures/Corr_Reg/corr_reg.html#regression-1",
    "href": "Pages/Lectures/Corr_Reg/corr_reg.html#regression-1",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Regression",
    "text": "Regression\n\nNow, the noise part of our model makes it impossible to know the true values of \\(\\beta_0\\) and \\(\\beta_1\\).\n\nIn this way, we can think of them as population parameters.\n\nAs such, we seek to find point estimators \\(\\widehat{\\beta_0}\\) and \\(\\widehat{\\beta_1}\\) that best estimate \\(\\beta_0\\) and \\(\\beta_1\\), respectively.\nTo quantify what we mean by “best”, we employed the condition of minimizing the residual sum of squares.\n\nEffectively, this means finding the line \\(\\widehat{\\beta_0} + \\widehat{\\beta_1} \\cdot \\texttt{x}\\) that minimizes the average distance between the points in the dataset and the line."
  },
  {
    "objectID": "Pages/Lectures/Corr_Reg/corr_reg.html#regression-2",
    "href": "Pages/Lectures/Corr_Reg/corr_reg.html#regression-2",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Regression",
    "text": "Regression"
  },
  {
    "objectID": "Pages/Lectures/Corr_Reg/corr_reg.html#regression-3",
    "href": "Pages/Lectures/Corr_Reg/corr_reg.html#regression-3",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Regression",
    "text": "Regression\n\nSuch estimators (i.e. those that minimize the RSS) are said to be ordinary least squares (OLS) estimates.\n\nThe resulting line \\(\\widehat{\\beta_0} + \\widehat{\\beta_1} \\cdot \\texttt{x}\\) is thus called the OLS Regression Line\n\nIt turns out that the OLS estimates of \\(\\beta_0\\) and \\(\\beta_1\\) are: \\[\\begin{align*}\n\\widehat{\\beta_1}   & = \\frac{\\sum_{i=1}^{n} (x_i - \\overline{x})(y_i - \\overline{y})}{\\sum_{i=1}^{n} (x - \\overline{x})^2} = \\frac{s_Y}{s_X} \\cdot r \\\\\n\\widehat{\\beta_0}   & = \\overline{y} - \\widehat{\\beta_1} \\cdot \\overline{x}\n\\end{align*}\\] where r denotes Pearson’s Correlation Coefficient \\[ r = \\frac{1}{n - 1} \\sum_{i=1}^{n} \\left( \\frac{x_i - \\overline{x}}{s_X} \\right) \\left( \\frac{y_i - \\overline{y}}{s_Y} \\right)  \\]"
  },
  {
    "objectID": "Pages/Lectures/Corr_Reg/corr_reg.html#regression-4",
    "href": "Pages/Lectures/Corr_Reg/corr_reg.html#regression-4",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Regression",
    "text": "Regression\n\nThe values along the OLS regression line corresponding to x values observed in the dataset are called fitted values:\n\n\n\n\n\nIn a sense, the fitted values represent guess/estimate of the de-noised value of y"
  },
  {
    "objectID": "Pages/Lectures/Corr_Reg/corr_reg.html#regression-5",
    "href": "Pages/Lectures/Corr_Reg/corr_reg.html#regression-5",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Regression",
    "text": "Regression\n\nWe can use the OLS regression line to perform prediction; i.e. to infer response values associated with explanatory values that were not included in the original dataset."
  },
  {
    "objectID": "Pages/Lectures/Corr_Reg/corr_reg.html#example",
    "href": "Pages/Lectures/Corr_Reg/corr_reg.html#example",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Example",
    "text": "Example\n\n\n\n\n\n\nWorked-Out Example 3 (formerly a “Your Turn” Exercise)\n\n\n\n\nAn airline is interested in determining the relationship between flight duration (in minutes) and the net amount of soda consumed (in oz.). Letting x denote flight duration (the explanatory variable) and y denote amount of soda consumed (the response variable), a sample of size 100 yielded the following results: \\[ \\begin{array}{cc}\n  \\displaystyle \\sum_{i=1}^{100} x_i  = 10,\\!211.7;   & \\displaystyle \\sum_{i=1}^{100} (x_i - \\overline{x})^2 =  38,\\!760.68    \\\\\n  \\displaystyle \\sum_{i=1}^{100} y_i  = 14,\\!3995.8;   & \\displaystyle \\sum_{i=1}^{100} (y_i - \\overline{y})^2 =  87.23984   \\\\\n\\displaystyle \\sum_{i=1}^{100} (x_i - \\overline{x})(y_i - \\overline{y}) = 379.945 \\\\\n\\end{array} \\]\n\nFind the equation of the OLS Regression line.\nIf a particular flight has a duration of 110 minutes, how many ounces of soda would we expect to be consumed on the flight?"
  },
  {
    "objectID": "Pages/Lectures/Corr_Reg/corr_reg.html#solutions",
    "href": "Pages/Lectures/Corr_Reg/corr_reg.html#solutions",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Solutions",
    "text": "Solutions\n\n\\[\\begin{align*}\n  \\widehat{\\beta_0}   & = \\frac{\\sum_{i=1}^{n}(x_i - \\overline{x})(y_i - \\overline{y})} {\\sum_{i=1}^{n} (x_i - \\overline{x})^2} = \\frac{379.945}{38,\\!760.68} \\approx \\boxed{0.0098}   \\\\\n  \\widehat{\\beta_0}   & = \\overline{y} - \\widehat{\\beta_1} \\cdot \\overline{x} = \\boxed{1438.957}\n\\end{align*}\\] Therefore, \\[ \\widehat{(\\texttt{amt. of soda})} = 1436.159 + (0.0098) \\cdot (\\texttt{flight duration}) \\]\n\n\n\n\n\\(\\widehat{y}^{(110)} = 1438.957 + (0.0098)(110) = \\boxed{1440.035 \\text{ oz.}}\\)"
  },
  {
    "objectID": "Pages/Lectures/Corr_Reg/corr_reg.html#extrapolation",
    "href": "Pages/Lectures/Corr_Reg/corr_reg.html#extrapolation",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Extrapolation",
    "text": "Extrapolation\n\nRemember that it is dangerous to try and use the OLS regression line to predict response values for explanatory variables that are far outside of the scope of the original data.\nFor example, if the dataset in the previous example only included flights between 100 minutes and 230 minutes, it would be dangerous to try to predict the amount of soda that would be consumed on a 13-hr flight (780 mins) using the OLS regression line, as we cannot be certain that the relationship between amt. of soda and flight duration remains linear for larger values of flight duration.\nRecall that this relates to extrapolation."
  },
  {
    "objectID": "Pages/Lectures/Corr_Reg/corr_reg.html#inference-on-the-slope",
    "href": "Pages/Lectures/Corr_Reg/corr_reg.html#inference-on-the-slope",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Inference on the Slope",
    "text": "Inference on the Slope\n\nWe also talked about how we can perform inference on the slope \\(\\beta_1\\) of the OLS regression line.\nSpecifically, we may want to test \\[ \\left[ \\begin{array}{rl}\nH_0:    & \\beta_1 = 0   \\\\\nH_A:    & \\beta_1 \\neq 0\n\\end{array} \\right. \\]\n\nThe reason we want to test this is that, if we have reason to believe that \\(\\beta_1\\) could be zero, then there might not be a linear relationship between y and x at all!\n\nUnder normality conditions, \\[ \\frac{\\widehat{\\beta_1} - \\beta_1}{\\mathrm{SD}(\\widehat{\\beta_1})} \\stackrel{H_0}{\\sim} t_{n - 2} \\]"
  },
  {
    "objectID": "Pages/Lectures/Corr_Reg/corr_reg.html#example-1",
    "href": "Pages/Lectures/Corr_Reg/corr_reg.html#example-1",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Example",
    "text": "Example\n\n\n\n\n\n\nWorked-Out Example 4\n\n\n\n\nThe results of regressing a variable y onto another variable x are shown below:\n\n\n\n\nEstimate\nStd. Error\nt-value\nPr(>|t|)\n\n\n\n\nIntercept\n-0.05185\n0.24779\n-0.209\n0.836\n\n\nSlope\n0.08783\n0.07869\n1.116\n0.272\n\n\n\nIs it possible that there exists no linear relationship between y and x? (Use a 5% level of significance wherever necessary.) Explain.\n\n\n\n\n\n\nSince the _p_value of testing \\(H_0: \\beta_1 = 0\\) vs \\(H_A: \\beta_1 \\neq 0\\) is \\(0.272\\), which is greater than a significance level of 5%, we would fail to reject the null; that is, it is possible that there exists no linear relationship between y and x."
  },
  {
    "objectID": "Pages/Lectures/Corr_Reg/corr_reg.html#sampling-procedures",
    "href": "Pages/Lectures/Corr_Reg/corr_reg.html#sampling-procedures",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Sampling Procedures",
    "text": "Sampling Procedures\n\nFinally, last lecture, we returned to the basics- data!\nSpecifically, we discussed different ways data can be collected; i.e. the different sampling procedures that are available to us.\nIn a simple random sample, every individual in the population has an equal chance of being included in the sample.\n\nThis can sometimes be costly, or even lead to biased samples.\n\nIn a stratified sampling scheme, the population is first divided into several strata (groups), and an SRS is taken from each stratum.\n\nThis has the benefit of creating a potentially more representative sample, though can still be quite costly. Results are also heavily dependent on the strata that were created.\n\nA cluster sampling scheme again divides the population into groups (now called clusters), takes an SRS of clusters, and then takes an SRS from the selected clusters.\n\nThis has the benefit of being (potentially) cheaper, but can again lead to biased samples and is also heavily dependent on the clusters that were created."
  },
  {
    "objectID": "Pages/Lectures/Corr_Reg/corr_reg.html#sampling-procedures-1",
    "href": "Pages/Lectures/Corr_Reg/corr_reg.html#sampling-procedures-1",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Sampling Procedures",
    "text": "Sampling Procedures\n\nA convenience sample is one in which individuals are included (or excluded) from the sample based on convenience; e.g. people who are nearby (geographically) are included whereas people who are farther away are not.\n\nConvenience Samples are cheap and, well, convenient, but can lead to very skewed or biased results.\n\nSpeaking of bias, there was another form of bias we discussed: non-response bias.\n\nThis occurs when certain individuals (or potentially even demographics, genders, etc.) do not participate in a survey, despite having been included in the sample of surveyed individuals."
  },
  {
    "objectID": "Pages/Lectures/Corr_Reg/corr_reg.html#other-distinctions",
    "href": "Pages/Lectures/Corr_Reg/corr_reg.html#other-distinctions",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Other Distinctions",
    "text": "Other Distinctions\n\nIn an observational study, treatment is neither administered nor withheld from subjects.\nIn an experiment, treatment is administered (or possibly withheld) from subjects.\nIn a longitudinal study, subjects are tracked over a period of time. (Observations are therefore correlated)\nIn a cross-sectional study, there is no tracking of subjects over time."
  },
  {
    "objectID": "Pages/Lectures/Corr_Reg/corr_reg.html#example-2",
    "href": "Pages/Lectures/Corr_Reg/corr_reg.html#example-2",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Example",
    "text": "Example\n\n\n\n\n\n\n\nExample (1.20 from OpenIntro)\n\n\n\n\nOn a large college campus first-year students and sophomores live in dorms located on the eastern part of the campus and juniors and seniors live in dorms located on the western part of the campus. Suppose you want to collect student opinions on a new housing structure the college administration is proposing and you want to make sure your survey equally represents opinions from students from all years.\n\nWhat type of study is this?\nSuggest a sampling strategy for carrying out this study."
  },
  {
    "objectID": "Pages/Lectures/Corr_Reg/corr_reg.html#solutions-1",
    "href": "Pages/Lectures/Corr_Reg/corr_reg.html#solutions-1",
    "title": "PSTAT 5A: Lecture 20",
    "section": "Solutions",
    "text": "Solutions\n\nTreatment has neither been administered nor withheld, meaning this is an observational study.\n\n\n\n\nStratified sampling seems like the way to go, with western campus and eastern campus being the two strata.\n\nSpecifically, we should take an SRS from both western campus and eastern campus students, to ensure that students across all years are (somewhat) equally represented."
  },
  {
    "objectID": "Pages/Lectures/Lecture00/Lec00.html#course-staff",
    "href": "Pages/Lectures/Lecture00/Lec00.html#course-staff",
    "title": "PSTAT 5A: Lecture 00",
    "section": "Course Staff",
    "text": "Course Staff\n\n\n\n\nInstructor:\n\nEthan (He/Him)\nepmarzban@pstat.ucsb.edu\nOH: HW Clinic: Tuesdays, 4:30 - 5:30pm in ELLSN 2626  OH: Fridays, 12 - 1pm in SH 5607F\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is a HW Clinic?\n\nBasically, it will be like a regular office hours but with a focus on Homework (along with a possible short review at the start)."
  },
  {
    "objectID": "Pages/Lectures/Lecture00/Lec00.html#course-staff-1",
    "href": "Pages/Lectures/Lecture00/Lec00.html#course-staff-1",
    "title": "PSTAT 5A: Lecture 00",
    "section": "Course Staff",
    "text": "Course Staff\n\n\nTeaching Assistants:\n\n\n\n\nNickolas Thiessen\nnickolas@ucsb.edu\nOH: F, 9 - 11am in B434 Rm 113\n\n\n\n\n\nJason Teng\njteng@ucsb.edu\nOH: Th, 10 - 11am in SH 5421\n\n\n\n\n\n\nYuan Zhou\nyuan_zhou@ucsb.edu\nOH: T, 11am - 12pm in SH 5421"
  },
  {
    "objectID": "Pages/Lectures/Lecture00/Lec00.html#course-staff-2",
    "href": "Pages/Lectures/Lecture00/Lec00.html#course-staff-2",
    "title": "PSTAT 5A: Lecture 00",
    "section": "Course Staff",
    "text": "Course Staff\nUndergraduate Learning Assistant:\n\n\nCatherine Li\ncatherine_li@ucsb.edu\nOH: T, 2 - 4pm and Th, 9 - 11am (over Zoom)\nStudy Groups:\n\nT 3:30 - 4:30pm (location TBD)\nTh 3:30 - 5:30pm (location TBD)\n\n\n\n\n\nCatherine’s Help Hours will begin Next Week"
  },
  {
    "objectID": "Pages/Lectures/Lecture00/Lec00.html#course-resources",
    "href": "Pages/Lectures/Lecture00/Lec00.html#course-resources",
    "title": "PSTAT 5A: Lecture 00",
    "section": "Course Resources",
    "text": "Course Resources\n\nCanvas: for grades and quizzes\nGradescope: for homeworks and exams\nCourse Website: https://pstat5a.github.io\n\nAll relevant course material will be posted to the website!\nOne exception: quizzes, which will be randomized across students and administered over Canvas.\n\nPlease read the syllabus fully and carefully!\n\nEspecially when it comes to switching Sections!"
  },
  {
    "objectID": "Pages/Lectures/Lecture00/Lec00.html#discord",
    "href": "Pages/Lectures/Lecture00/Lec00.html#discord",
    "title": "PSTAT 5A: Lecture 00",
    "section": "Discord",
    "text": "Discord\n\n\n\n\n\n\n\n\n\n\nbit.ly/sp235adisc"
  },
  {
    "objectID": "Pages/Lectures/Lecture00/Lec00.html#bit.lysp235adisc",
    "href": "Pages/Lectures/Lecture00/Lec00.html#bit.lysp235adisc",
    "title": "PSTAT 5A: Lecture 00",
    "section": "bit.ly/sp235adisc",
    "text": "bit.ly/sp235adisc"
  },
  {
    "objectID": "Pages/Lectures/Lecture00/Lec00.html#what-is-data-science-1",
    "href": "Pages/Lectures/Lecture00/Lec00.html#what-is-data-science-1",
    "title": "PSTAT 5A: Lecture 00",
    "section": "What is Data Science?",
    "text": "What is Data Science?\n\nNot a bad definition!\nThough, there isn’t a single agreed-upon definition of what data science is.\nMost people agree that Data science is cross-disciplinary, drawing experience and expertise from a wide variety of different fields.\n\nPerhaps the two main fields from which Data Science draws are Statistics and Computer Science\n\nLike ChatGPT suggested, computation is an integral part of Data Science.\n\nAs we will soon see, the data that is being analyzed these days is huge; certainly too large to be able to do anything with it on pen and paper."
  },
  {
    "objectID": "Pages/Lectures/Lecture00/Lec00.html#the-path-forward",
    "href": "Pages/Lectures/Lecture00/Lec00.html#the-path-forward",
    "title": "PSTAT 5A: Lecture 00",
    "section": "The Path Forward",
    "text": "The Path Forward\n\nSo, how does this course factor into things?\nFrom the course description:\n\n\n\nIntroduction to data science. Concepts of statistical thinking. Topics include random variables, sampling distributions, hypothesis testing, correlation and regression. Visualizing, analyzing and interpreting real world data using Python. Computing labs required.\n\n\n\nSo this course is designed to be an introduction to data science."
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#last-time",
    "href": "Pages/Lectures/Lecture09/Lec09.html#last-time",
    "title": "PSTAT 5A: Lecture 09",
    "section": "Last Time",
    "text": "Last Time\n\nLast lecture we started talking about random variables.\nA random variable is a numeric outcome of some random process or experiment.\n\nFor example, “number of heads observed in \\(5\\) independent tosses of a fair coin”\n\nThe state space of a random variable \\(X\\) is the set \\(S_X\\) of possible values the random variable could attain.\n\nIf \\(S_X\\) has jumps, we say \\(X\\) is a “discrete random variable”\nOtherwise, we say \\(X\\) is a “continuous random variable.”\n\nToday we’ll talk about continuous random variables."
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#continuous-random-variables",
    "href": "Pages/Lectures/Lecture09/Lec09.html#continuous-random-variables",
    "title": "PSTAT 5A: Lecture 09",
    "section": "Continuous Random Variables",
    "text": "Continuous Random Variables\n\nContinuous random variables are described by their so-called probability density function (or p.d.f. for short).\n\nThe graph of a p.d.f. is called the density curve.\n\nThe p.d.f. is such that probabilities are found as areas underneath the density curve.\nFor example, if the random variable \\(X\\) has the following density curve…"
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#two-properties",
    "href": "Pages/Lectures/Lecture09/Lec09.html#two-properties",
    "title": "PSTAT 5A: Lecture 09",
    "section": "Two Properties",
    "text": "Two Properties\n\nSince probabilities are areas underneath the density curve, we arrive at the following two properties (which themselves follow from the Axioms of Probability):\n\n\n\n\n\n\n\n\nProperties of a P.D.F.\n\n\n\n\nDensity curves must always be nonnegative; i.e. the corresponding p.d.f. \\(f_X(x)\\) must obey \\(f_X(x) \\geq 0\\) for every \\(x\\).\nThe area underneath a density curve must be \\(1\\).\n\n\n\n\n\n\n\nIn this lecture, we will examine two continuous distributions: the uniform distribution, and the normal distribution.\n\nWe will see that the density curves/p.d.f.’s of these two distributions will satisfy the above two properties."
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#uniform-distribution-1",
    "href": "Pages/Lectures/Lecture09/Lec09.html#uniform-distribution-1",
    "title": "PSTAT 5A: Lecture 09",
    "section": "Uniform Distribution",
    "text": "Uniform Distribution\n\nThe uniform distribution takes two parameters: \\(a\\) and \\(b\\), with \\(a < b\\).\n\nWe denote the fact that a random variable \\(X\\) follows the uniform distribution with parameters \\(a\\) and \\(b\\) using the notation \\[ X \\sim \\mathrm{Unif}(a, \\ b) \\]\n\nThe \\(\\mathrm{Unif}(a, \\ b)\\) distribution has the following p.d.f.: \\[ f_X(x) = \\begin{cases} \\displaystyle \\frac{1}{b - a} & \\text{if } a \\leq x \\leq b \\\\[3mm] 0 & \\text{otherwise} \\\\ \\end{cases} \\] which corresponds to a rectangular density curve:"
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#uniform-density-curves",
    "href": "Pages/Lectures/Lecture09/Lec09.html#uniform-density-curves",
    "title": "PSTAT 5A: Lecture 09",
    "section": "Uniform Density Curves",
    "text": "Uniform Density Curves\n\nOftentimes, we will be a bit lazy with our density curve and omit the open/closed circles. For example, we might sketch the density curve of the \\(\\mathrm{Unif}(1, \\ 2.15)\\) distribution as"
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#effect-of-changing-a-and-b",
    "href": "Pages/Lectures/Lecture09/Lec09.html#effect-of-changing-a-and-b",
    "title": "PSTAT 5A: Lecture 09",
    "section": "Effect of Changing \\(a\\) and \\(b\\)",
    "text": "Effect of Changing \\(a\\) and \\(b\\)\n\nviewof a = Inputs.range(\n  [-3, 3], \n  {value: 0, step: 0.1, label: \"a=\"}\n)\n\nviewof b = Inputs.range(\n  [-3, 3], \n  {value: 1, step: 0.1, label: \"b=\"}\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmargin2 = ({top: 20, right: 30, bottom: 30, left: 40})\n\nheight2 = 400\n\nx_values2 = d32.scaleLinear()\n.domain(d32.extent(data2, d => d.x))\n.range([margin2.left, width - margin2.right])\n\ny_values2 = d32.scaleLinear()\n.domain([Math.min(d32.min(data2, d => d.y),0), Math.max(1,d32.max(data2, d => d.y))]).nice()\n.range([height2 - margin2.bottom, margin2.top])\n\nline2 = d32.line()\n.x(d => x_values2(d.x))\n.y(d => y_values2(d.y))\n\nxAxis2 = g => g\n.attr(\"transform\", `translate(0,${height2 - margin2.bottom})`)\n.call(d32.axisBottom(x_values2)\n      .ticks(width / 80)\n      .tickSizeOuter(0))\n\nyAxis2 = g => g\n.attr(\"transform\", `translate(${margin2.left},0)`)\n.call(d32.axisLeft(y_values2)\n      .tickValues(d32.scaleLinear().domain(y_values2.domain()).ticks()))\n\nfunction unif_pdf (input_value, mu, sigsq) {\nif(input_value < a){\n  return 0\n} else if(input_value > b){\n  return 0\n} else{\n  return 1 / (b - a)\n}\n}\n\nabs_x2=6\n\ndata2 = {\n  let values = [];\n  for (let x = -abs_x2; x < abs_x2; x=x+0.01) values.push({\"x\":x,\"y\":unif_pdf(x, µ, sigsquared)});\n  return values;\n}\n\nd32 = require(\"https://d3js.org/d3.v5.min.js\")\n\nchart2 = {\n  const svg = d32.select(DOM.svg(width, height2));\n  \n  svg.append(\"g\")\n  .call(xAxis2);\n  \n  svg.append(\"g\")\n  .call(yAxis2);\n  \n  svg.append(\"path\")\n  .datum(data2)\n  .attr(\"fill\", \"none\")\n  .attr(\"stroke\", \"steelblue\")\n  .attr(\"stroke-width\", 4)\n  .attr(\"stroke-linejoin\", \"round\")\n  .attr(\"stroke-linecap\", \"round\")\n  .attr(\"d\", line);\n  \n  return svg.node();\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCredit to https://observablehq.com/@dswalter/normal-distribution for the base of the applet code"
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#uniform-probabilities",
    "href": "Pages/Lectures/Lecture09/Lec09.html#uniform-probabilities",
    "title": "PSTAT 5A: Lecture 09",
    "section": "Uniform Probabilities",
    "text": "Uniform Probabilities\n\nRecall, from our initial discussion on continuous random variables, that probabilities are found as areas underneath the density curve.\nDue to the rectangular shape of the Uniform density curves, finding probabilities under the Uniform distribution ends up being relatively straightforward (so long as we remember how to find the area of a rectangle!)\nLet’s work through an example together.\n\n\n\n\n\n\n\n\nWorked-Out Example 1\n\n\n\n\nIf \\(X \\sim \\mathrm{Unif}(-1, \\ 1)\\), compute \\(\\mathbb{P}(X \\leq 0.57)\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#solution",
    "href": "Pages/Lectures/Lecture09/Lec09.html#solution",
    "title": "PSTAT 5A: Lecture 09",
    "section": "Solution",
    "text": "Solution\n\nWhen working through probability problems involving continuous distributions, sketching a picture is always a good first step.\n\nSometimes, we will explicitly make that the first step of a problem, meaning failure to sketch a relevant picture may result in less-than-full marks!\n\nThe density curve of the \\(\\mathrm{Unif}(-1, \\ 1)\\) distribution is given by"
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#solution-1",
    "href": "Pages/Lectures/Lecture09/Lec09.html#solution-1",
    "title": "PSTAT 5A: Lecture 09",
    "section": "Solution",
    "text": "Solution\n\nThe desired probability is thus\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is a rectangle with base \\((0.57 - (-1)) = 1.57\\) and height \\(1 / (1 - (-1)) = 1/2\\). Therefore, the area of this rectangle - and, also, the desired probability - is \\[ (1.57) \\times \\frac{1}{2} = \\boxed{0.785 = 78.5\\%} \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#another-example",
    "href": "Pages/Lectures/Lecture09/Lec09.html#another-example",
    "title": "PSTAT 5A: Lecture 09",
    "section": "Another Example",
    "text": "Another Example\n\n\n\n\n\n\n\nWorked-Out Example 2\n\n\n\n\nIf \\(X \\sim \\mathrm{Unif}(0, 1)\\), compute \\(\\mathbb{P}(0.25 \\leq X \\leq 0.75)\\).\n\n\n\n\n\n\n\nWe are going to solve this problem in two different ways.\nAgain, we always begin with a sketch of the desired probability as an area underneath the density curve:"
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#tail-probabilities",
    "href": "Pages/Lectures/Lecture09/Lec09.html#tail-probabilities",
    "title": "PSTAT 5A: Lecture 09",
    "section": "Tail Probabilities",
    "text": "Tail Probabilities\n\nThis is not a coincidence!\nFor a more arbitrary distribution:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncan be decomposed as\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[ \\huge - \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#tail-probabilities-1",
    "href": "Pages/Lectures/Lecture09/Lec09.html#tail-probabilities-1",
    "title": "PSTAT 5A: Lecture 09",
    "section": "Tail Probabilities",
    "text": "Tail Probabilities\n\nIn math, what we have found is:\n\n\n\n\n\n\n\n\nImportant\n\n\n\n\\[ \\mathbb{P}(x_1 \\leq X \\leq x_2)  = \\mathbb{P}(X \\leq x_2) - \\mathbb{P}(X \\leq x_1) \\]\n\n\n\n\n\n\nThe quantity \\(\\mathbb{P}(X \\leq x)\\), where we view \\(x\\) as an arbitrary input (and hence the quantity \\(\\mathbb{P}(X \\leq x)\\) as a function of \\(x\\)) is called the cumulative distribution function (or c.d.f. for short) of \\(X\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#your-turn",
    "href": "Pages/Lectures/Lecture09/Lec09.html#your-turn",
    "title": "PSTAT 5A: Lecture 09",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\n\n\n\n\n\nExercise 1\n\n\n\n\nThe time (in minutes) spent waiting in line at Starbucks is found to vary uniformly between 5mins and 15mins.\n\nDefine the random variable of interest, and call it \\(X\\).\nIf a person is selected at random from the line at Starbucks, what is the probability that they spend between 3 and 7 minutes waiting in line?\nWhat is the c.d.f. of wait times? (I.e., find the probability that a randomly selected person spends less than \\(x\\) minutes waiting in line, for an arbitrary value \\(x\\). Yes, your final answer will depend on \\(x\\); that’s why the c.d.f. is a function!)"
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#probability-of-attaining-an-exact-value",
    "href": "Pages/Lectures/Lecture09/Lec09.html#probability-of-attaining-an-exact-value",
    "title": "PSTAT 5A: Lecture 09",
    "section": "Probability of Attaining an Exact Value",
    "text": "Probability of Attaining an Exact Value\n\nIf \\(X \\sim \\mathrm{Unif}[0, 1]\\), what is the probability that \\(X\\) equals, say \\(0.5\\)?\n\nThe area this corresponds to is a rectangle of height \\(1 / (1 - 0) = 1\\), but with width \\(0\\).\nTherefore, the probability is zero.\n\nThis is not unique to the Uniform distribution!\n\n\n\n\n\n\n\n\nProbability of Attaining an Exact Value\n\n\n\nIf \\(X\\) is a continuous random variable, \\(\\mathbb{P}(X = x) = 0\\) for any value \\(x\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#mean-and-variance-of-the-uniform-distribution",
    "href": "Pages/Lectures/Lecture09/Lec09.html#mean-and-variance-of-the-uniform-distribution",
    "title": "PSTAT 5A: Lecture 09",
    "section": "Mean and Variance of the Uniform Distribution",
    "text": "Mean and Variance of the Uniform Distribution\n\nIf \\(X \\sim \\mathrm{Unif}[a, b]\\), we have the following results:\n\n\\(\\displaystyle \\mathbb{E}[X] = \\frac{a + b}{2}\\)\n\\(\\displaystyle \\mathrm{Var}(X) = \\frac{1}{12}(b - a)^2\\)\n\n\n\n\n\n\n\n\n\nExercise 2\n\n\n\n\nConsider again the setup of Exerise 1: the time (in minutes) spent waiting in line at Starbucks is found to vary uniformly on between 5mins and 15mins.\n\nIf we select a person at random, what is the expected amount of time (in minutes) they will spend waiting in line? What about the variance and standard deviation of the time (in minutes) they will spend waiting in line?"
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#normal-distribution-1",
    "href": "Pages/Lectures/Lecture09/Lec09.html#normal-distribution-1",
    "title": "PSTAT 5A: Lecture 09",
    "section": "Normal Distribution",
    "text": "Normal Distribution\n\nThe normal distribution takes two parameters \\(\\mu\\) and \\(\\sigma\\). We use the notation \\(X \\sim \\mathcal{N}(\\mu, \\ \\sigma)\\) to denote “\\(X\\) follows the normal distribution with parameters \\(\\mu\\) and \\(\\sigma\\).”\nThe normal distribution has distribution function given by \\[ f(x) = \\frac{1}{\\sigma \\cdot \\sqrt{2 \\pi}} \\cdot \\exp\\left\\{ - \\frac{1}{2} \\cdot \\left( \\frac{x - \\mu}{\\sigma} \\right)^2 \\right\\} \\]\nLet’s determine how the parameters affect the shape of the density curve."
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#changing-mu-and-sigma",
    "href": "Pages/Lectures/Lecture09/Lec09.html#changing-mu-and-sigma",
    "title": "PSTAT 5A: Lecture 09",
    "section": "Changing \\(\\mu\\) and \\(\\sigma\\)",
    "text": "Changing \\(\\mu\\) and \\(\\sigma\\)\n\nviewof µ = Inputs.range(\n  [-3, 3], \n  {value: 0, step: 0.1, label: \"µ:\"}\n)\n\nviewof σ = Inputs.range(\n  [0.2, 3.1], \n  {value: 1, step: 0.01, label: \"σ:\"}\n)\n\nsigsquared = σ**2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmargin = ({top: 20, right: 30, bottom: 30, left: 40})\n\nheight = 400\n\nx_values = d3.scaleLinear()\n    .domain(d3.extent(data, d => d.x))\n    .range([margin.left, width - margin.right])\n\ny_values = d3.scaleLinear()\n    .domain([Math.min(d3.min(data, d => d.y),0), Math.max(1,d3.max(data, d => d.y))]).nice()\n    .range([height - margin.bottom, margin.top])\n    \nline = d3.line()\n    .x(d => x_values(d.x))\n    .y(d => y_values(d.y))\n\nxAxis = g => g\n  .attr(\"transform\", `translate(0,${height - margin.bottom})`)\n  .call(d3.axisBottom(x_values)\n      .ticks(width / 80)\n      .tickSizeOuter(0))\n\nyAxis = g => g\n  .attr(\"transform\", `translate(${margin.left},0)`)\n  .call(d3.axisLeft(y_values)\n      .tickValues(d3.scaleLinear().domain(y_values.domain()).ticks()))\n    \nfunction normal_pdf (input_value, mu, sigsq) {\n  let left_chunk = 1/(Math.sqrt(2*Math.PI*sigsq))\n  let right_top = -((input_value-mu)**2)\n  let right_bottom = 2*sigsq\n  return left_chunk * Math.exp(right_top/right_bottom)\n}\n\nabs_x=6\n\ndata = {\n  let values = [];\n  for (let x = -abs_x; x < abs_x; x=x+0.01) values.push({\"x\":x,\"y\":normal_pdf(x, µ, sigsquared)});\n  return values;\n}\n\nd3 = require(\"https://d3js.org/d3.v5.min.js\")\n\nchart = {\n  const svg = d3.select(DOM.svg(width, height));\n\n  svg.append(\"g\")\n      .call(xAxis);\n\n  svg.append(\"g\")\n      .call(yAxis);\n  \n  svg.append(\"path\")\n      .datum(data)\n      .attr(\"fill\", \"none\")\n      .attr(\"stroke\", \"steelblue\")\n      .attr(\"stroke-width\", 4)\n      .attr(\"stroke-linejoin\", \"round\")\n      .attr(\"stroke-linecap\", \"round\")\n      .attr(\"d\", line);\n  \n  return svg.node();\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCredit to https://observablehq.com/@dswalter/normal-distribution for the majority of the applet code"
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#changing-mu",
    "href": "Pages/Lectures/Lecture09/Lec09.html#changing-mu",
    "title": "PSTAT 5A: Lecture 09",
    "section": "Changing \\(\\mu\\)",
    "text": "Changing \\(\\mu\\)\nHolding \\(\\sigma = 1\\) fixed and varying \\(\\mu\\), we find:"
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#changing-sigma",
    "href": "Pages/Lectures/Lecture09/Lec09.html#changing-sigma",
    "title": "PSTAT 5A: Lecture 09",
    "section": "Changing \\(\\sigma\\)",
    "text": "Changing \\(\\sigma\\)\nHolding \\(\\mu = 0\\) fixed and varying \\(\\sigma\\), we find:"
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#standard-normal-distribution",
    "href": "Pages/Lectures/Lecture09/Lec09.html#standard-normal-distribution",
    "title": "PSTAT 5A: Lecture 09",
    "section": "Standard Normal Distribution",
    "text": "Standard Normal Distribution\n\n\n\n\n\n\n\nDefinition\n\n\n\nThe standard normal distribution is the normal distribution with \\(\\mu = 0\\) and \\(\\sigma = 1\\); i.e. \\(\\mathcal{N}(0, 1)\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#normal-probabilities",
    "href": "Pages/Lectures/Lecture09/Lec09.html#normal-probabilities",
    "title": "PSTAT 5A: Lecture 09",
    "section": "Normal Probabilities",
    "text": "Normal Probabilities\n\nRecall that for continuous variables, probabilities are found as areas underneath the density curve. For example, if \\(X \\sim \\mathcal{N}(0, 1)\\), then \\(\\mathbb{P}(X \\leq -1)\\) is found by computing the area below:"
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#normal-probabilities-1",
    "href": "Pages/Lectures/Lecture09/Lec09.html#normal-probabilities-1",
    "title": "PSTAT 5A: Lecture 09",
    "section": "Normal Probabilities",
    "text": "Normal Probabilities\n\nNow, unlike with the Uniform density curve, we don’t have a simple closed-form formula for areas under the Normal curve.\nFor instance, how would you get a numerical value for the area shaded on the previous slide?\nThe answer is by way of what is known as a normal table, or z-table.\nTo illustrate how to read a normal table, let’s work through an example:\n\n\n\n\n\n\n\n\nWorked-Out Example 3\n\n\n\n\nIf \\(Z \\sim \\mathcal{N}(0, 1)\\), compute \\(\\mathbb{P}(Z \\leq 0.83)\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#normal-table",
    "href": "Pages/Lectures/Lecture09/Lec09.html#normal-table",
    "title": "PSTAT 5A: Lecture 09",
    "section": "Normal Table",
    "text": "Normal Table"
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#reading-the-normal-table",
    "href": "Pages/Lectures/Lecture09/Lec09.html#reading-the-normal-table",
    "title": "PSTAT 5A: Lecture 09",
    "section": "Reading the Normal Table",
    "text": "Reading the Normal Table\n\nTo find \\(\\mathbb{P}(Z \\leq 0.83)\\), we break up \\(0.83\\) as \\[ 0.83 = 0.8 + 0.03 \\]\nThis tells us to find the desired probability in the intersection of the \\(0.8\\) row and the \\(0.03\\) column:"
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#another-example-1",
    "href": "Pages/Lectures/Lecture09/Lec09.html#another-example-1",
    "title": "PSTAT 5A: Lecture 09",
    "section": "Another Example",
    "text": "Another Example\n\n\n\n\n\n\n\n\n\nWorked-Out Example 4\n\n\n\n\nIf \\(Z \\sim \\mathcal{N}(0, 1)\\), find\n\n\\(\\mathbb{P}(Z \\leq -1.01)\\)\n\\(\\mathbb{P}(Z \\leq -2.25)\\)\n\\(\\mathbb{P}(-2.25 \\leq Z \\leq -1.01)\\)\n\\(\\mathbb{P}(X \\geq -0.7)\\)"
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#standardization",
    "href": "Pages/Lectures/Lecture09/Lec09.html#standardization",
    "title": "PSTAT 5A: Lecture 09",
    "section": "Standardization",
    "text": "Standardization\n\nNow, all of our considerations above were in the case of the standard normal distribution. How do we find areas under nonstandard normal density curves?\nThe answer: we use a process called standardization.\n\n\n\n\n\n\n\n\nStandardization\n\n\n\nIf \\(X \\sim \\mathcal{N}(\\mu, \\ \\sigma)\\), then \\[ \\left( \\frac{X - \\mu}{\\sigma} \\right) \\sim \\mathcal{N}(0, 1) \\] That is, if we take a normally distributed random variable, subtract off its mean, and divide by its standard deviation, we obtain a random variable whose distribution is the standard normal distribution."
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#normal-probabilities-general-case",
    "href": "Pages/Lectures/Lecture09/Lec09.html#normal-probabilities-general-case",
    "title": "PSTAT 5A: Lecture 09",
    "section": "Normal Probabilities; General Case",
    "text": "Normal Probabilities; General Case\n\nThus, if \\(X \\sim \\mathcal{N}(\\mu, \\ \\sigma)\\), here are the steps we use to compute \\(\\mathbb{P}(X \\leq x)\\):\n\nCompute the \\(z-\\)score \\(z = \\frac{x - \\mu}{\\sigma}\\), rounded to two decimal places.\nLook up the corresponding entry in a standard normal table."
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#your-turn-1",
    "href": "Pages/Lectures/Lecture09/Lec09.html#your-turn-1",
    "title": "PSTAT 5A: Lecture 09",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\n\n\n\n\n\nExercise 3\n\n\n\n\nIt is found that the scores on a particular exam are normally distributed with a mean of 83 and a standard deviation of 5.\n\nDefine the random variable of interest, and call it \\(X\\).\nIf a student is selected at random, what is the probability that they scored 81 or lower?\nIf a student is selected at random, what is the probability that they scored 75 or higher?"
  },
  {
    "objectID": "Pages/Lectures/Lecture09/Lec09.html#mean-and-variance-of-the-normal-distribution",
    "href": "Pages/Lectures/Lecture09/Lec09.html#mean-and-variance-of-the-normal-distribution",
    "title": "PSTAT 5A: Lecture 09",
    "section": "Mean and Variance of the Normal Distribution",
    "text": "Mean and Variance of the Normal Distribution\n\nIf \\(X \\sim \\mathcal{N}(\\mu, \\ \\sigma)\\), we have the following results:\n\n\\(\\displaystyle \\mathbb{E}[X] = \\mu\\)\n\\(\\displaystyle \\mathrm{Var}(X) = \\sigma^2\\)\n\nSo, the two parameters we use to describe the normal distribution are the mean and the variance.\nWe’ll talk more about parameters in the next lecture."
  },
  {
    "objectID": "Pages/Lectures/Lecture08/Lec08.html#recap-of-probability",
    "href": "Pages/Lectures/Lecture08/Lec08.html#recap-of-probability",
    "title": "PSTAT 5A: Lecture 08",
    "section": "Recap of Probability",
    "text": "Recap of Probability\n\nRecall the basic ingredients of probability we have discussed so far:\n\nExperiment: any procedure we can repeat an infinite number of times, where on each reptition there is a fixed set of things (called outcomes) that can happen.\nOutcome space (\\(\\boldsymbol{\\Omega}\\)): the set of all outcomes associated with a particular experiment\nEvent: a subset of the outcome space\nProbability: a function that maps events to a number; specifically, one that quantifies our beliefs about a particular event"
  },
  {
    "objectID": "Pages/Lectures/Lecture08/Lec08.html#an-experiment",
    "href": "Pages/Lectures/Lecture08/Lec08.html#an-experiment",
    "title": "PSTAT 5A: Lecture 08",
    "section": "An Experiment",
    "text": "An Experiment\n\nLet’s actually conduct an experiment together!\nSpecifically, suppose we toss a coin 3 times and record the outcomes.\n\n\n\nviewof toss = Inputs.button(\"Toss\")\n\n\n\n\n\n\n\ndummy = toss + 1\ncoin = [\"H\", \"T\"]\ns1 = coin[Math.floor(Math.random()*coin.length*dummy/dummy)];\ns2 = coin[Math.floor(Math.random()*coin.length*dummy/dummy)];\ns3 = coin[Math.floor(Math.random()*coin.length*dummy/dummy)];\ns4 = coin[Math.floor(Math.random()*coin.length*dummy/dummy)];\n[s1, s2, s3];\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdditionally, let’s keep track of the number of heads we observe each time we run this experiment.\n\nIn fact, let’s do this on the chalkboard."
  },
  {
    "objectID": "Pages/Lectures/Lecture08/Lec08.html#an-experiment-1",
    "href": "Pages/Lectures/Lecture08/Lec08.html#an-experiment-1",
    "title": "PSTAT 5A: Lecture 08",
    "section": "An Experiment",
    "text": "An Experiment\n\n\nAlright, let’s make note of a few things.\n\n\n\nNote that each time we run this experiment, we (sure enough) get an element of the outcome space, which is\n\n\n\n\n\n\n\n\n\ntree_diagram\n\n  \n\nbase\n\no   \n\nH1\n\nH   \n\nbase->H1\n\n    \n\nT1\n\nT   \n\nbase->T1\n\n    \n\nH21\n\nH   \n\nH1->H21\n\n    \n\nT21\n\nT   \n\nH1->T21\n\n    \n\nH22\n\nH   \n\nT1->H22\n\n    \n\nT22\n\nT   \n\nT1->T22\n\n    \n\nH311\n\nH   \n\nH21->H311\n\n    \n\nT311\n\nT   \n\nH21->T311\n\n    \n\nH321\n\nH   \n\nT21->H321\n\n    \n\nT321\n\nT   \n\nT21->T321\n\n    \n\nH312\n\nH   \n\nH22->H312\n\n    \n\nT312\n\nT   \n\nH22->T312\n\n    \n\nH322\n\nH   \n\nT22->H322\n\n    \n\nT322\n\nT   \n\nT22->T322"
  },
  {
    "objectID": "Pages/Lectures/Lecture08/Lec08.html#an-experiment-2",
    "href": "Pages/Lectures/Lecture08/Lec08.html#an-experiment-2",
    "title": "PSTAT 5A: Lecture 08",
    "section": "An Experiment",
    "text": "An Experiment\n\n\nBut, also note that each time we run the experiment, the number of heads doesn’t necessarily remain fixed.\n\n\n\nIn fact, each outcome in the outcome space corresponds to a different number of heads:\n\n\n\n\n\nOutcome\nNumber of Heads\n\n\n\n\n(H,  H,   H)\n3\n\n\n(H,   H,   T)\n2\n\n\n(H,   T,   H)\n2\n\n\n(T,   H,   H)\n2\n\n\n(H,   T,   T)\n1\n\n\n(T,   H,   T)\n1\n\n\n(T,   T,   H)\n1\n\n\n(T,   T,   T)\n0"
  },
  {
    "objectID": "Pages/Lectures/Lecture08/Lec08.html#random-variables",
    "href": "Pages/Lectures/Lecture08/Lec08.html#random-variables",
    "title": "PSTAT 5A: Lecture 08",
    "section": "Random Variables",
    "text": "Random Variables\n\nThis leads us to the notion of random variables.\nLoosely speaking, a random variable is a variable or process with a numerical outcome.\nWe denote random variables using capital letters; e.g. \\(X\\), \\(Y\\), \\(Z\\), \\(W\\), etc.\nSo, for example, \\(X =\\) “the number of heads in 3 tosses of a coin” is a random variable because (a) it is a numerical outcome of an experiment and (b) it is random (i.e. its value changes depending on the outcome of the experiment)."
  },
  {
    "objectID": "Pages/Lectures/Lecture08/Lec08.html#state-space",
    "href": "Pages/Lectures/Lecture08/Lec08.html#state-space",
    "title": "PSTAT 5A: Lecture 08",
    "section": "State Space",
    "text": "State Space\n\nA key part of the definition of random variables is that they must be numerical.\nWhat this means is we can always look at the set of values a random variable could take: this is what we call the state space of a random variable.\nFor example: if \\(X =\\) “number of heads in 3 tosses of a coin”, we see that \\(X\\) will only ever be \\(0\\), \\(1\\), \\(2\\), or \\(3\\).\n\nThis is because it is not possible to toss 3 coins and get, say, 5 heads, or a negative number of heads!\n\nWe often denote the state space of a random variable using the notation \\(S_{\\verb|<variable>|}\\); e.g. \\(S_X\\) to mean the state space of \\(X\\), \\(S_Y\\) to mean the state space of \\(Y\\), etc."
  },
  {
    "objectID": "Pages/Lectures/Lecture08/Lec08.html#classifying-random-variables",
    "href": "Pages/Lectures/Lecture08/Lec08.html#classifying-random-variables",
    "title": "PSTAT 5A: Lecture 08",
    "section": "Classifying Random Variables",
    "text": "Classifying Random Variables\n\nBecause random variables are numerical, their state spaces will always be numerical sets of values.\nThis means we can classify state spaces using our Variable Classification scheme from Week 1!\n\nSpecifically: the state space \\(S_X\\) of a random variable will either have “jumps”, or not.\n\nWe extend the same classification language to random variables:\n\n\n\n\n\n\n\n\nDefinition\n\n\n\nGiven a random variable \\(X\\), we say that:\n\n\\(X\\) is a discrete random variable (or just “\\(X\\) is discrete) if \\(S_X\\) is has jumps\n\\(X\\) is a continuous random variable (or just “\\(X\\) is continuous) if \\(S_X\\) has no jumps"
  },
  {
    "objectID": "Pages/Lectures/Lecture08/Lec08.html#leadup",
    "href": "Pages/Lectures/Lecture08/Lec08.html#leadup",
    "title": "PSTAT 5A: Lecture 08",
    "section": "Leadup",
    "text": "Leadup\n\nLet’s return to our coin tossing example.\nWhat is the probability that we observe zero heads?\nWell, in the language of our random variable \\(X\\) (which counts the number of heads in these three tosses of our fair coin), we can translate “zero heads” to the event “\\(\\{X = 0\\}\\)’’, meaning we want to find \\(\\mathbb{P}(X = 0)\\).\nObserving zero heads is equivalent to observing all tails, meaning the event \\(\\{X = 0\\}\\) is equivalent to the event { (T,  T,  T) }.\nNow, up to this point I have been careful to avoid explicitly mentioning whether our coin is fair or not.\n\nFor the time being, let’s assume that the probability our coin lands ‘heads’ on any given toss is some fixed value \\(p\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture08/Lec08.html#probability-mass-function",
    "href": "Pages/Lectures/Lecture08/Lec08.html#probability-mass-function",
    "title": "PSTAT 5A: Lecture 08",
    "section": "Probability Mass Function",
    "text": "Probability Mass Function\n\nThe table on the previous slide is called a probability mass function, and is often abbreviated as p.m.f..\nIn general, the p.m.f. of an arbitrary random variable \\(X\\) is a table or formula that specifies all the possible values a random variable can take (i.e. the state space), along with the probability with which the random variable attains those values.\nWe use the term “function” to describe this because, in abstraction, we can notate the p.m.f. as \\[ p_X(k) := \\mathbb{P}(X = k) \\] where \\(k\\) can be any value in the state space of \\(X\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture08/Lec08.html#example",
    "href": "Pages/Lectures/Lecture08/Lec08.html#example",
    "title": "PSTAT 5A: Lecture 08",
    "section": "Example",
    "text": "Example\n\n\n\n\n\n\nWorked-Out Example 1\n\n\n\n\nSuppose we toss three fair coins independently, and let \\(X\\) denote the number of heads observed. Construct the p.m.f. (probability mass function) of \\(X\\).\n\n\n\n\n\n\nBy our work from above, the p.m.f. of \\(X\\) is given by \\[\\begin{array}{r|cccc}\n\\boldsymbol{k}    &     0   & 1   & 2   & 3   \\\\\n\\hline\n\\boldsymbol{\\mathbb{P}(X = k)}   & 1/8   & 3/8  & 3/8 &  1/8\n\\end{array}\\]\nBy the way, notice that the probabilities in the p.m.f. sum up to 1.\n\nThis is not a coincidence! Because the probabilities represent the probabilities of all values \\(X\\) can take, they must sum up to 1."
  },
  {
    "objectID": "Pages/Lectures/Lecture08/Lec08.html#properties-of-pmfs",
    "href": "Pages/Lectures/Lecture08/Lec08.html#properties-of-pmfs",
    "title": "PSTAT 5A: Lecture 08",
    "section": "Properties of PMF’s",
    "text": "Properties of PMF’s\n\nThis leads us to posit the following two properties of probability mass functions:\n\n\n\n\n\n\n\n\n\nProperties of a PMF\n\n\n\n\nThe values in a PMF must sum to 1\nThe values in a PMF must always be nonnegative\n\n\n\n\n\n\n\n\nAlso: we implicitly set probabilities not contained in the p.m.f. to be zero.\n\nFor instance: in our coin tossing example, \\(\\mathbb{P}(X = 1.5) = 0\\).\nThis makes sense! If \\(k \\notin S_X\\), then by definition of the state space it is impossible for \\(X\\) to attain the value \\(k\\), and so \\(\\mathbb{P}(X = k) = 0\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture08/Lec08.html#worked-out-example",
    "href": "Pages/Lectures/Lecture08/Lec08.html#worked-out-example",
    "title": "PSTAT 5A: Lecture 08",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\nWorked-Out Example 2\n\n\n\n\nA random variable \\(X\\) has the following p.m.f.: \\[\\begin{array}{r|cccc}\n  \\boldsymbol{k}    &     -1.4   & 0   & 3   & 4.15   \\\\\n  \\hline\n  \\boldsymbol{\\mathbb{P}(X = k)}   & 0.1 & 0.2 & \\boldsymbol{a} & 0.6\n\\end{array}\\] What must be the value of \\(\\boldsymbol{a}\\)?\n\n\n\n\n\n\nBecause the values in a p.m.f. must sum to 1, we must have \\[ 0.1 + 0.2 + a + 0.6 = 1 \\] which means \\[ a = 1 - (0.1 + 0.2 + 0.6) = \\boxed{0.1} \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture08/Lec08.html#worked-out-example-3",
    "href": "Pages/Lectures/Lecture08/Lec08.html#worked-out-example-3",
    "title": "PSTAT 5A: Lecture 08",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\nWorked-Out Example 3\n\n\n\n\nA random variable \\(X\\) has the following p.m.f.: \\[\\begin{array}{r|cccc}\n  \\boldsymbol{k}    &     -1.4   & 0   & 3   & 4.15   \\\\\n  \\hline\n  \\boldsymbol{\\mathbb{P}(X = k)}   & 0.1 & 0.2 & 0.1 & 0.6\n\\end{array}\\] Compute both \\(\\mathbb{P}(X = 0)\\) and \\(\\mathbb{P}(X \\leq 0)\\).\n\n\n\n\n\n\nFor \\(\\mathbb{P}(X = 0)\\), we can simply read off the corresponding element from the p.m.f.:"
  },
  {
    "objectID": "Pages/Lectures/Lecture08/Lec08.html#expected-value",
    "href": "Pages/Lectures/Lecture08/Lec08.html#expected-value",
    "title": "PSTAT 5A: Lecture 08",
    "section": "Expected Value",
    "text": "Expected Value\n\n\n\n\n\n\n\nDefinition\n\n\n\nThe expected value (or just expectation) of a discrete random variable \\(X\\) is \\[ \\mathbb{E}[X] = \\sum_{\\text{all $k$}} k \\cdot \\mathbb{P}(X = k) \\] where the sum ranges over all values of \\(k\\) in the state space.\n\n\n\n\n\n\nIn words: multiply each value in the state space by the corresponding probability, and then sum.\nThe expected value is a sort of ‘center’ of a random variable."
  },
  {
    "objectID": "Pages/Lectures/Lecture08/Lec08.html#worked-out-example-4",
    "href": "Pages/Lectures/Lecture08/Lec08.html#worked-out-example-4",
    "title": "PSTAT 5A: Lecture 08",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\nWorked-Out Example 4\n\n\n\n\nA random variable \\(X\\) has the following p.m.f.: \\[\\begin{array}{r|cccc}\n  \\boldsymbol{k}    &     -1.4   & 0   & 3   & 4.15   \\\\\n  \\hline\n  \\boldsymbol{\\mathbb{P}(X = k)}   & 0.1 & 0.2 & 0.1 & 0.6\n\\end{array}\\] Compute \\(\\mathbb{E}[X]\\).\n\n\n\n\n\n\nWe compute \\[\\begin{align*}\n\\mathbb{E}[X]   & = \\sum_{\\text{all $k$}} k \\cdot \\mathbb{P}(X = k)   \\\\[5mm]\n  & = (-1.4) \\cdot \\mathbb{P}(X = -1.4) + (0) \\cdot \\mathbb{P}(X = 0) + (3) \\cdot \\mathbb{P}(X = 3)   \\\\\n    & \\hspace{10mm} + (4.15) \\cdot \\mathbb{P}(X = 4.15)    \\\\[5mm]\n    & = (-1.4) \\cdot (0.1) + (0) \\cdot (0.2) + (3) \\cdot (0.1) + (4.15) \\cdot (0.6) = \\boxed{2.65}\n\\end{align*}\\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture08/Lec08.html#variance-and-sd",
    "href": "Pages/Lectures/Lecture08/Lec08.html#variance-and-sd",
    "title": "PSTAT 5A: Lecture 08",
    "section": "Variance and SD",
    "text": "Variance and SD\n\n\n\n\n\n\n\nDefinition\n\n\n\nThe variance of a discrete random variable \\(X\\) is \\[ \\mathrm{Var}(X) = \\sum_{\\text{all $k$}} (k - \\mathbb{E}[X])^2 \\cdot \\mathbb{P}(X = k) \\] where the sum ranges over all values of \\(k\\) in the state space. The standard deviation is the square root of the variance: \\[ \\mathrm{SD}(X) = \\sqrt{\\mathrm{Var}(X)} \\]\n\n\n\n\n\n\n\n\n\n\n\n\nSecond Formula for Variance\n\n\n\n\\[ \\mathrm{Var}(X) = \\left( \\sum_{\\text{all $k$}} k^2 \\cdot \\mathbb{P}(X = k) \\right) - \\left( \\mathbb{E}[X] \\right)^2 \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture08/Lec08.html#worked-out-example-5",
    "href": "Pages/Lectures/Lecture08/Lec08.html#worked-out-example-5",
    "title": "PSTAT 5A: Lecture 08",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\nWorked-Out Example 5\n\n\n\n\nA random variable \\(X\\) has the following p.m.f.: \\[\\begin{array}{r|cccc}\n  \\boldsymbol{k}    &     -1.4   & 0   & 3   & 4.15   \\\\\n  \\hline\n  \\boldsymbol{\\mathbb{P}(X = k)}   & 0.1 & 0.2 & 0.1 & 0.6\n\\end{array}\\] Compute \\(\\mathrm{Var}(X)\\) and \\(\\mathrm{SD}(X)\\).\n\n\n\n\n\n\nWe previously found that \\(\\mathbb{E}[X] = 2.65\\).\nHence, we need only to find \\(\\sum_{k} k^2 \\cdot \\mathbb{P}(X = x)\\): \\[\\begin{align*}\n\\sum_{\\text{all $k$}} k^2 \\cdot \\mathbb{P}(X = k) & = (-1.4)^2 \\cdot \\mathbb{P}(X = -1.4) + (0)^2 \\cdot \\mathbb{P}(X = 0) + (3)^2 \\cdot \\mathbb{P}(X = 3)   \\\\\n    & \\hspace{10mm} + (4.15)^2 \\cdot \\mathbb{P}(X = 4.15)    \\\\[5mm]\n    & = (-1.4)^2 \\cdot (0.1) + (0)^2 \\cdot (0.2) + (3)^2 \\cdot (0.1) + (4.15)^2 \\cdot (0.6) = 11.4295\n\\end{align*}\\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture08/Lec08.html#your-turn",
    "href": "Pages/Lectures/Lecture08/Lec08.html#your-turn",
    "title": "PSTAT 5A: Lecture 08",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\n\n\n\n\nExercise 1\n\n\n\n\nSuppose \\(X\\) is a random variable with p.m.f. (probability mass function) given by \\[\\begin{array}{r|cccc}\n  \\boldsymbol{k}    &     -1 & 0 & 1 & 2   \\\\\n  \\hline\n  \\boldsymbol{\\mathbb{P}(X = k)}   & 0.3 & 0.2 & 0.1 & \\boldsymbol{a}\n\\end{array}\\]\n\nFind the state space \\(S_X\\) of \\(X\\).\nFind the value of \\(\\boldsymbol{a}\\)\nFind \\(\\mathbb{P}(X = 0.5)\\)\nFind \\(\\mathbb{P}(X \\leq 1)\\)\nFind \\(\\mathbb{P}(X > 1)\\)\nFind \\(\\mathbb{E}[X]\\)\nFind \\(\\mathrm{Var}(X)\\) and \\(\\mathrm{SD}(X)\\)"
  },
  {
    "objectID": "Pages/Lectures/Lecture08/Lec08.html#your-turn-1",
    "href": "Pages/Lectures/Lecture08/Lec08.html#your-turn-1",
    "title": "PSTAT 5A: Lecture 08",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\n\n\n\n\nExercise 2\n\n\n\n\nConsider the following game: a fair six-sided die is rolled. If the number showing is 1 or 2, you win a dollar; if the number showing is 3, 4, or 5 you win 2 dollars; if the number showing is 6, you lose 1 dollar. Let \\(W\\) denote your net winnings after playing this game once.\n\nWrite down the state space \\(S_W\\) of \\(W\\).\nFind the p.m.f. of \\(W\\).\nWhat are your expected winnings after one round of the game?"
  },
  {
    "objectID": "Pages/Lectures/Lecture08/Lec08.html#back-to-coins",
    "href": "Pages/Lectures/Lecture08/Lec08.html#back-to-coins",
    "title": "PSTAT 5A: Lecture 08",
    "section": "Back to Coins",
    "text": "Back to Coins\n\nAlright, let’s close out this lecture by returning to our coin tossing example.\nAs a reminder: if we let \\(X\\) denote the number of heads in 3 tosses of a \\(p-\\)coin (i.e. a coin that lands ‘heads’ with probability \\(p\\)), the p.m.f. of \\(X\\) is given by\n\n\n\\[\\begin{array}{r|cccc}\n  \\boldsymbol{k}    &     0   & 1   & 2   & 3   \\\\\n  \\hline\n  \\boldsymbol{\\mathbb{P}(X = k)}   & (1 - p)^3   & 3  p (1 - p)^2  & 3  p^2 (1 - p) &  p^3\n\\end{array}\\]\n\n\nWhat if instead of tossing 3 coins, we had tossed 4? Or 5? Or 10?\nWe could go through the same steps we did before, when deriving the p.m.f. for three tosses, but let’s be a little smarter about this; let’s answer the following more general question:"
  },
  {
    "objectID": "Pages/Lectures/Lecture08/Lec08.html#binomial-distribution",
    "href": "Pages/Lectures/Lecture08/Lec08.html#binomial-distribution",
    "title": "PSTAT 5A: Lecture 08",
    "section": "Binomial Distribution",
    "text": "Binomial Distribution\n\n\n\n\n\n\nThe Binomial Distribution\n\n\n\n\nSuppose the probability of a single trial resulting in a ‘success’ is \\(p\\). Letting \\(X\\) denote the number of successes in \\(n\\) independent trials, then we say that \\(X\\) follows the Binomial Distribution with parameters \\(n\\) and \\(p\\). We use the notation \\(X \\sim \\mathrm{Bin}(n, p)\\) to denote this.\n\n\n\n\n\n\n\n\n\n\n\n\nFacts about the Binomial Distribution\n\n\n\n\nIf \\(X \\sim \\mathrm{Bin}(n, p)\\), then\n\n\\(\\displaystyle \\mathbb{P}(X = k) = \\binom{n}{k} \\cdot p^k \\cdot (1 - p)^{n - k}\\)\n\\(\\mathbb{E}[X] = np\\) and \\(\\mathrm{Var}(X) = np(1 - p)\\)"
  },
  {
    "objectID": "Pages/Lectures/Lecture08/Lec08.html#binomial-conditions",
    "href": "Pages/Lectures/Lecture08/Lec08.html#binomial-conditions",
    "title": "PSTAT 5A: Lecture 08",
    "section": "Binomial Conditions",
    "text": "Binomial Conditions\n\n\n\n\n\n\nFour Conditions to Check\n\n\n\n\nIf \\(X\\) counts the number of successes in \\(n\\) trials, there are four conditions that need to be satisfied in order for \\(X\\) to follow the Binomial Distribution:\n\nThe trials must be independent\nThe number of trials, \\(n\\), must be fixed\nThere should be a well-defined notion of “success” and “failure” on each trial\nThe probability of “success” must remain constant across trials.\n\n\n\n\n\n\n\nSo, remember: \\(X \\sim \\mathrm{Bin}(n, p)\\) just means “\\(X\\) counts the number of successes in \\(n\\) trials, where success occurs with probability \\(p\\) on any given trial, subject to the four conditions above being satisfied."
  },
  {
    "objectID": "Pages/Lectures/Lecture08/Lec08.html#worked-out-example-6",
    "href": "Pages/Lectures/Lecture08/Lec08.html#worked-out-example-6",
    "title": "PSTAT 5A: Lecture 08",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\nWorked-Out Example 6\n\n\n\n\n\nIf we roll a fair \\(6-\\)sided die \\(13\\) times (assume rolls are independent of each other) and let \\(X\\) denote the number of times we observe an even number, is \\(X\\) binomially distributed?\nIn a large population of \\(100\\) students, of which \\(70\\) own Android phones, we draw a random sample of 10 without replacement and let \\(Y\\) denote the number of students in this sample that have Android phones. Is \\(Y\\) binomially distributed?\nConsider the same setup as in part (b) above, except this time suppose students are selected with replacement. Is \\(Y\\) binomially distributed?"
  },
  {
    "objectID": "Pages/Lectures/Lecture08/Lec08.html#part-a",
    "href": "Pages/Lectures/Lecture08/Lec08.html#part-a",
    "title": "PSTAT 5A: Lecture 08",
    "section": "Part (a)",
    "text": "Part (a)\n\nWe check the Binomial Conditions.\n\nIndependent trials? Yup!\nFixed number of trials? Yup! (\\(n = 13\\))\nWell-defined notion of success? Yup! (“success” = “rolling an even number” and “failure” = “rolling an odd number”)\nFixed probability of success? Yup! (\\(p = 1/2\\)).\n\nSince all 4 conditions are satisfied, \\(X\\) binomially distributed: specifically, \\[ X \\sim \\mathrm{Bin}(13, \\ 1/2) \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture08/Lec08.html#part-b",
    "href": "Pages/Lectures/Lecture08/Lec08.html#part-b",
    "title": "PSTAT 5A: Lecture 08",
    "section": "Part (b)",
    "text": "Part (b)\n\nWe check the Binomial Conditions.\n\nIndependent trials? ; because sampling is done without replacement, trials are no longer independent (i.e. the result of our second trial is very much dependent on the result of our first).\n\nSince at least one condition is violated, \\(Y\\) does follow the binomial distribution."
  },
  {
    "objectID": "Pages/Lectures/Lecture08/Lec08.html#part-c",
    "href": "Pages/Lectures/Lecture08/Lec08.html#part-c",
    "title": "PSTAT 5A: Lecture 08",
    "section": "Part (c)",
    "text": "Part (c)\n\nWe check the Binomial Conditions.\n\nIndependent trials? Yup!\nFixed number of trials? Yup! (\\(n = 10\\))\nWell-defined notion of success? Yup! (“success” = “owning an Android phone” and “failure” = “not owning an Android phone”)\nFixed probability of success? Yup! (\\(p = 7/10\\)).\n\nSince all 4 conditions are satisfied, \\(Y\\) binomially distributed: specifically, \\[ Y \\sim \\mathrm{Bin}(10, \\ 7/10) \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture08/Lec08.html#your-turn-2",
    "href": "Pages/Lectures/Lecture08/Lec08.html#your-turn-2",
    "title": "PSTAT 5A: Lecture 08",
    "section": "Your Turn!",
    "text": "Your Turn!\n\n\n\n\n\n\nExercise 3\n\n\n\n\nSuppose Jana toss \\(65\\) different \\(12-\\)sided dice, independently of each other; let \\(Z\\) denote the number of times a multiple of three results.\n\nVerify that \\(Z\\) follows the Binomial Distribution, and identify its parameters.\nWhat is the probability that Jana observes exactly 23 multiples of three?\nWhat is the expected number of multiples of three Jana will observe?\nWhat is the standard deviation of the number of multiples of three Jana will observe?"
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01_v2.html#what-is-data",
    "href": "Pages/Lectures/Lecture01/Lec01_v2.html#what-is-data",
    "title": "PSTAT 5A: Lecture 01",
    "section": "What is Data?",
    "text": "What is Data?"
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01_v2.html#what-is-data-1",
    "href": "Pages/Lectures/Lecture01/Lec01_v2.html#what-is-data-1",
    "title": "PSTAT 5A: Lecture 01",
    "section": "What is Data?",
    "text": "What is Data?\n\nAccording to Merriam-Webster (source), there are three definitions for data:\n\n\nfactual information (such as measurements or statistics) used as a basis for reasoning, discussion, or calculation\ninformation in digital form that can be transmitted or processed\ninformation output by a sensing device or organ that includes both useful and irrelevant or redundant information and must be processed to be meaningful"
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01_v2.html#example-of-data",
    "href": "Pages/Lectures/Lecture01/Lec01_v2.html#example-of-data",
    "title": "PSTAT 5A: Lecture 01",
    "section": "Example of Data",
    "text": "Example of Data\n\nAs a concrete example of a dataset, let’s explore the so-called palmerpenguins dataset.\nCollected by Dr. Kristen Gorman at the Palmer Station in Antarctica, this dataset contains various measurements of 344 different penguins Dr. Gorman encountered."
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01_v2.html#the-data-matrix",
    "href": "Pages/Lectures/Lecture01/Lec01_v2.html#the-data-matrix",
    "title": "PSTAT 5A: Lecture 01",
    "section": "The Data Matrix",
    "text": "The Data Matrix\n\nEach row of the data matrix above corresponds to an individual penguin.\n\nIn general, we refer to a given row of the data matrix as an observational unit, or case.\n\nFor each penguin, we can see that there are observations on several different characteristics; specifically, for each penguin she encountered, Dr. Gorman measured and recorded the penguin’s species, island, bill length (in mm), bill depth (in mm), flipper length (in mm), body mass (in grams), sex, and year of observation.\n\nNotice that these are the column names in our data matrix above. In general, the columns of the data matrix are referred to as variables."
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01_v2.html#numerical-vs.-categorical",
    "href": "Pages/Lectures/Lecture01/Lec01_v2.html#numerical-vs.-categorical",
    "title": "PSTAT 5A: Lecture 01",
    "section": "Numerical vs. Categorical",
    "text": "Numerical vs. Categorical\n\nNumerical variables are variables whose observations consist of numbers.\n\nExamples: heights, temperatures, number of free throws, etc.\n\nNot all variables are numerical. For example, I could take a poll asking people’s opinions on the movie Avatar: The Way of Water- the observations of this variable will most certainly not be numerical.\n\nRather, the observations of this variable will fall into one of a series of fixed categories (e.g. “Enjoyed the movie”, “Neutral about the movie”, and “Hated the movie”).\nAs such, we describe non-numerical variables as categorical variables."
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01_v2.html#a-note-on-language",
    "href": "Pages/Lectures/Lecture01/Lec01_v2.html#a-note-on-language",
    "title": "PSTAT 5A: Lecture 01",
    "section": "A Note on Language",
    "text": "A Note on Language\n\nQuestion: can we say that data is numerical? Or, can we say we have “categorical data”?\nSure- if our data consists of just a single variable!\nThat is to say- the classification terms we learned (and will learn) can be used to describe data, provided our data contains only one variable.\nThe definition of data we are using (i.e. in the context of the data matrix) is that data is comprised of several variables. As such, we cannot simply take the classification of variables and apply that to the entire dataset (unless our dataset consists of only one variable).\n\nThis may seem like a subtle point… and it is! I’m just pointing it out so you are aware of it."
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01_v2.html#continuous-vs.-discrete-variables",
    "href": "Pages/Lectures/Lecture01/Lec01_v2.html#continuous-vs.-discrete-variables",
    "title": "PSTAT 5A: Lecture 01",
    "section": "Continuous vs. Discrete Variables",
    "text": "Continuous vs. Discrete Variables\n\nThere is a way we can further subdivide numerical variables.\nAs an example, let us consider two different variables, both of which are numerical: heights, and number of accidents on a stretch of highway.\n\nIt is perfectly conceivable to observe a height of 5.15 feet, or 5.1302 feet, or 5.02391829 feet. In other words, there are an infinite number of possible heights between, say, 5 feet and 6 feet.\nOn the other hand, it doesn’t make sense to talk about “1.5 accidents” occurring on a stretch of highway; the number of accidents needs to be an integer."
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01_v2.html#ordinal-vs.-nominal-variables",
    "href": "Pages/Lectures/Lecture01/Lec01_v2.html#ordinal-vs.-nominal-variables",
    "title": "PSTAT 5A: Lecture 01",
    "section": "Ordinal vs. Nominal Variables",
    "text": "Ordinal vs. Nominal Variables\n\nJust as there was a way to subdivide numerical variables, there is a way to further subdivide categorical variables as well.\nAs an example, consider the following two categorical variables: color, and letter grades (i.e. A, B+, etc.)\n\nFirstly, I hope you can see that both of these variables are indeed categorical: there are only a fixed set of values that “color” and “letter grade” can take, with nothing in between.\nNow, clearly letter grades can be ordered: that is, an A is better than a B, which is better than a C, and so on and so forth.\nIn contrast, “green” isnt inherently better than “red”, which isn’t inherently better than “grey”, and so on and so forth."
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01_v2.html#full-classification-scheme",
    "href": "Pages/Lectures/Lecture01/Lec01_v2.html#full-classification-scheme",
    "title": "PSTAT 5A: Lecture 01",
    "section": "Full Classification Scheme",
    "text": "Full Classification Scheme\n\n\nHere is a diagram of the full classification scheme:\n\n\n\n\n\n\n\n\n\ndata_classification\n\n \n\ncluster_main\n\n  \n\ncluster_0\n\n  \n\ncluster_1\n\n  \n\ncluster_2\n\n  \n\ncluster_3\n\n   \n\nData\n\n Variable   \n\nnumerical\n\n Numerical   \n\nData->numerical\n\n    \n\ncategorical\n\n Categorical   \n\nData->categorical\n\n    \n\ncontinuous\n\n Continuous   \n\nnumerical->continuous\n\n    \n\ndiscrete\n\n Discrete   \n\nnumerical->discrete\n\n    \n\nnominal\n\n Nominal   \n\ncategorical->nominal\n\n    \n\nordinal\n\n Ordinal   \n\ncategorical->ordinal"
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01_v2.html#time-for-an-exercise",
    "href": "Pages/Lectures/Lecture01/Lec01_v2.html#time-for-an-exercise",
    "title": "PSTAT 5A: Lecture 01",
    "section": "Time for an Exercise!",
    "text": "Time for an Exercise!\n\n\n\n\n\n\nExercise 1\n\n\n\nClassify each of the following variables as either discrete, continuous, ordinal, or nominal.\n\n\nThe number of times a computer program returns an error\nThe time it takes an experienced swimmer to complete 4 laps of a pool\nThe favorite flavor of donut of a randomly selected person\nThe months of the year, as written in MM format (e.g. “01” for “January”, “02” for “February”, etc.)\n\n\nDiscuss with your neighbors!"
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01_v2.html#important-note",
    "href": "Pages/Lectures/Lecture01/Lec01_v2.html#important-note",
    "title": "PSTAT 5A: Lecture 01",
    "section": "Important Note",
    "text": "Important Note\n\nIt is important to note that categorical data can be encoded using numbers (as we saw in the previous slide).\n\nIndeed, this is a fairly common practice as computers are more adept at dealing with numbers than things like words or symbols.\nAs such, when classifying data, it is not always enough to just check whether the data consists of numbers or not- it is important to think critically about what the data itself represents.\nAs a quick rule-of-thumb: check whether adding two numbers in your dataset makes interpretive sense. 12in \\(+\\) 13in is 15in, whereas blue + gold does not equal anything, regardless of whether blue is being encoded as 0 and gold is being encoded as 1."
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01_v2.html#real-world-data-set",
    "href": "Pages/Lectures/Lecture01/Lec01_v2.html#real-world-data-set",
    "title": "PSTAT 5A: Lecture 01",
    "section": "Real-World Data Set",
    "text": "Real-World Data Set\n\nLet’s return to the palmerpenguins dataset.\nSpecifically, let’s examine the species variable:\n\n\n\n\n  [1] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n  [8] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [15] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [22] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [29] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [36] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [43] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [50] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [57] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [64] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [71] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [78] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [85] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [92] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [99] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[106] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[113] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[120] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[127] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[134] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[141] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[148] Adelie    Adelie    Adelie    Adelie    Adelie    Gentoo    Gentoo   \n[155] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[162] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[169] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[176] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[183] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[190] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[197] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[204] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[211] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[218] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[225] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[232] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[239] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[246] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[253] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[260] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[267] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[274] Gentoo    Gentoo    Gentoo    Chinstrap Chinstrap Chinstrap Chinstrap\n[281] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[288] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[295] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[302] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[309] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[316] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[323] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[330] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[337] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[344] Chinstrap\nLevels: Adelie Chinstrap Gentoo"
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01_v2.html#descriptive-statistics",
    "href": "Pages/Lectures/Lecture01/Lec01_v2.html#descriptive-statistics",
    "title": "PSTAT 5A: Lecture 01",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\n\n\nThis is the goal of Descriptive Statistics- to find different summarizing techniques to desribe the data.\n\n\n\nThere are two ways we can seek to summarize data: numerically (using numbers), and graphically.\nLet’s start with the latter- that is, let’s discuss how we might summarize our data using graphs."
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01_v2.html#back-to-penguins",
    "href": "Pages/Lectures/Lecture01/Lec01_v2.html#back-to-penguins",
    "title": "PSTAT 5A: Lecture 01",
    "section": "Back To Penguins",
    "text": "Back To Penguins\n\nHere is the species variable one more time:\n\n\n\n\n  [1] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n  [8] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [15] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [22] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [29] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [36] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [43] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [50] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [57] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [64] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [71] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [78] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [85] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [92] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n [99] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[106] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[113] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[120] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[127] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[134] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[141] Adelie    Adelie    Adelie    Adelie    Adelie    Adelie    Adelie   \n[148] Adelie    Adelie    Adelie    Adelie    Adelie    Gentoo    Gentoo   \n[155] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[162] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[169] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[176] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[183] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[190] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[197] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[204] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[211] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[218] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[225] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[232] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[239] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[246] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[253] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[260] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[267] Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo    Gentoo   \n[274] Gentoo    Gentoo    Gentoo    Chinstrap Chinstrap Chinstrap Chinstrap\n[281] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[288] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[295] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[302] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[309] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[316] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[323] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[330] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[337] Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap Chinstrap\n[344] Chinstrap\nLevels: Adelie Chinstrap Gentoo"
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01_v2.html#bargraphsbarplots",
    "href": "Pages/Lectures/Lecture01/Lec01_v2.html#bargraphsbarplots",
    "title": "PSTAT 5A: Lecture 01",
    "section": "Bargraphs/Barplots",
    "text": "Bargraphs/Barplots\n\n\nThis is an example of what is known as a bargraph or barplot.\n\n\n\n\n\n\n\n\nResult\n\n\n\nA bargraph is the best type of visualization for categorical data.\n\n\n\n\n\nIn general, if you have \\(k\\) categories, then you will have \\(k\\) bars in your bargraph, each with height propotional to the number of observations within the corresponding category.\nAs you can see, computing software is very useful when it comes to data visualization! In a few weeks, you will explore how to generate visualizations of your own in Python during Lab."
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01_v2.html#time-for-another-exercise",
    "href": "Pages/Lectures/Lecture01/Lec01_v2.html#time-for-another-exercise",
    "title": "PSTAT 5A: Lecture 01",
    "section": "Time For Another Exercise!",
    "text": "Time For Another Exercise!\n\n\n\n\n\n\nExercise 2\n\n\n\nA recent survey asked 120 different PSTAT students what their favorite color is. The bargraph of the results is displayed below:\n\n\n\n\n\nApproximately what proportion of the students in the sample reported either blue or gold as their favorite color? Discuss with your neighbor!"
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01_v2.html#leadup",
    "href": "Pages/Lectures/Lecture01/Lec01_v2.html#leadup",
    "title": "PSTAT 5A: Lecture 01",
    "section": "Leadup",
    "text": "Leadup\n\n\nAll of our discussions above were related to categorical variables.\n\n\n\nAs we discussed at the beginning of this lecture, not all variables are categorical- how do we visualize numerical variables?\nAgain, I find it useful to consider a concrete example: this time, let’s use the bill_length_mm variable from the palmerpenguins dataset."
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01_v2.html#discretizationbinning",
    "href": "Pages/Lectures/Lecture01/Lec01_v2.html#discretizationbinning",
    "title": "PSTAT 5A: Lecture 01",
    "section": "Discretization/Binning",
    "text": "Discretization/Binning\n\n\nThis is what is known as discretizing or binning our variable.\n\n\n\nIn other words, when we discretize our data, we carve it up into a bunch of chunks of equal width and see how many observations fall in each chunk.\n\nThe width of each chunk is what we call the binwidth. For example, if my categories are “between 30 and 35”, “between 35 and 40”, etc., then the binwidth is 5mm as each category spans a width of 5mm."
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01_v2.html#the-importance-of-binwidth",
    "href": "Pages/Lectures/Lecture01/Lec01_v2.html#the-importance-of-binwidth",
    "title": "PSTAT 5A: Lecture 01",
    "section": "The Importance of Binwidth",
    "text": "The Importance of Binwidth\n\nNotice that our notion of a histogram is intimately tied with our choice of binwidth.\nDifferent binwidths can produce wildly different histograms!\nHere is a demo\nIn practice, it is a good idea to play around with different binwidths to find one that results in a histogram that displays a moderate amount of detail without becoming so detailed as to lose sight of the bigger picture."
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01_v2.html#boxplots",
    "href": "Pages/Lectures/Lecture01/Lec01_v2.html#boxplots",
    "title": "PSTAT 5A: Lecture 01",
    "section": "Boxplots",
    "text": "Boxplots\n\nIt turns out there is another way to summarize numerical data visually: using what is known as a boxplot.\nBoxplots can be a seem a bit peculiar at first, so let’s take a look at one together. Before diving back into the palmerpenguins dataset, let’s look at a slightly different dataset.\n\nThis dataset contains only one variable, which records the scores (out of 100 points) of 140 different students on a final exam.\n\n\n\n\n\n  [1] 88.236 77.348 81.050 74.431 75.083 79.569 74.998 80.099 74.264 83.850\n [11] 89.857 81.427 79.439 84.260 78.565 77.570 78.224 73.780 88.085 79.341\n [21] 80.554 77.317 81.155 83.842 87.051 78.362 81.528 72.148 74.131 78.927\n [31] 75.446 79.791 78.199 90.769 85.640 78.420 83.484 79.045 97.909 86.736\n [41] 73.723 76.973 81.320 79.238 85.803 86.621 85.781 81.844 82.896 80.478\n [51] 75.903 84.565 76.302 83.432 85.448 69.695 81.049 85.575 84.791 82.525\n [61] 78.361 77.803 86.542 84.171 86.103 72.772 78.730 76.189 75.187 79.194\n [71] 77.159 82.048 82.661 84.021 76.008 79.474 79.015 86.992 72.524 76.094\n [81] 78.765 80.623 82.497 75.776 70.614 79.677 81.182 77.943 76.863 85.561\n [91] 89.569 96.695 73.680 77.770 81.584 81.965 78.373 76.295 73.212 79.229\n[101] 87.273 87.364 82.706 83.843 75.864 82.791 82.637 78.685 72.626 69.302\n[111] 93.408 73.189 83.764 77.832 82.803 80.278 94.962 79.616 85.667 82.710\n[121] 86.823 76.656 74.623 71.508 91.131 78.318 81.058 86.239 76.585 85.652\n[131] 77.122 86.036 83.127 83.234 80.746 83.878 75.544 73.780 81.106 85.523"
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01_v2.html#anatomy-of-a-boxplot",
    "href": "Pages/Lectures/Lecture01/Lec01_v2.html#anatomy-of-a-boxplot",
    "title": "PSTAT 5A: Lecture 01",
    "section": "Anatomy of a Boxplot",
    "text": "Anatomy of a Boxplot"
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01_v2.html#understanding-boxplots",
    "href": "Pages/Lectures/Lecture01/Lec01_v2.html#understanding-boxplots",
    "title": "PSTAT 5A: Lecture 01",
    "section": "Understanding Boxplots",
    "text": "Understanding Boxplots\n\nLet’s discuss each of the quantities represented on the boxplot separately.\nBefore we do, there’s a bit of math we need to cover.\nThe first quantity we will define is a term you may have heard before- percentile.\nLet’s return to our histogram of scores (since we’re a bit more comfortable with reading histograms than boxplots, at this point)"
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01_v2.html#percentiles",
    "href": "Pages/Lectures/Lecture01/Lec01_v2.html#percentiles",
    "title": "PSTAT 5A: Lecture 01",
    "section": "Percentiles",
    "text": "Percentiles\n\n\n\n\n\n\n\nDefinition\n\n\n\nThe pth percentile of a set of observations \\(X\\) is the value \\(\\pi_{x, \\ p}\\) such that p% of observations lie to the left of (i.e. are less than) \\(\\pi_{x, \\ p}\\).\n\n\n\n\n\n\nMaybe now you can see why I switched over to this data of scores- I think percentiles are sometimes easier to interpret in the context of exam scores, since they are very commonly reported with standardized testing scores (e.g. SAT, GRE, etc.)\n\nIn the context of scores: someone who scored at the pth percentile performed better than p% of all test-takers."
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01_v2.html#quartiles",
    "href": "Pages/Lectures/Lecture01/Lec01_v2.html#quartiles",
    "title": "PSTAT 5A: Lecture 01",
    "section": "Quartiles",
    "text": "Quartiles\n\nWe give a special name to the 25th and 75th percentiles of a set of observations- we call these the first quartile and third quartile, respectively, and use the notation \\(Q_1\\) and \\(Q_3\\) to denote them, respectively.\n\nSo, \\(Q_1\\) is the value such that 25% of observations are less than \\(Q_1\\), and \\(Q_3\\) is the value such that 75% of observations are less than \\(Q_3\\)\n\nThe second quartile (i.e. the 50th percentile) is called the median.\n\nAs such, the median is the value that “splits the data in half”.\nWe’ll talk more about the median in the next lecture."
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01_v2.html#small-caveat",
    "href": "Pages/Lectures/Lecture01/Lec01_v2.html#small-caveat",
    "title": "PSTAT 5A: Lecture 01",
    "section": "Small Caveat",
    "text": "Small Caveat\n\nI should quickly mention one small caveat- computing softwares often use a different procedure for computing quartiles.\nThis procedure is quite long and complicated, and is based off an entire paper written back in the 90’s.\nFor example, if we consider the set \\(S = \\{1, 2, 3, 4, 5, 6\\}\\), we would (based on the definition from the previous slide) call the first quartile \\(2\\), whereas most softwares would return a value of \\(2.25\\).\n\nI’ll show you on the chalkboard why the first quartile is \\(2\\).\n\nDon’t worry about why this is- whenever we talk about quartiles in this class, you can just think of the definition I posed on the previous slide."
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01_v2.html#whiskers",
    "href": "Pages/Lectures/Lecture01/Lec01_v2.html#whiskers",
    "title": "PSTAT 5A: Lecture 01",
    "section": "Whiskers",
    "text": "Whiskers\n\nFinally, we discuss the role of the whiskers on the boxplot.\nThere are several different conventions for how far the whiskers extend. In some conventions, the whiskers extend to the minimum and maximum values of the data.\nThe convention we will use is the following: the whiskers will never reach farther than \\(\\boldsymbol{1.5 \\times (Q_3 - Q_1)}\\).\n\nWhat this means is that there may be points in our dataset that lie beyond the reach of the whiskers. These points are what we call outliers."
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01_v2.html#whiskers-1",
    "href": "Pages/Lectures/Lecture01/Lec01_v2.html#whiskers-1",
    "title": "PSTAT 5A: Lecture 01",
    "section": "Whiskers",
    "text": "Whiskers\n\n\nThe rationale for constructing the whiskers in this way is to try and highlight any points that are unusually distant from the rest of the data.\n\n\n\nFor example, returning to our dataset of scores, we can see that though the median score was around 80.3% there was one person who scored a 97.9%. Because this score is unusually large, we would label it an outlier."
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01_v2.html#time-for-an-exercise-1",
    "href": "Pages/Lectures/Lecture01/Lec01_v2.html#time-for-an-exercise-1",
    "title": "PSTAT 5A: Lecture 01",
    "section": "Time for an Exercise!",
    "text": "Time for an Exercise!\n\n\n\n\n\n\nExercise 4\n\n\n\nHere is a boxplot of the bill_length_mm variable from the palmerpenguins dataset:\n\n\n\n\n\n\n\n\nWhat is the median bill length?\nApproximately what percent of penguins had bills shorter than 37mm in length?\nAre there any outliers?"
  },
  {
    "objectID": "Pages/Lectures/Lecture01/Lec01_v2.html#summary",
    "href": "Pages/Lectures/Lecture01/Lec01_v2.html#summary",
    "title": "PSTAT 5A: Lecture 01",
    "section": "Summary",
    "text": "Summary\n\nWe started off by talking about the structure of data, and the data matrix.\nWe then discussed how to classify variables.\nNext, we explored graphical methods for summarizing data.\n\nBargraphs are best-suited for categorical data\nHistograms and boxplots are best-suited for numerical data\n\nWe also introduced the notions of percentiles, the median, and outliers.\nNext time we’ll discuss how to visualize the relationship between two variables.\nWe’ll also discuss some numerical summaries for data, including the mean, median, standard deviation, and IQR."
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#hypothesis-testing-1",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#hypothesis-testing-1",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\n\nRecall that in the framework of hypothesis testing, we wish to utilize data to assess the plausibility/validity of a hypothesis, called the null hypothesis.\n\nSpecifically, we wish to determine whether or not the data provides support to reject the null in favor of the alternative hypothesis or not.\n\nIn the case of hypothesis testing for a population proportion \\(p\\), our null takes the form \\(H_0: p = p_0\\) and there are several different alternative hypotheses we could consider:\n\nTwo-tailed alternative/test: \\(H_A: \\ p \\neq p_0\\)\nLower-tailed alternative/test: \\(H_A: \\ p < p_0\\)\nUpper-tailed alternative/test: \\(H_A: \\ p > p_0\\)\nSimple-vs-simple alternative/test: \\(H_A: \\ p = p_1\\) for \\(p_1 \\neq p_0\\)"
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#hypothesis-testing-for-a-proportion",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#hypothesis-testing-for-a-proportion",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Hypothesis Testing for a Proportion",
    "text": "Hypothesis Testing for a Proportion\n\nThe different alternative hypotheses lead to different forms of our hypothesis test.\nLast lecture, we discussed how to construct a two-sided hypothesis test.\nOn the homework, I ask you to derive the lower-tailed hypothesis test, and I also provide you with the form for an upper-tailed hypothesis test.\nTo help you with Problem 1 on the upcoming homework, allow me to re-do the derivations we did last lecture, but this time more in the style of Problem 1 on HW01."
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#two-sided-test-for-a-proportion",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#two-sided-test-for-a-proportion",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Two-Sided Test for a Proportion",
    "text": "Two-Sided Test for a Proportion\n\nWe start with the test statistic \\[ \\mathrm{TS} = \\frac{\\widehat{P} - p_0}{\\sqrt{\\frac{p_0 (1 - p_0)}{n}}} \\]\nIf the null were true, i.e. if the true value of \\(p\\) were \\(p_0\\), then this test statistic would follow the Normal Distribution provided we are able to invoke the Central Limit Theorem for proportions, which we can do only if\n\n\\(np_0 \\geq 10\\)\n\\(n(1 - p_0) \\geq 10\\)"
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#two-sided-test-for-a-proportion-1",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#two-sided-test-for-a-proportion-1",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Two-Sided Test for a Proportion",
    "text": "Two-Sided Test for a Proportion\n\nIf these conditions hold, we then know that (by the Central Limit Theorem for Proportions) \\[ \\widehat{P} \\stackrel{H_0}{\\sim} \\mathcal{N}\\left( p_0, \\ \\sqrt{\\frac{p_0 (1 - p_0)}{n}} \\right) \\] which means, by our Standardization Result, that \\[\\mathrm{TS} \\stackrel{H_0}{\\sim} \\mathcal{N}(0, \\ 1)\\]"
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#two-sided-test-for-a-proportion-2",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#two-sided-test-for-a-proportion-2",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Two-Sided Test for a Proportion",
    "text": "Two-Sided Test for a Proportion\n\nNow, let’s think about when we would reject the null that \\(p = p_0\\) in favor of the alternative that \\(p \\neq p_0\\).\n\nIf we observed a value of \\(\\widehat{p}\\) that was much greater than \\(p_0\\) (i.e. if \\(\\mathrm{TS}\\) was much larger than 0), we would probably reject the null in favor of the alternative.\nBut, if we observed a value of \\(\\widehat{p}\\) that was much smaller than \\(p_0\\) (i.e. if \\(\\mathrm{TS}\\) was much smaller than 0), we would probably also reject the null in favor of the alternative.\nTherefore, we reject when the magnitude of \\(\\mathrm{TS}\\) is large; i.e. with \\(|\\mathrm{TS}| > c\\) for some constant \\(c\\).\n\nThis means our test should take the form \\[ \\texttt{decision}(\\mathrm{TS}) = \\begin{cases} \\texttt{reject } H_0 & \\text{if } |\\mathrm{TS}| > c \\\\ \\texttt{fail to reject } H_0 & \\text{otherwise}\\\\ \\end{cases}  \\]"
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#two-sided-test-for-a-proportion-3",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#two-sided-test-for-a-proportion-3",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Two-Sided Test for a Proportion",
    "text": "Two-Sided Test for a Proportion\n\nFinally, we need to consider how large to make our cutoff (or critical value) \\(c\\).\nThis is where we need to consider the two types of errors we could commit: Type I and Type II Errors\n\nType I error: the null was rejected when it was in fact true (convicting an innocent person)\nType II error: the null was not rejected when it was in fact false (letting a guilty person go free)\n\nWe call the probability of committing a Type I error the level of significance \\(\\alpha\\), which we fix before beginning our testing procedure.\nIn terms of the critical value, this means \\(c\\) should satisfy the equation \\[ \\mathbb{P}_{H_0}(|\\mathrm{TS}| > c) = \\alpha \\] where \\(\\mathbb{P}_{H_0}\\) means “probability of, assuming the null is actually true (i.e. that \\(p = p_0\\))”."
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#two-sided-test-for-a-proportion-4",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#two-sided-test-for-a-proportion-4",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Two-Sided Test for a Proportion",
    "text": "Two-Sided Test for a Proportion\n\nSince we know that \\(\\mathrm{TS}\\) follows a standard normal distribution under the null, this means that \\(c\\) should be the \\((\\alpha / 2) \\times 100\\)th percentile of the standard normal distribution.\n\nFor example, if \\(\\alpha = 0.05\\), then the critical value is the \\((0.05) / 2 \\times 100 = 2.5\\)th percentile of the standard normal distribution, which we see to be \\(1.96\\).\n\nAs such, the final form of our test is \\[ \\texttt{decision}(\\mathrm{TS}) = \\begin{cases} \\texttt{reject } H_0 & \\text{if } |\\mathrm{TS}| > z_{1 - \\alpha/2} \\\\ \\texttt{fail to reject } H_0 & \\text{otherwise}\\\\ \\end{cases}  \\] where we can use the notation \\(z_{1 - \\alpha/2}\\) to denote the \\((\\alpha / 2) \\times 100\\)th percentile of the standard normal distribution, scaled by negative 1."
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#quick-note",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#quick-note",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Quick Note",
    "text": "Quick Note\n\nIt turns out that the \\((\\alpha / 2) \\times 100\\)th percentile of the standard normal distribution scaled by negative 1 is equivalent to the \\((1 - \\alpha / 2)\\)th percentile of the standard normal distribution, not scaled by negative 1.\n\n\n\nimport scipy.stats as sps\n\n-sps.norm.ppf(0.05/2)\n\n1.9599639845400545\n\n\n\n\n\nimport scipy.stats as sps\n\nsps.norm.ppf(1 - 0.05/2)\n\n1.959963984540054\n\n\n\n\nBasically, when doing a two-sided test, ensure that the critical value is positive."
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#worked-out-example",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#worked-out-example",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\n\nWorked-Out Example 1\n\n\n\n\nAdministration within a Statistics department at an unnamed university claims to admit 24% of all applicants. A disgruntled student, dubious of the administration’s claims, takes a representative sample of 120 students who applied to the Statistics major, and found that 20% of these students were actually admitted into the major.\n\nConduct a two-sided hypothesis test at a 5% level of significance on the administrator’s claims that 24% of applicants into the Statistics major are admitted. Be sure you phrase your conclusion clearly, and in the context of the problem."
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#solutions",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#solutions",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Solutions",
    "text": "Solutions\n\nWe first phrase the hypotheses.\nLet \\(p\\) denote the true proportion of applicants who get admitted into the major. Since we are performing a two-sided test, our hypotheses take the form \\[ \\left[ \\begin{array}{rr}\nH_0:    p = 0.24    \\\\\nH_A:    p \\neq 0.24\n\\end{array} \\right.\\]\nNow we compute the value of the test statistic: \\[ \\mathrm{TS} = \\frac{\\widehat{p} - p_0}{\\sqrt{\\frac{p_0(1 - p_0)}{n}}} = \\frac{0.2 - 0.24}{\\sqrt{\\frac{(0.24) \\cdot (1 - 0.24)}{120}}} \\approx -1.026 \\]\nNext, we compute the critical value. Since we are using an \\(\\alpha = 0.05\\) level of significance, we will use the critical value \\(1.96\\)"
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#solutions-1",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#solutions-1",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Solutions",
    "text": "Solutions\n\nFinally, we perform the test: we will reject the null in favor of the alternative if \\(|\\mathrm{TS}|\\) is larger than the critical value.\nIn this case, \\(|\\mathrm{TS}| = |-1.026| = 1.026 < 1.96\\) meaning we fail to reject the null:\n\n\n\nAt an \\(\\alpha = 0.05\\) level of significance, there was insufficient evidence to reject the null hypothesis that 24% of applicants are admitted into the major as opposed to the alternative that the true admittance rate was not 24%."
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#basics-of-random-variables",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#basics-of-random-variables",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Basics of Random Variables",
    "text": "Basics of Random Variables\n\nRecall long ago we discussed the notion of an experiment: any procedure we can repeat an infinite number of times where each time we repeat the experiment the same fixed set of things (i.e. the outcomes) can occur.\nA random variable, loosely speaking, is some sort of numerical variable that keeps track of certain quantities relating to an experiment.\nFor example, if we toss 7 coins and let \\(X\\) denote the number of heads we observe in these 7 coin tosses, then \\(X\\) would be a random variable.\nThe set of all values a random variable can attain is called the state space, and is denoted \\(S_X\\).\nWe classify random variables based on their state space:\n\nIf \\(S_X\\) has jumps, we say \\(X\\) is a discrete random variable\nIf \\(S_X\\) does not have jumps, we say \\(X\\) is a continuous random variable"
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#discrete-random-variables",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#discrete-random-variables",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Discrete Random Variables",
    "text": "Discrete Random Variables\n\nDiscrete random variables are described/summarized by a probability mass function (p.m.f.), which is a specification of the values the random variable can take (i.e. the state space) along with the probabilities with which the random variable attains those values.\n\nP.M.F.’s are often displayed in tabular form: e.g. \\[ \\begin{array}{r|cccc}\n\\boldsymbol{k}              & -1    & 0   & 1   & 2   \\\\\n\\hline\n\\boldsymbol{\\mathbb{P}(X = k)}   & 0.1   & 0.2   & 0.3   & 0.4\n  \\end{array}\\]\nNote that the probability values in a P.M.F. must sum to 1.\n\nQuantities like \\(\\mathbb{P}(X \\leq k)\\) are found by summing up the values of \\(\\mathbb{P}(X = x)\\) for all values of \\(x\\) in the state space that are less than \\(k\\).\n\nFor example, in the example above, \\[\\mathbb{P}(X \\leq 0.5) = \\mathbb{P}(X = -1) + \\mathbb{P}(X = 0)  = 0.1 + 0.2 = 0.3 \\]"
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#expected-value",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#expected-value",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Expected Value",
    "text": "Expected Value\n\nThe expected value of a random variable \\(X\\), denoted \\(\\mathbb{E}[X]\\), represents a sort of “average” of \\(X\\), and is computed as \\[ \\mathbb{E}[X] = \\sum_{\\text{all $k$}} k \\cdot \\mathbb{P}(X = k) \\]\n\nAgain, don’t be scared by the sigma notation! It just represents a sum.\nSo, for example, using our P.M.F. from the previous slide, \\[\\begin{align*}\n\\mathbb{E}[X]   & = (-1) \\cdot \\mathbb{P}(X = -1) + (0) \\cdot \\mathbb{P}(X = 0)    \\\\\n& \\hspace{10mm} + (1) \\cdot \\mathbb{P}(X = 1) + (2) \\cdot \\mathbb{P}(X = 2)    \\\\[3mm]\n  & = (-1) \\cdot (0.1) + (0) \\cdot (0.2) + (1) \\cdot (0.3) + (2) \\cdot (0.4)     \\\\[3mm]\n  & = \\boxed{1}\n  \\end{align*}\\]"
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#variance-and-standard-deviation",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#variance-and-standard-deviation",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Variance and Standard Deviation",
    "text": "Variance and Standard Deviation\n\nThere are two formulas we can use for the variance of a random variable \\(X\\): \\[ \\mathrm{Var}(X) = \\sum_{\\text{all $k$}} (k - \\mathbb{E}[X])^2 \\cdot \\mathbb{P}(X = k) \\] or \\[ \\mathrm{Var}(X) = \\left(\\sum_{\\text{all $k$}} k^2 \\cdot \\mathbb{P}(X = k) \\right) - (\\mathbb{E}[X])^2 \\]\nThe standard deviation of a random variable is simply the square root of the variance: \\[ \\mathrm{SD}(X) = \\sqrt{\\mathrm{Var}(X)} \\]"
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#variance-and-standard-deviation-1",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#variance-and-standard-deviation-1",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Variance and Standard Deviation",
    "text": "Variance and Standard Deviation\n\nFor example, using the PMF from a few slides ago, the first formula for variance tells us to compute \\[\\begin{align*}\n\\mathrm{Var}(X)     & = \\sum_{\\text{all $k$}} (k - \\mathbb{E}[X])^2 \\cdot \\mathbb{P}(X = k)    \\\\\n  & = (-1 - 1)^2 \\cdot \\mathbb{P}(X = -1) + (0 - 1)^2 \\cdot \\mathbb{P}(X = 0)    \\\\\n    & \\hspace{10mm} + (1 - 1)^2 \\cdot \\mathbb{P}(X = 1) + (2 - 1)^2 \\cdot \\mathbb{P}(X = 2)    \\\\[3mm]\n      & = (-1 - 1)^2 \\cdot (0.1) + (0 - 1)^2 \\cdot (0.2)    \\\\\n      & \\hspace{10mm}+ (1 - 1)^2 \\cdot (0.3) + (2 - 1)^2 \\cdot (0.4)   \\\\[3mm]\n      & = \\boxed{1}\n\\end{align*}\\]"
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#variance-and-standard-deviation-2",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#variance-and-standard-deviation-2",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Variance and Standard Deviation",
    "text": "Variance and Standard Deviation\n\n\nUsing the second formula for variance, we first compute \\[\\begin{align*}\n\\sum_{\\text{all $k$}} k^2 \\mathbb{P}(X = k)    &  = (-1)^2 \\cdot \\mathbb{P}(X = -1) + (0)^2 \\cdot \\mathbb{P}(X = 0)    \\\\\n    & \\hspace{10mm} + (1)^2 \\cdot \\mathbb{P}(X = 1) + (2 - 1)^2 \\cdot \\mathbb{P}(X = 2)    \\\\[3mm]\n      & = (-1)^2 \\cdot (0.1) + (0)^2 \\cdot (0.2)    \\\\\n      & \\hspace{10mm}+ (1)^2 \\cdot (0.3) + (2)^2 \\cdot (0.4)   \\\\[3mm]\n      & = 2\n\\end{align*}\\] which means \\[ \\mathrm{Var}(X) = 2 - (1)^2 = \\boxed{1}\\]"
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#binomial-distribution",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#binomial-distribution",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Binomial Distribution",
    "text": "Binomial Distribution\n\nSuppose we have \\(n\\) independent trials, each resulting in “success” with probability \\(p\\) and “failure” with probability \\(1 - p\\). If \\(X\\) denotes the number of successes in these \\(n\\) trials, we say \\(X\\) follow the Binomial distribution with parameters \\(n\\) and \\(p\\), notated \\[ X \\sim \\mathrm{Bin}(n, \\ p) \\]\nIf \\(X \\sim \\mathrm{Bin}(n, \\ p)\\), then:\n\n\\(S_X = \\{0, 1, 2, \\cdots, n\\}\\)\n\\(\\mathbb{P}(X = k) = \\binom{n}{k} p^k (1 - p)^{n - k}\\)\n\\(\\mathbb{E}[X] = np\\)\n\\(\\mathrm{Var}(X) = np(1 - p)\\)"
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#binomial-distribution-1",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#binomial-distribution-1",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Binomial Distribution",
    "text": "Binomial Distribution\n\nIn order to verify that the Binomial distribution is appropriate to use, we need to check the Binomial Criteria:\n\nIndependence across trials\nFixed number \\(n\\) of trials\nWell-defined notion of “success” and “failure”\nFixed probability \\(p\\) of success across trials.\n\nIf you are going to use the Binomial distribution in a problem, you must check all four of these!"
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#case-study-airline-bookings",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#case-study-airline-bookings",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Case Study: Airline Bookings",
    "text": "Case Study: Airline Bookings"
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#worked-out-example-2",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#worked-out-example-2",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\n\nWorked-Out Example 2\n\n\n\n\nSuppose that GauchoAir has found that each passenger that books a ticket on the GA5A flight from SBA to GCV (GauchoVille) actually shows up with probability 90%.\nIf flight GA5A has only 186 seats, but sells 195 tickets, what is the probability that GauchoAir will need to re-book certain passengers?"
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#solution",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#solution",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Solution",
    "text": "Solution\n\nAs always, we start by defining quantities.\nLet \\(X\\) denote the number of passengers, out of the 195 booked on the flight, that actually show up for the flight.\nThe video claims that \\(X\\) follows a Binomial Distribution- let’s work with that claim for a moment (and then we can revisit that assumption later).\nSpecifically, \\(X \\sim \\mathrm{Bin}(195, \\ 90)\\).\nNow, the airline will only need to re-book passengers when the number of passengers that arive (\\(X\\)) exceeds the capacity of the plane (186).\nSo, the quantity we seek is \\(\\mathbb{P}(X > 186) = \\mathbb{P}(X \\geq 187)\\).\nThough we could do this by hand, let’s use Python.\nWe can also discuss whether we think the Binomial critera really are satisfied in this case or not."
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#geometric-distribution",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#geometric-distribution",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Geometric Distribution",
    "text": "Geometric Distribution\n\nAnother distribution we encountered (this time in Discussion Section) is the so-called Geometric Distribution.\nIf \\(X\\) counts the number of trials (including the final trial) until we observe our first “success”, and if each trial results in a “success” with probability \\(p\\), then \\(X\\) is said to follow the Geometric distribution with parameter \\(p\\): \\[ X \\sim \\mathrm{Geom}(p) \\]\nIf \\(X \\sim \\mathrm{Geom}(p)\\), we have:\n\n\\(S_X = \\{1, 2, 3, \\cdots \\}\\)\n\\(\\mathbb{P}(X = k) = (1 - p)^{k - 1} \\cdot p\\)\n\\(\\mathbb{E}[X] = \\frac{1}{p}\\)\n\\(\\mathrm{Var}(X) = \\frac{1 - p}{p^2}\\) (wasn’t explicitly mentioned before, but it’s good to know!)"
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#continuous-random-variables",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#continuous-random-variables",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Continuous Random Variables",
    "text": "Continuous Random Variables\n\nNot all random variables are discrete- many are continuous!\nContinuous random variables are characterized by a probability density function (pdf) \\(f_X(x)\\), which must obey the following two properties:\n\n\\(f_X(x)\\) must be nonnegative everywhere\nThe area underneath the graph of \\(f_X(x)\\) must be 1\n\nThe graph of a pdf is called a density curve\nProbabilities are found as areas underneath the density curve."
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#areas-under-the-density-curve",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#areas-under-the-density-curve",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Areas Under the Density Curve",
    "text": "Areas Under the Density Curve\n\n\n\n\nFor example, the area above represents \\(\\mathbb{P}(0.25 \\leq X \\leq 0.75)\\)."
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#uniform-distribution",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#uniform-distribution",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Uniform Distribution",
    "text": "Uniform Distribution\n\nAn specific example of a continuous distribution is the Uniform distribution with parameters \\(a\\) and \\(b\\): \\(X \\sim \\mathrm{Unif}(a, \\ b)\\).\nThe p.d.f. is given by \\[ f_X(x) = \\begin{cases} \\frac{1}{b - a} & \\text{if } a \\leq x \\leq b \\\\ 0 & \\text{otherwise} \\\\ \\end{cases} \\]"
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#effect-of-changing-a-and-b",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#effect-of-changing-a-and-b",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Effect of Changing \\(a\\) and \\(b\\)",
    "text": "Effect of Changing \\(a\\) and \\(b\\)\n\nviewof a = Inputs.range(\n  [-3, 3], \n  {value: 0, step: 0.1, label: \"a=\"}\n)\n\nviewof b = Inputs.range(\n  [-3, 3], \n  {value: 1, step: 0.1, label: \"b=\"}\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmargin2 = ({top: 20, right: 30, bottom: 30, left: 40})\n\nheight2 = 400\n\nx_values2 = d32.scaleLinear()\n.domain(d32.extent(data2, d => d.x))\n.range([margin2.left, width - margin2.right])\n\ny_values2 = d32.scaleLinear()\n.domain([Math.min(d32.min(data2, d => d.y),0), Math.max(1,d32.max(data2, d => d.y))]).nice()\n.range([height2 - margin2.bottom, margin2.top])\n\nline2 = d32.line()\n.x(d => x_values2(d.x))\n.y(d => y_values2(d.y))\n\nxAxis2 = g => g\n.attr(\"transform\", `translate(0,${height2 - margin2.bottom})`)\n.call(d32.axisBottom(x_values2)\n      .ticks(width / 80)\n      .tickSizeOuter(0))\n\nyAxis2 = g => g\n.attr(\"transform\", `translate(${margin2.left},0)`)\n.call(d32.axisLeft(y_values2)\n      .tickValues(d32.scaleLinear().domain(y_values2.domain()).ticks()))\n\nfunction unif_pdf (input_value, mu, sigsq) {\nif(input_value < a){\n  return 0\n} else if(input_value > b){\n  return 0\n} else{\n  return 1 / (b - a)\n}\n}\n\nabs_x2=6\n\ndata2 = {\n  let values = [];\n  for (let x = -abs_x2; x < abs_x2; x=x+0.01) values.push({\"x\":x,\"y\":unif_pdf(x, µ, sigsquared)});\n  return values;\n}\n\nd32 = require(\"https://d3js.org/d3.v5.min.js\")\n\nchart2 = {\n  const svg = d32.select(DOM.svg(width, height2));\n  \n  svg.append(\"g\")\n  .call(xAxis2);\n  \n  svg.append(\"g\")\n  .call(yAxis2);\n  \n  svg.append(\"path\")\n  .datum(data2)\n  .attr(\"fill\", \"none\")\n  .attr(\"stroke\", \"steelblue\")\n  .attr(\"stroke-width\", 4)\n  .attr(\"stroke-linejoin\", \"round\")\n  .attr(\"stroke-linecap\", \"round\")\n  .attr(\"d\", line);\n  \n  return svg.node();\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCredit to https://observablehq.com/@dswalter/normal-distribution for the base of the applet code"
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#uniform-distribution-1",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#uniform-distribution-1",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Uniform Distribution",
    "text": "Uniform Distribution\n\nThe expected value and variance are: \\[ \\mathbb{E}[X] = \\frac{a + b}{2} ; \\quad \\mathrm{Var}(X) = \\frac{(b - a)^2}{12} \\]\nAgain, probabilities are found as areas underneath the density curve:"
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#tail-probabilities",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#tail-probabilities",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Tail Probabilities",
    "text": "Tail Probabilities\n\nVisualizing probabilities as areas also enables us to write more complicated probabilistic expressions as differences of tail probabilities:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncan be decomposed as\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[ \\huge - \\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[ \\mathbb{P}(a \\leq X \\leq b) = \\underbrace{\\mathbb{P}(X \\leq b)}_{\\text{c.d.f. at $b$}} - \\underbrace{\\mathbb{P}(X \\leq a)}_{\\text{c.d.f. at $a$}} \\]"
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#chalkboard-example",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#chalkboard-example",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Chalkboard Example",
    "text": "Chalkboard Example\n\n\n\n\n\n\n\nChalkboard Example 1\n\n\n\n\nThe time (in minutes) spent waiting in line at Starbucks is found to vary uniformly between 5mins and 15mins.\nWhat is the c.d.f. of wait times? (I.e., find the probability that a randomly selected person spends less than \\(x\\) minutes waiting in line, for an arbitrary value \\(x\\). Yes, your final answer will depend on \\(x\\); that’s why the c.d.f. is a function!)"
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#chalkboard-example-2",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#chalkboard-example-2",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Chalkboard Example",
    "text": "Chalkboard Example\n\n\n\n\n\n\n\nChalkboard Example 2\n\n\n\n\nThe time (in minutes) spent waiting in line at Starbucks is found to vary uniformly between 5mins and 15mins.\nA random sample of 10 customers is taken; what is the probability that exactly 4 of these customers will spend between 10 and 13 minutes waiting in line?"
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#normal-distribution",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#normal-distribution",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Normal Distribution",
    "text": "Normal Distribution\n\nWe also learned about the Normal Distribution: \\(X \\sim \\mathcal{N}(\\mu, \\ \\sigma)\\)\nThe normal density curve is bell-shaped"
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#changing-mu-and-sigma",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#changing-mu-and-sigma",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Changing \\(\\mu\\) and \\(\\sigma\\)",
    "text": "Changing \\(\\mu\\) and \\(\\sigma\\)\n\nviewof µ = Inputs.range(\n  [-3, 3], \n  {value: 0, step: 0.1, label: \"µ:\"}\n)\n\nviewof σ = Inputs.range(\n  [0.2, 3.1], \n  {value: 1, step: 0.01, label: \"σ:\"}\n)\n\nsigsquared = σ**2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmargin = ({top: 20, right: 30, bottom: 30, left: 40})\n\nheight = 400\n\nx_values = d3.scaleLinear()\n    .domain(d3.extent(data, d => d.x))\n    .range([margin.left, width - margin.right])\n\ny_values = d3.scaleLinear()\n    .domain([Math.min(d3.min(data, d => d.y),0), Math.max(1,d3.max(data, d => d.y))]).nice()\n    .range([height - margin.bottom, margin.top])\n    \nline = d3.line()\n    .x(d => x_values(d.x))\n    .y(d => y_values(d.y))\n\nxAxis = g => g\n  .attr(\"transform\", `translate(0,${height - margin.bottom})`)\n  .call(d3.axisBottom(x_values)\n      .ticks(width / 80)\n      .tickSizeOuter(0))\n\nyAxis = g => g\n  .attr(\"transform\", `translate(${margin.left},0)`)\n  .call(d3.axisLeft(y_values)\n      .tickValues(d3.scaleLinear().domain(y_values.domain()).ticks()))\n    \nfunction normal_pdf (input_value, mu, sigsq) {\n  let left_chunk = 1/(Math.sqrt(2*Math.PI*sigsq))\n  let right_top = -((input_value-mu)**2)\n  let right_bottom = 2*sigsq\n  return left_chunk * Math.exp(right_top/right_bottom)\n}\n\nabs_x=6\n\ndata = {\n  let values = [];\n  for (let x = -abs_x; x < abs_x; x=x+0.01) values.push({\"x\":x,\"y\":normal_pdf(x, µ, sigsquared)});\n  return values;\n}\n\nd3 = require(\"https://d3js.org/d3.v5.min.js\")\n\nchart = {\n  const svg = d3.select(DOM.svg(width, height));\n\n  svg.append(\"g\")\n      .call(xAxis);\n\n  svg.append(\"g\")\n      .call(yAxis);\n  \n  svg.append(\"path\")\n      .datum(data)\n      .attr(\"fill\", \"none\")\n      .attr(\"stroke\", \"steelblue\")\n      .attr(\"stroke-width\", 4)\n      .attr(\"stroke-linejoin\", \"round\")\n      .attr(\"stroke-linecap\", \"round\")\n      .attr(\"d\", line);\n  \n  return svg.node();\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCredit to https://observablehq.com/@dswalter/normal-distribution for the majority of the applet code"
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#standardization",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#standardization",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Standardization",
    "text": "Standardization\n\nIf \\(X \\sim \\mathcal{N}(\\mu, \\ \\sigma)\\), \\[ \\left( \\frac{X - \\mu}{\\sigma} \\right) \\sim \\mathcal{N}(0, \\ 1)\\]\n\n\n\n\nAlso, if \\(X \\sim \\mathcal{N}(\\mu, \\ \\sigma)\\):\n\n\\(\\mathbb{E}[X] = \\mu\\)\n\\(\\mathrm{Var}(X) = \\sigma^2\\)"
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#inferential-statistics-1",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#inferential-statistics-1",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Inferential Statistics",
    "text": "Inferential Statistics\n\nThe primary goal of inferential statistics is to take samples from some population, and use summary statistics to try and make inferences about population parameters\nFor example, we could take samples, compute sample proportions \\(\\widehat{P}\\), and try to make inferences about the population proportion \\(p\\).\nWe could also take samples, compute sample means \\(\\overline{X}\\), and try to make inferences about the population mean \\(\\mu\\).\nOur summary statistics will often be point estimators (i.e. quantities that have expected value equal to the corresponding population parameter), which are random variables as they depend on the sample taken.\n\nFor example, different samples of people will have different average heights.\n\nThe distribution of a point estimator is called the sampling distribution of the estimator."
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#sampling-distribution-of-widehatp",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#sampling-distribution-of-widehatp",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Sampling Distribution of \\(\\widehat{P}\\)",
    "text": "Sampling Distribution of \\(\\widehat{P}\\)\n\nGiven a population with population proportion \\(p\\), we use \\(\\widehat{P}\\) as a point estimator of \\(p\\).\nAssume the success-failure conditions are met; i.e.\n\n\\(n p \\geq 10\\)\n\\(n (1 - p) \\geq 10\\)\n\nThen, the Central Limit Theorem for Proportions tells us that \\[ \\widehat{P} \\sim \\mathcal{N}\\left(p, \\ \\sqrt{\\frac{p(1 - p)}{n}} \\right) \\]\nIf we don’t have access to \\(p\\) directly (as is often the case), we use the substitution approximation to check whether\n\n\\(n \\widehat{p} \\geq 10\\)\n\\(n (1 - \\widehat{p}) \\geq 10\\)"
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#sampling-distribution-of-overlinex",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#sampling-distribution-of-overlinex",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Sampling Distribution of \\(\\overline{X}\\)",
    "text": "Sampling Distribution of \\(\\overline{X}\\)\n\nGiven a population with population mean \\(\\mu\\) and population standard deviation \\(\\sigma\\), we use \\(\\overline{X}\\) as a point estimator of \\(\\mu\\).\nIf the population is normally distributed, then \\[ \\overline{X} \\sim \\mathcal{N}\\left(\\mu, \\ \\frac{\\sigma}{\\sqrt{n}} \\right) \\] or, equivalently, \\[ \\frac{\\overline{X} - \\mu}{\\sigma / \\sqrt{n}} \\sim \\mathcal{N}\\left(0, \\ 1 \\right) \\]\nIf the population is not normally distributed, but the sample size \\(n\\) is at least 30, then the Central Limit Theorem for the Sample Mean (or just the Central Limit Theorem) tells us \\[ \\frac{\\overline{X} - \\mu}{\\sigma / \\sqrt{n}} \\sim \\mathcal{N}\\left(0, \\ 1 \\right) \\]"
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#sampling-distribution-of-overlinex-1",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#sampling-distribution-of-overlinex-1",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Sampling Distribution of \\(\\overline{X}\\)",
    "text": "Sampling Distribution of \\(\\overline{X}\\)\n\nIf the population is non-normal, the sample size is large, and we don’t have access to \\(\\sigma\\) (but access to \\(s\\), the sample standard deviation instead), then \\[ \\frac{\\overline{X} - \\mu}{s / \\sqrt{n}} \\sim t_{n - 1}\\]\nRecall that the \\(t-\\)distribution looks like a standard normal distribution, but has wider tails than the standard normal distribution (which accounts for the additional uncertainty injected into the problem by using \\(s\\), a random variable, in place of \\(\\sigma\\), a deterministic constant).\nAlso recall that \\(t_{\\infty}\\) (i.e. the \\(t-\\)distribution with an infinite degrees of freedom) is the same thing as the standard normal distribution."
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#confidence-intervals",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#confidence-intervals",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\n\nInstead of using point estimators (which are random) to estimate population parameters (which are deterministic), it may make more sense to provide an interval that, with some confidence level, contains the true parameter value.\nIn general, when constructing a confidence interval for a parameter \\(\\theta\\), we use \\[ \\widehat{\\theta} \\pm c \\cdot \\mathrm{SD}(\\widehat{\\theta}) \\] where \\(c\\) is some constant that depends on our confidence level.\n\nAgain, think of the fishing analogy from the textbook- if we want to be more certain we’ll catch a fish, we should cast a wider net; i.e. higher confidence levels will lead to wider intervals.\n\nThe coefficient \\(c\\) will also depend on the sampling distribution of \\(\\widehat{\\theta}\\)."
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#confidence-intervals-for-a-population-proportion",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#confidence-intervals-for-a-population-proportion",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Confidence Intervals for a Population Proportion",
    "text": "Confidence Intervals for a Population Proportion\n\nTo construct a confidence interval for an unknown population proportion \\(p\\), we use \\[ \\widehat{p} \\pm (-c) \\cdot \\sqrt{\\frac{\\widehat{p} \\cdot (1 - \\widehat{p})}{n}} \\] where \\(c\\) denotes the \\((1 - \\alpha) / 2 \\times 100\\)th percentile of the standard normal distribution.\n\nAgain, just remember that the coefficient should be positive"
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#confidence-intervals-for-a-population-mean",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#confidence-intervals-for-a-population-mean",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Confidence Intervals for a Population Mean",
    "text": "Confidence Intervals for a Population Mean\n\nTo construct a confidence interval for an unknown population mean \\(\\mu\\), we use \\[ \\overline{x} \\pm (z_{\\alpha}) \\cdot \\frac{\\sigma}{\\sqrt{n}} \\] or \\[ \\overline{x} \\pm (-t_{n - 1, \\ \\alpha}) \\cdot \\frac{s}{\\sqrt{n}} \\] depending on the conditions listed in the previous section of these slides."
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#worked-out-example-3",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#worked-out-example-3",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\n\nWorked-Out Example 3\n\n\n\n\nSaoirse would like to construct a 95% confidence interval for the true proportion of California Residents that speak Spanish. To that end, she took a representative sample of 120 CA residents and found that 36 of these residents speak Spanish.\n\nIdentify the population\nDefine the parameter of interest.\nDefine the random variable of interest.\nConstruct a 95% confidence interval for the true proportion of CA residents that speak Spanish."
  },
  {
    "objectID": "Pages/Lectures/MT2Rev/mt2rev.html#solutions-2",
    "href": "Pages/Lectures/MT2Rev/mt2rev.html#solutions-2",
    "title": "PSTAT 5A: Lecture 14",
    "section": "Solutions",
    "text": "Solutions\n\nThe population is the set of all California residents.\nThe parameter of interest is \\(p\\), the true proportion of CA residents that speak Spanish.\nThe random variable of interest is \\(\\widehat{P}\\), the proportion of people in a representative sample of 120 CA residents that speak spanish.\nWe check the success-failure conditions, with the substitution approximation:\n\n\\(n \\widehat{p} = (120) \\cdot \\left( \\frac{36}{120} \\right) = 36 \\ \\checkmark\\)\n\\(n (1 - \\widehat{p}) = (120) \\cdot \\left( \\frac{84}{120} \\right) = 84 \\ \\checkmark\\)\n\n\n\nSince these conditions are met, we can proceed in constructing our confidence interval as \\[ 0.3 \\pm 1.96 \\cdot \\sqrt{\\frac{0.3 \\cdot 0.7}{120}} = \\boxed{[0.218 \\ , \\ 0.382]} \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#previously",
    "href": "Pages/Lectures/Lecture12/Lec12.html#previously",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Previously",
    "text": "Previously\n\nOver the course of the past few lectures, we’ve been dealing primarily with population proportions.\nA natural point estimate of \\(p\\) is \\(\\widehat{P}\\), the sample proportion, and used the Central Limit Theorem to figure out what its sampling distribution is.\nWe then used the sampling distribution of \\(\\widehat{P}\\) to construct confidence intervals for the true proportion \\(p\\).\nNow we will turn our attention to a different population parameter."
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#leadup",
    "href": "Pages/Lectures/Lecture12/Lec12.html#leadup",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Leadup",
    "text": "Leadup\n\nRecall from last lecture that any of the descriptive statistics we discussed in Week 1 can be viewed as population parameters, when they apply to the population.\n\nE.g. population proportion (\\(p\\)), population variance (\\(\\sigma^2\\)), etc.\n\nOf particular interest to statisticians is often the population mean, \\(\\mu\\).\nLet’s try and draw some analogies from our work with population proportions.\nWhen trying to make inferences on a population proportion \\(p\\), we used the sample proportion \\(\\widehat{P}\\) as a proxy (specifically, a point estimator).\n\nAny guesses on what we might use as a point estimator of \\(\\mu\\)?\nThat’s right- the sample mean \\(\\overline{X}\\)!"
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#notation",
    "href": "Pages/Lectures/Lecture12/Lec12.html#notation",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Notation",
    "text": "Notation\n\nAgain, it will be useful to establish some notation:\n\n\\(\\mu\\) represents the population mean, and is deterministic (i.e. fixed) but unknown.\n\\(\\overline{X}\\) represents the mean of some hypothetical sample, and is therefore random (as different samples result in different sample means)\n\\(\\overline{x}\\) represents the mean of a specific sample, and is therefore deterministic (i.e. “we’ve taken this particular sample right here and computed its mean).\n\nJust as \\(\\widehat{P}\\) has a sampling distribution, so too does \\(\\overline{X}\\).\nThe sampling distribution of \\(\\overline{X}\\), however, will end up requiring a bit more work."
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#general-confidence-intervals",
    "href": "Pages/Lectures/Lecture12/Lec12.html#general-confidence-intervals",
    "title": "PSTAT 5A: Lecture 12",
    "section": "General Confidence Intervals",
    "text": "General Confidence Intervals\n\nWe will follow the general idea we used before of constructing confidence intervals as \\(\\widehat{\\theta} \\pm \\mathrm{m.e.}\\).\nIn this case, we use \\(\\overline{X}\\) as our point estimator.\nIt turns out that, assuming the population mean is \\(\\mu\\) and the population standard deviation is \\(\\sigma\\), and if \\(\\overline{X}\\) denotes the mean of a sample of size \\(n\\), we have that \\[ \\mathrm{SD}(\\overline{X}) = \\frac{\\sigma}{\\sqrt{n}} \\]\nTherefore, our confidence intervals will take the form \\[ \\overline{X} \\pm c \\cdot \\frac{\\sigma}{\\sqrt{n}} \\] where the constant \\(c\\) depends on both the sampling distribution of \\(\\overline{X}\\) as well as the confidence level."
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#normal-population",
    "href": "Pages/Lectures/Lecture12/Lec12.html#normal-population",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Normal Population",
    "text": "Normal Population\n\nLet’s work on finding the sampling distribution of \\(\\overline{X}\\).\nIt turns out that the first thing we need to ask is whether the underlying population is normally distributed or not.\nIf the underlying population is normally distributed [again with population mean \\(\\mu\\) and population standard deviation \\(\\sigma\\)], we have that \\[ \\frac{\\overline{X} - \\mu}{\\sigma / \\sqrt{n}} \\sim \\mathcal{N}(0, \\ 1) \\] meaning the constant \\(c\\) should be selected as the appropriate percentile of the standard normal distribution: \\[ \\overline{x} \\pm z_\\alpha \\cdot \\frac{\\sigma}{\\sqrt{n}} \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#worked-out-example",
    "href": "Pages/Lectures/Lecture12/Lec12.html#worked-out-example",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\n\nWorked-Out Example 1\n\n\n\n\nThe heights of adult males is assumed to follow a normal distribution with mean 70 in and standard deviation 15 in. A representative sample of 120 adult males is taken, and the average height of males in this sample is recorded.\n\nWhat is the random variable of interest?\nIs the value of 70 in a population parameter or a sample statistic?\nWhat is the probability that the average height of males in the sample is between 69.5 in and 71.5 in?"
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#solutions",
    "href": "Pages/Lectures/Lecture12/Lec12.html#solutions",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Solutions",
    "text": "Solutions\n\n\\(\\overline{X} =\\) the average height of a sample of 120 adult males.\nThe value of 70 in is a population parameter, as it is the true average height of all adult males.\nThe quantity we seek is \\(\\mathbb{P}(69.5 \\leq \\overline{X} \\leq 71.5)\\). Because the population is normally distributed, we can use our result above to conclude \\[ \\overline{X} \\sim \\mathcal{N}\\left( 70, \\ \\frac{15}{\\sqrt{120}} \\right) \\sim \\mathcal{N}\\left( 70, \\ 1.369 \\right) \\]\n\n\nTo find \\(\\mathbb{P}(69.5 \\leq \\overline{X} \\leq 71.5)\\), we therefore utilize the same techniques we used previously, when dealing with normal distribution problems: \\[\\mathbb{P}(69.5 \\leq \\overline{X} \\leq 71.5) = \\mathbb{P}(\\overline{X} \\leq 71.5) - \\mathbb{P}(\\overline{X} \\leq 69.5) \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#solutions-1",
    "href": "Pages/Lectures/Lecture12/Lec12.html#solutions-1",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Solutions",
    "text": "Solutions\n\n\nThe associated \\(z-\\)scores are \\[\\begin{align*}\nz_1     & = \\frac{71.5 - 70}{\\left( \\frac{15}{\\sqrt{120}} \\right)} \\approx 1.10    \\\\\nz_2     & = \\frac{69.5 - 70}{\\left( \\frac{15}{\\sqrt{120}} \\right)} \\approx -0.37\n\\end{align*}\\]\n\n\n\nThe associated probabilities (from a \\(z-\\)table) are \\(0.8643\\) and \\(0.3557\\), respectively, meaning the desired probability is \\[ 0.8643 - 0.3557 = \\boxed{ 0.5086 = 50.86\\% } \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#non-normal-population",
    "href": "Pages/Lectures/Lecture12/Lec12.html#non-normal-population",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Non-Normal Population",
    "text": "Non-Normal Population\n\nAlright, so that explains what to do if the population values follow a normal distribution.\nBut what if they don’t? In real-world settings, we don’t typically get to know exactly what the population distribution is.\nIf our population is not normally distributed, we need to ask ourselves whether we have a “large enough sample”.\nAdmittedly, there isn’t a single agreed-upon cutoff for “large enough”- for the purposes of this class, we will use \\(n \\geq 30\\) to mean “large enough” and \\(n < 30\\) to therefore be “not large enough.”"
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#non-normal-population-n-30",
    "href": "Pages/Lectures/Lecture12/Lec12.html#non-normal-population-n-30",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Non-Normal Population, \\(n < 30\\)",
    "text": "Non-Normal Population, \\(n < 30\\)\n\nIf the population is non-normal, and the sample size is not large enough…\n… we can’t do anything.\nMore specifically, there aren’t any results we can use to confidently make inferences about the population mean- there is just too much uncertainty, between the uncertainty regarding the population’s distribution and the small sample size."
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#non-normal-population-n-geq-30",
    "href": "Pages/Lectures/Lecture12/Lec12.html#non-normal-population-n-geq-30",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Non-Normal Population, \\(n \\geq 30\\)",
    "text": "Non-Normal Population, \\(n \\geq 30\\)\n\nIf the population is non-normal, and the sample size is large enough…\n… we’re still (perhaps surprisingly) in business!\nIt turns out that if \\(n\\) is large enough, \\[ \\frac{\\overline{X} - \\mu}{\\sigma / \\sqrt{n}} \\sim \\mathcal{N}(0, \\ 1) \\] that is, the sample mean once again has a normal sampling distribution!\nIn fact, this is such an important result, we give it a name:"
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#central-limit-theorem-for-the-sample-mean",
    "href": "Pages/Lectures/Lecture12/Lec12.html#central-limit-theorem-for-the-sample-mean",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Central Limit Theorem for the Sample Mean",
    "text": "Central Limit Theorem for the Sample Mean\n\n\n\n\n\n\n\nCentral Limit Theorem for the Sample Mean\n\n\n\nIf we have reasonably representative samples of large enough size \\(n\\), taken from a population with true mean \\(\\mu\\) and true standard deviation \\(\\sigma\\), then \\[ \\frac{\\overline{X} - \\mu}{\\sigma / \\sqrt{n}} \\sim \\mathcal{N}\\left(0, \\ 1 \\right) \\] or, equivalently, \\[ \\overline{X} \\sim \\mathcal{N}\\left( \\mu, \\ \\frac{\\sigma}{\\sqrt{n}} \\right) \\] where \\(\\overline{X}\\) denotes the sample mean."
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#worked-out-example-2",
    "href": "Pages/Lectures/Lecture12/Lec12.html#worked-out-example-2",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\n\nWorked-Out Example 2\n\n\n\n\nThe temperatures collected at all weather stations in Antarctica follow some unknown distribution with unknown mean and known standard deviation 8oF. A researcher records the temperature measurements from a representative sample of 81 different weather stations, and finds the average temperature to be 26oF.\n\nWhat is the population?\nWhat is the sample?\nDefine the random variable of interest.\nWhat is the probability that this observed average of 26oF lies within 1oF of the true average temperature across all weather stations in Antarctica?\nConstruct a 90% confidence interval for the true average temperature across all weather stations in Antarctica."
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#solutions-2",
    "href": "Pages/Lectures/Lecture12/Lec12.html#solutions-2",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Solutions",
    "text": "Solutions\n\nThe population is the set of all weather stations in Antarctica.\nThe sample is the 81 weather stations selected by the researcher.\nThe random variable of interest is \\(\\overline{X}\\), the average temperature across 81 randomly-selected weather stations in Antarctica.\n\n\nPart (d): This is where things get interesting!\n\n\nIs the population normally distributed?\n\nNo. Or, at least, we don’t know for certain, so it’s safer not to assume it is.\n\nIs our sample size large enough to invoke the CLT?\n\nYes, since \\(n = 81 \\geq 30\\).\n\nTherefore, the CLT applies and tells us that \\[ \\overline{X} \\sim \\mathcal{N}\\left(\\mu, \\ \\frac{8}{\\sqrt{81}} \\right) \\sim \\mathcal{N}\\left(\\mu, \\ \\frac{8}{9} \\right) \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#solutions-3",
    "href": "Pages/Lectures/Lecture12/Lec12.html#solutions-3",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Solutions",
    "text": "Solutions\n\nAgain, what we have found is \\[ \\overline{X} \\sim \\mathcal{N}\\left(\\mu, \\ \\frac{8}{\\sqrt{81}} \\right) \\sim \\mathcal{N}\\left(\\mu, \\ \\frac{8}{9} \\right) \\]\nWe seek \\(\\mathbb{P}(\\mu - 1 \\leq \\overline{X} \\leq \\mu + 1)\\), which we first write as \\[ \\mathbb{P}(\\overline{X} \\leq \\mu + 1) - \\mathbb{P}(\\overline{X} \\leq \\mu - 1) \\]\nComputing the necessary \\(z-\\)scores yields \\[\\begin{align*}\nz_1   &  = \\frac{(\\mu + 1) - \\mu}{8/9} = \\frac{9}{8} \\approx 1.13    \\\\\nz_2   &  = \\frac{(\\mu - 1) - \\mu}{8/9} = -\\frac{9}{8} \\approx -1.13\n\\end{align*}\\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#solutions-4",
    "href": "Pages/Lectures/Lecture12/Lec12.html#solutions-4",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Solutions",
    "text": "Solutions\n\nThe corresponding values from the normal table are \\(0.8708\\) and \\(0.1292\\), respectively, meaning the desired probability is \\[ 0.8708 - 0.1292 = \\boxed{74.16\\%} \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#unknown-sigma",
    "href": "Pages/Lectures/Lecture12/Lec12.html#unknown-sigma",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Unknown \\(\\sigma\\)?",
    "text": "Unknown \\(\\sigma\\)?\n\nNotice that in the previous worked-out example (and, indeed, in the CLT for sample means), we need information on the true population standard deviation \\(\\sigma\\).\nWhat happens if we don’t have access to \\(\\sigma\\)?\nWell, we encountered a somewhat similar situation in our discussion on proportions; the standard error of \\(\\widehat{P}\\) depended on \\(p\\), which proves to be a problem in practice (as, again, the true value of \\(p\\) is often unknown).\nDoes anyone remember how we solved this issue in the context of population proportions?\n\nThat’s right- we used the substitution approximation!\nSpecifically, we replaced the unknown parameter (\\(p\\)) with a natural point estimator of it."
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#unknown-sigma-1",
    "href": "Pages/Lectures/Lecture12/Lec12.html#unknown-sigma-1",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Unknown \\(\\sigma\\)?",
    "text": "Unknown \\(\\sigma\\)?\n\nCan anyone propose a point estimator for \\(\\sigma\\)?\nThat’s right; \\(s\\), the sample standard deviation! \\[ s = \\sqrt{ \\frac{1}{n - 1} \\sum_{i=1}^{n} (X_i - \\overline{X})^2} \\]\nIn other words, our proposition is to use confidence intervals of the form \\[ \\overline{x} \\pm c \\cdot \\frac{s}{\\sqrt{n}} \\]\nNotice, however, that this introduces additional uncertainty into the problem as \\(s\\) itself is a random variable (different samples result in different sample standard deviations).\nIt turns out that the additional uncertainty introduced is so large that we become no longer able to use the normal distribution."
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#using-s-in-place-of-sigma",
    "href": "Pages/Lectures/Lecture12/Lec12.html#using-s-in-place-of-sigma",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Using \\(s\\) in place of \\(\\sigma\\)",
    "text": "Using \\(s\\) in place of \\(\\sigma\\)\n\nFirstly, recall that we used percentiles of the standard normal distribution because \\[ \\frac{\\overline{X} - \\mu}{\\sigma / \\sqrt{n}} \\sim \\mathcal{N}(0, \\ 1) \\]\nMathematically, what the above discussion is saying is that the distribution of \\[ \\frac{\\overline{X} - \\mu}{s / \\sqrt{n}} \\] is no longer normal.\nIt turns out that, still assuming a large enough sample size, the quantity above follows what is known as a t-distribution."
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#the-t-distribution",
    "href": "Pages/Lectures/Lecture12/Lec12.html#the-t-distribution",
    "title": "PSTAT 5A: Lecture 12",
    "section": "The t-distribution",
    "text": "The t-distribution\n\nThe \\(t-\\)distribution looks very similar to the standard normal distribution in that it is centered at 1, and has a bell-like density curve.\nHowever, one key difference is that the \\(t-\\)distribution is parameterized by a single parameter, called the degrees of freedom, which we abbreviate \\(\\mathrm{df}\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#the-t-distribution-1",
    "href": "Pages/Lectures/Lecture12/Lec12.html#the-t-distribution-1",
    "title": "PSTAT 5A: Lecture 12",
    "section": "The t-distribution",
    "text": "The t-distribution\n\nAnother key property is that, for all finite degrees of freedom, the tails of the t-distribution density curve are “wider” (i.e. higher) than the tails of the standard normal density curve.\n\nWhat this means is that the t-distribution allows for higher probabilities of tail events, thereby incorporating the additional uncertainty injected into our confidence intervals by using \\(s\\) in place of \\(\\sigma\\)\n\nAn interesting fact is that the t-distribution with \\(\\infty\\) degrees of freedom is equivalent to the standard normal distribution.\n\nAs such, with greater degrees of freedom, the t-distribution - and its percentiles - more and more closely resembles the standard normal distribution."
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#back-to-confidence-intervals",
    "href": "Pages/Lectures/Lecture12/Lec12.html#back-to-confidence-intervals",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Back to Confidence Intervals",
    "text": "Back to Confidence Intervals\n\nHere is the result we’ve been working toward: with samples of reasonably large size \\(n\\) from a distribution with mean \\(mu\\) and standard deviation \\(\\sigma\\), \\[ \\frac{\\overline{X} - \\mu}{s / \\sqrt{n}} \\sim t_{n - 1} \\] where \\(t_{n - 1}\\) denotes the \\(t-\\)distribution with \\(n - 1\\) degrees of freedom.\nAs such, our confidence intervals become \\[ \\overline{x} \\pm t_{n - 1, \\ \\alpha} \\cdot \\frac{s}{\\sqrt{n}} \\] where \\(t_{n - 1, \\ \\alpha}\\) denotes the appropriate quantile (corresponding to our desired confidence level) of the \\(t_{n - 1}\\) distribution."
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#worked-out-example-3",
    "href": "Pages/Lectures/Lecture12/Lec12.html#worked-out-example-3",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\n\nWorked-Out Example 3\n\n\n\n\nA sociologist is interested in performing inference on the true average monthly income (in thousands of dollars) of all citizens of the nation of Gauchonia. As such, she takes a representative sample of 49 people, and finds that these 49 people have an average monthly income of 2.25 and a standard deviation of 1.66.\n\nWhat is the population?\nWhat is the sample?\nDefine the random variable of interest.\nConstruct a 95% confidence interval for the true average monthly income (in thousands of dollars) of Gauchonian citizens."
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#solutions-5",
    "href": "Pages/Lectures/Lecture12/Lec12.html#solutions-5",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Solutions",
    "text": "Solutions\n\nThe population is the set of all Gauchonian residents.\nThe sample is the set of 49 Gauchonian residents included in the sociologist’s sample.\nThe random variable of interest is \\(\\overline{X}\\), the sample average monthly income (in thousands of dollars) of a representative sample of 49 Gauchonian* residents."
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#solutions-contd",
    "href": "Pages/Lectures/Lecture12/Lec12.html#solutions-contd",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Solutions (cont’d)",
    "text": "Solutions (cont’d)\nPart (d)\n\nIs the population normally distributed?\n\nNo.\n\nIs the sample size large enough?\n\nYes; \\(n = 49 \\geq 30\\).\n\nDo we know the population standard deviation?\n\nNo, we only know \\(s\\).\n\nTherefore, we need to use the t-distribution with \\(n - 1 = 49 - 1 = 48\\) degrees of freedom.\nSpecifically, we need to find the 2.5th percentile of the \\(t_{48}\\) distribution.\nOn Monday, during Discussion Section, you will talk about how to read a \\(t-\\)table (which is read slightly differently than a standard normal table).\n\nPlease keep in mind that that will be potentially testable material on next week’s quiz, so be sure to attend Discussion Section!"
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#solutions-contd-1",
    "href": "Pages/Lectures/Lecture12/Lec12.html#solutions-contd-1",
    "title": "PSTAT 5A: Lecture 12",
    "section": "Solutions (cont’d)",
    "text": "Solutions (cont’d)\n\nFor now, we’ll use Python:\n\n\n\nimport scipy.stats as sps\nsps.t.ppf(0.025, 48)\n\n-2.010634754696446\n\n\n\n\nTherefore, our 95% confidence interval takes the form \\[ \\overline{x} \\pm 2.01 \\cdot \\frac{s}{\\sqrt{49}} \\] or, equivalently, \\[ (2.25) \\pm (2.01) \\cdot \\frac{1.66}{7} = 2.25 \\pm 0.477 = \\boxed{[1.773 \\ , \\ 2.727]}\\]\nThe interpretation of this interval is much the same as our intervals for proportions:\n\n\n\nWe are 95% confident that the true average monthly income (in thousands of dollars) of Gauchonian residents is between 1.773 and 2.727."
  },
  {
    "objectID": "Pages/Lectures/Lecture12/Lec12.html#general-flowchart",
    "href": "Pages/Lectures/Lecture12/Lec12.html#general-flowchart",
    "title": "PSTAT 5A: Lecture 12",
    "section": "General Flowchart",
    "text": "General Flowchart\n\n\n\n\ngraph TB\n  A[Is the population Normal?  . ] --> |Yes| B{{Use Normal .}}\n  A --> |No| C[Is n >= 30?  .]\n  C --> |Yes| D[sigma or s?  .]\n  C --> |No| E{{cannot proceed   .}}\n  D --> |sigma| F{{Use Normal .}}\n  D --> |s| G{{Use t }}"
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#cats",
    "href": "Pages/Lectures/Lecture13/Lec13.html#cats",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Cats!",
    "text": "Cats!\n\nAs we have previously seen…\n… I like cats.\nSo, let’s consider another cat example!\nIt is often stated that only 1 in 5 orange tabby cats is female; i.e. that only 20% of orange tabby cats are female.\nLet’s say we take a representative sample of 100 orange tabby cats and find that 19 of these cats are female.\nSince we observed a proportion of only 19% female cats in our sample, does that mean the claim of 20% of orange tabby cats being female is wrong?\nWell, no! We know that this 19% is actually an observed instance of \\(\\widehat{P}\\), which itself is random.\n\nFurthermore, 19% is pretty close to 20% so there’s nothing obviously letting us know that the claim is false."
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#cats-1",
    "href": "Pages/Lectures/Lecture13/Lec13.html#cats-1",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Cats!",
    "text": "Cats!\n\nHowever, if instead our sample of 100 orange tabby cats contained only 1 female in this sample, we might start to question the claim that 20% of orange tabby cats are female.\nOkay, what if in our sample of 100 orange tabby cats we actually only observed 15 female cats?\nThings are perhaps a bit less clear now… we know that there will be some variability in our point estimator, but how much variability would we really expect? Enough to plausibly observe a sample proportion of 15%?\n\n\n\n\nA little more abstractly: we started with a hypothesis (in this example, “20% of orange tabby cats are female”). We then wish to use our data to test how plausible this hypothesis is."
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#hypothesis-testing",
    "href": "Pages/Lectures/Lecture13/Lec13.html#hypothesis-testing",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\n\nThis is exactly the framework of hypothesis testing.\nIn hypothesis testing, we start with a pair of competing claims which we call the null hypothesis and alternative hypothesis, respectively.\n\nWe use \\(H_0\\), read as “h-naught”, to denote the null hypothesis and \\(H_A\\) to denote the alternative hypothesis.\n\nFor instance, in our cat example above the null hypothesis would be “\\(H_0\\): the true proportion of orange cats that are female is 20%”.\nOftentimes we will want to phrase our hypotheses in more mathematical terms. This is where the notation we’ve used over the past few lectures comes into play: letting \\(p\\) denote the true proportion of orange tabby cats that are female, we can write our null hypothesis as \\[ H_0: \\ p = 0.2 \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#hypothesis-testing-1",
    "href": "Pages/Lectures/Lecture13/Lec13.html#hypothesis-testing-1",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\n\nWhat about the alternative hypothesis?\nAs the name suggests, the alternative hypothesis provides some sort of alternative to the null.\nLet’s look at our cat example again. Here are some potential alternatives to the null:\n\n\\(p \\neq 0.2\\) (i.e. the true proportion of orange tabby cats that are female is not 20%)\n\\(p > 0.2\\) (i.e. the true proportion of orange tabby cats that are female is larger than 20%)\n\\(p < 0.2\\) (i.e. the true proportion of orange tabby cats that are female is less than 20%)\n\\(p = 0.10\\) (i.e. the true proportion of orange tabby cats that are female is 10%)"
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#hypothesis-testing-2",
    "href": "Pages/Lectures/Lecture13/Lec13.html#hypothesis-testing-2",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\n\nEach of these alternative hypotheses has a name.\nBefore we talk about these names, let’s establish a slightly more general framework for conducting hypothesis testing on a proportion.\nOur null hypothesis will often take the form \\[ H_0 : \\ p = p_0 \\] for some prespecified value \\(p_0\\) (e.g. 20%, like in our cat example above).\nThis leads to four possible alternative hypotheses:\n\n\\(H_A: \\ p \\neq p_0\\)\n\\(H_A: \\ p > p_0\\)\n\\(H_A: \\ p < p_0\\)\n\\(H_A: \\ p = p_1\\) (where \\(p_1 \\neq p_0\\))\n\nI’d like to stress: in a specific hypothesis testing problem, we need to pick one of these alternative hypotheses"
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#terminology",
    "href": "Pages/Lectures/Lecture13/Lec13.html#terminology",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Terminology",
    "text": "Terminology\n\nWhen our alternative hypothesis is of the form \\(H_A: \\ p \\neq p_0\\), we refer to the situation as a two-sided hypothesis test.\nWhen our alternative hypothesis is of the form \\(H_A: \\ p > p_0\\) or \\(H_A: \\ p < p_0\\), we refer to the situation as a one-sided hypothesis test. Specifically:\n\n\\(H_A: \\ p < p_0\\) leads to a lower-tailed test\n\\(H_A: \\ p > p_0\\) leads to an upper-tailed test\n\nWhen our alternative hypothesis is of the form \\(H_A: \\ p = p_1\\) (for some value \\(p_1\\) different than our null value \\(p_0\\)), we refer to the situation as a simple-vs-simple hypothesis test.\nAgain, our test will be only one of the above!\nAdditionally, it is usually up to the tester (i.e. the statistician or datascientist in charge of conducting the hypothesis test) to pick which set of hypotheses to use."
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#setting-the-hypotheses",
    "href": "Pages/Lectures/Lecture13/Lec13.html#setting-the-hypotheses",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Setting the Hypotheses",
    "text": "Setting the Hypotheses\n\nOkay, so which test to we use when?\nIn practice, there isn’t a one-size-fits-all approach to knowing which set of hypotheses to adapt in a given situation.\nUsually, in the absence of any additional information, we adopt a two-sided test as it tends to be the most general.\nHowever, sometimes additional information may be available to us that may influence us to select a different type of test.\n\nFor example, if previous studies have resulted in numerous observed sample proportions much less than the null value \\(p_0\\), we may want to adopt a lower-tailed test.\n\nHow do we set the null hypothesis? Well, typically the null hypothesis is easier to set: I like to think of it as the “status quo”.\n\nIn other words, the null hypothesis is often taken to be whatever the existing claims state; e.g. that 20% of orange tabby cats are female."
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#worked-out-example",
    "href": "Pages/Lectures/Lecture13/Lec13.html#worked-out-example",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\n\nWorked-Out Example 1\n\n\n\n\nForbes magazine has claimed that, as of May 2023, 91.7% of US households own a vehicle.\n\nWhat is the population?\nWhat is the sample?\nDefine the parameter of interest.\nDefine the random variable of interest.\nWrite down the null hypothesis for this test.\nWrite down the two-sided alternative hypothesis for this test.\nWrite down the lower-tailed hypothesis for this test.\nWrite down the upper-tailed hypothesis for this test."
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#solutions",
    "href": "Pages/Lectures/Lecture13/Lec13.html#solutions",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Solutions",
    "text": "Solutions\n\nThe population is the set of all US households.\nThe sample is the representative sample of 500 US households we took.\nThe parameter of interest is \\(p =\\) the true proportion of US households that own a vehicle.\nThe random variable of interest is \\(\\widehat{P} =\\) the proportion of households in a sample of 500 that own a vehicle.\nRecall that the null hypothesis can be thought of as the “status quo”.\n\nIn other words, we take the null to be whatever claim we want to test: \\[ H_0 : \\ p = 0.917 \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#solutions-contd",
    "href": "Pages/Lectures/Lecture13/Lec13.html#solutions-contd",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Solutions (cont’d)",
    "text": "Solutions (cont’d)\n\nThe two-sided alternative hypothesis would be that the proportion of households that own a vehicle is not equal to 91.7%: \\[ H_A: \\ p \\neq 0.917 \\]\nThe lower-tailed alternative hypothesis would be that the proportion of households that own a vehicle is less than 91.7%: \\[ H_A: \\ p < 0.917 \\]\nThe lower-tailed alternative hypothesis would be that the proportion of households that own a vehicle is greater than 91.7%: \\[ H_A: \\ p > 0.917 \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#hypothesis-testing-3",
    "href": "Pages/Lectures/Lecture13/Lec13.html#hypothesis-testing-3",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\n\nAlright, here is what we have so far in terms of hypotheses:\n\nThe null hypothesis represents a sort of “status quo” statement.\nThe alternative hypothesis represents an alternative to the status quo.\n\nSo, what is a hypothesis test?\nA hypothesis test is a framework/procedure that allows us to determine whether or not the null should be rejected in favor of the alternative.\nNaturally, a hypothesis test will depend on data! As such, we can think of a hypothesis test as a function that takes in data and outputs either reject H0 or fail to reject H0. \\[ \\texttt{decision}(\\texttt{data}) = \\begin{cases} \\texttt{reject } H_0 & \\text{if } \\texttt{...} \\\\  \\texttt{fail to reject } H_0 & \\text{if } \\texttt{...} \\\\ \\end{cases}  \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#fail-to-reject",
    "href": "Pages/Lectures/Lecture13/Lec13.html#fail-to-reject",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Fail To Reject?",
    "text": "Fail To Reject?\n\nBy the way, the results of a hypothesis test are always framed in terms of the null hypothesis; e.g. “reject \\(H_0\\)” or “fail to reject \\(H_0\\)”.\nWait, why are we saying “fail to reject \\(H_0\\)”? Isn’t that just equivalent to “accept \\(H_0\\)”?\nWell, not quite…\nThink of it this way: just because we are saying the particular alternative hypothesis we picked is less plausible than the null, doesn’t mean there isn’t a different alternative hypothesis that is more plausible than the null.\nAll we are saying when we fail to reject the null is exactly that- we didn’t have enough information to reject \\(H_0\\) outright. We are not saying that \\(H_0\\) must be true.\nAdmittedly, some statisticians have gotten a little lax with this distinction and you may encounter textbooks and/or professors that use terms like “accept the null”.\n\nFor better or for worse, I am a bit of a traditionalist in this respect and will adhere to the terminology “fail to reject” in favor of “accept”."
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#four-states-of-the-world",
    "href": "Pages/Lectures/Lecture13/Lec13.html#four-states-of-the-world",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Four States of the World",
    "text": "Four States of the World\n\nOkay, so we’ve talked a bit more about what a hypothesis test actually is: it is a procedure that takes in data and outputs a decision on whether or not to reject the null.\nBehind the scenes, however, the null will either be true or not.\nThis leads to the following four situations:\n\n\n\n\n\n\n\n\n\nResult of Test\n\n\n\n\n\n\n\nReject\n\n\nFail to Reject\n\n\n\n\nH0\n\n\nTrue\n\n\n\n\n\n\n\n\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\nSome of these situations are good, and some of these are bad! Which are which?"
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#four-states-of-the-world-1",
    "href": "Pages/Lectures/Lecture13/Lec13.html#four-states-of-the-world-1",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Four States of the World",
    "text": "Four States of the World\n\n\n\n\n\n\n\n\n\nResult of Test\n\n\n\n\n\n\n\nReject\n\n\nFail to Reject\n\n\n\n\nH0\n\n\nTrue\n\n\nBAD\n\n\nGOOD\n\n\n\n\nFalse\n\n\nGOOD\n\n\nBAD\n\n\n\n\n\n\n\nWe give names to the two “bad” situations: Type I and Type II errors.\n\n\n\n\n\n\n\n\n\nResult of Test\n\n\n\n\n\n\n\nReject\n\n\nFail to Reject\n\n\n\n\nH0\n\n\nTrue\n\n\nType I Error\n\n\nGOOD\n\n\n\n\nFalse\n\n\nGOOD\n\n\nType II Error"
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#type-i-and-type-ii-errors",
    "href": "Pages/Lectures/Lecture13/Lec13.html#type-i-and-type-ii-errors",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Type I and Type II Errors",
    "text": "Type I and Type II Errors\n\n\n\n\n\n\n\nDefinition: Type I and Type II errors\n\n\n\n\n\nA Type I Error occurs when we reject \\(H_0\\), when \\(H_0\\) was actually true.\nA Type II Error occurs when we fail to reject \\(H_0\\), when \\(H_0\\) was actually false.\n\n\n\n\n\n\n\n\nA common way of interpreting Type I and Type II errors are in the context of the judicial system.\nThe US judicial system is built upon a motto of “innocent until proven guilty.” As such, the null hypothesis is that a given person is innocent.\nA Type I error represents convicting an innocent person.\nA Type II error represents letting a guilty person go free."
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#type-i-and-type-ii-errors-1",
    "href": "Pages/Lectures/Lecture13/Lec13.html#type-i-and-type-ii-errors-1",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Type I and Type II Errors",
    "text": "Type I and Type II Errors\n\nViewing the two errors in the context of the judicial system also highlights a tradeoff.\nIf we want to reduce the number of times we wrongfully convict an innocent person, we may want to make the conditions for convicting someone even stronger.\nBut, this would have the consequence of having fewer people overall convicted, thereby (and inadvertently) increasing the chance we let a guilty person go free.\nAs such, controlling for one type of error increses the likelihood of committing the other type."
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#worked-out-example-2",
    "href": "Pages/Lectures/Lecture13/Lec13.html#worked-out-example-2",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\n\nWorked-Out Example 2\n\n\n\n\nForbes magazine has claimed that, as of May 2023, 91.7% of US households own a vehicle.\nAssuming we are conducting a two-sided test, what would a Type I error be in the context of this experiment? What about a Type II error?"
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#solutions-1",
    "href": "Pages/Lectures/Lecture13/Lec13.html#solutions-1",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Solutions",
    "text": "Solutions\n\nA Type I error would be concluding that the true proportion of US households that own a vehicle is not 91.7%, when in fact 91.7% of US households own a vehicle.\nA Type II error would be concluding that the true proportion of US households that own a vehicle is 91.7%, when in fact the true proportion is not 91.8%."
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#leadup",
    "href": "Pages/Lectures/Lecture13/Lec13.html#leadup",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Leadup",
    "text": "Leadup\n\nAlright, now we know about the basics and background surrounding hypothesis tests.\nHow do we actually construct one?\nLet’s focus on hypothesis testing for population proportions for now; we’ll deal with sample means later.\nRecall our setup: our hypothesis test should be some sort of decision-making process of the form \\[ \\texttt{decision}(\\texttt{data}) = \\begin{cases} \\texttt{reject } H_0 & \\text{if } \\texttt{...} \\\\  \\texttt{fail to reject } H_0 & \\text{if } \\texttt{...} \\\\ \\end{cases}  \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#leadup-1",
    "href": "Pages/Lectures/Lecture13/Lec13.html#leadup-1",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Leadup",
    "text": "Leadup\n\nFor the moment, let’s return to the cat example from the beginning of the lecture.\nLetting \\(p\\) denote the true proportion of orange tabby cats that are female, our null hypothesis takes the form \\(H_0: \\ p = 0.2\\).\nSuppose we take a two-sided alternative: \\(H_A: \\ p \\neq 0.2\\).\nNow, we have a good summary statistic for proportions: \\(\\widehat{P}\\).\nAs such, our decision process should probably be of the form \\[ \\texttt{decision}(\\widehat{p}) = \\begin{cases} \\texttt{reject } H_0 & \\text{if } \\texttt{...} \\\\  \\texttt{fail to reject } H_0 & \\text{if } \\texttt{...} \\\\ \\end{cases}  \\]\nSaid differently: if we observe a value of \\(\\widehat{p} = 0.82\\), or a value of \\(\\widehat{p} = 0.001\\), we would likely be inclined to reject the null."
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#test-statistic",
    "href": "Pages/Lectures/Lecture13/Lec13.html#test-statistic",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Test Statistic",
    "text": "Test Statistic\n\nSo, it makes sense to reject \\(H_0\\) when \\(\\widehat{p}\\) is very far away from \\(p_0\\) (which, in the cat example, is \\(0.2\\)). \\[ \\texttt{decision}(\\widehat{p}) = \\begin{cases} \\texttt{reject } H_0 & \\text{if $\\widehat{p}$ is far from $p_0$} \\\\  \\texttt{fail to reject } H_0 & \\text{otherwise}\\\\ \\end{cases}  \\]\nFor reasons that will become clear in a few slides, we typically avoid using \\(\\widehat{p}\\) and instead use a standardized version of \\(\\widehat{p}\\): \\[ \\mathrm{TS} = \\frac{\\widehat{P} - p_0}{\\sqrt{\\frac{p_0 (1 - p_0)}{n}}} \\] where \\(\\mathrm{TS}\\) stands for test statistic."
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#test-statistic-1",
    "href": "Pages/Lectures/Lecture13/Lec13.html#test-statistic-1",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Test Statistic",
    "text": "Test Statistic\n\nLet’s try and convert our decision-making process to be in terms of the test statistic.\nFirst, note that saying \\(\\widehat{p}\\) is “far away” from \\(p_0\\) could mean one of two things:\n\n\\(\\widehat{p}\\) was much larger than \\(p_0\\)\n\\(\\widehat{p}\\) was much smaller than \\(p_0\\)\n\nThese two cases can be combined into a single case if we think in terms of the magnitude of the distance bewteen \\(\\widehat{p}\\) and \\(p_0\\), which is equivalent to considering \\(|\\mathrm{TS}|\\).\nWhat I’m getting at is this: if \\(\\widehat{p}\\) was far away from \\(p_0\\), then \\(|\\mathrm{TS}|\\) must be large.\nHence, we can rephrase our decision process as \\[ \\texttt{decision}(\\mathrm{TS}) = \\begin{cases} \\texttt{reject } H_0 & \\text{if $|\\mathrm{TS}|$ is large} \\\\  \\texttt{fail to reject } H_0 & \\text{otherwise}\\\\ \\end{cases}  \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#constructing-the-test",
    "href": "Pages/Lectures/Lecture13/Lec13.html#constructing-the-test",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Constructing the Test",
    "text": "Constructing the Test\n\nOkay, but how large is “large”?\nThat is, for what values of \\(|\\mathrm{TS}|\\) will we reject the null?\n\nBy the way, the set of values that correspond to a decision of reject is called the rejection region of a test.\n\nIn other words, if our test takes the form \\[ \\texttt{decision}(\\mathrm{TS}) = \\begin{cases} \\texttt{reject } H_0 & \\text{if } |\\mathrm{TS}| > c \\\\ \\texttt{fail to reject } H_0 & \\text{otherwise}\\\\ \\end{cases}  \\] what value should we take \\(c\\) to be?"
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#the-errors",
    "href": "Pages/Lectures/Lecture13/Lec13.html#the-errors",
    "title": "PSTAT 5A: Lecture 13",
    "section": "The Errors",
    "text": "The Errors\n\nWell, to answer this question, we need to return to our considerations of Type II and Type II errors.\nRecall that a Type I error occurs when we reject \\(H_0\\) when \\(H_0\\) was actually true, and a Type II error occurs when we fail to reject \\(H_0\\) when \\(H_0\\) was false.\nChanging the value of \\(c\\) changes the probability of committing the two types of errors!\nSpecifically, setting a larger value of \\(c\\) corresponds to rejecting \\(H_0\\) for fewer values, thereby decreasing the probability of committing a Type I errror but increasing the probability of committing a Type II error.\nConversely, setting a smaller value of \\(c\\) corresponds to rejecting \\(H_0\\) for more values, thereby increasing the probability of committing a Type I error but decreasing the probability of committing a Type II error."
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#the-compromise",
    "href": "Pages/Lectures/Lecture13/Lec13.html#the-compromise",
    "title": "PSTAT 5A: Lecture 13",
    "section": "The Compromise",
    "text": "The Compromise\n\nWe need to compromise!\nIn practice, we go into the test knowing how much leeway we are going to allow ourselves to commit a Type I error. That is, we prespecify our tolerance for committing a Type I error.\nThe probability of committing a Type I error is called the level of significance (or just significance level), and is often denoted \\(\\alpha\\).\nStatisticians therefore construct a hypothesis test around a specific value of \\(\\alpha\\).\nA common level of significance is \\(\\alpha = 0.05\\), though \\(\\alpha = 0.01\\) and \\(\\alpha = 0.1\\) are sometimes used as well.\nOkay, so what does this mean for our test?\nWe now know that \\(\\alpha\\) denotes the probability of rejecting the null when the null is true; i.e. \\[ \\mathbb{P}_{H_0}(|\\mathrm{TS}| > c) = \\alpha \\] where the symbol \\(\\mathbb{P}_{H_0}\\) just means “assuming the null, the probability of….”"
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#the-compromise-1",
    "href": "Pages/Lectures/Lecture13/Lec13.html#the-compromise-1",
    "title": "PSTAT 5A: Lecture 13",
    "section": "The Compromise",
    "text": "The Compromise\n\nAgain, remember that \\(\\alpha\\) is fixed (e.g. \\(0.05\\)); it is the value of \\(c\\) we are after!\nSo, a natural question arises: what is the distribution of \\(\\mathrm{TS}\\) under the null?\nRecall that \\[ \\mathrm{TS} = \\frac{\\widehat{P} - p_0}{\\sqrt{\\frac{p_0 (1 - p_0)}{n}}} \\]\nNow, assuming the null is true (i.e. that \\(p = p_0\\)), the Central Limit Theorem for Proportions tells us \\[ \\widehat{P} \\stackrel{H_0}{\\sim} \\mathcal{N}\\left(p_0, \\ \\sqrt{\\frac{p_0(1 - p_0)}{n}} \\right) \\] where the symbol \\(\\stackrel{H_0}{\\sim}\\) is just a shorthand for “distributed as, under the null”"
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#the-distribution-of-the-test-statistic",
    "href": "Pages/Lectures/Lecture13/Lec13.html#the-distribution-of-the-test-statistic",
    "title": "PSTAT 5A: Lecture 13",
    "section": "The Distribution of the Test Statistic",
    "text": "The Distribution of the Test Statistic\n\nTherefore, assuming the null is correct, we have \\[ \\mathrm{TS} \\sim \\mathcal{N}(0, \\ 1)\\]\nSo, our condition \\[ \\mathbb{P}_{H_0}(|\\mathrm{TS}| > c) = \\alpha \\] which, by the symmetry of the standard normal distribution, is equivalent to \\[ \\mathbb{P}_{H_0}(\\mathrm{TS} < -c) = \\frac{\\alpha}{2} \\]\nHence, \\(-c\\) is just the \\((\\alpha / 2) \\times 100\\) percentile of the standard normal distribution!!!"
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#the-test",
    "href": "Pages/Lectures/Lecture13/Lec13.html#the-test",
    "title": "PSTAT 5A: Lecture 13",
    "section": "The Test",
    "text": "The Test\n\n\n\n\n\n\n\nTwo-Sided Test for a Proportion:\n\n\n\n\nWhen testing \\(H_0: \\ p = p_0\\) vs \\(H_A: \\ p \\neq p_0\\) at an \\(\\alpha\\) level of significance, where \\(p\\) denotes a population proportion, the test takes the form \\[ \\texttt{decision}(\\mathrm{TS}) = \\begin{cases} \\texttt{reject } H_0 & \\text{if } |\\mathrm{TS}| > z_{1 - \\alpha/2} \\\\ \\texttt{fail to reject } H_0 & \\text{otherwise}\\\\ \\end{cases}  \\] where:\n\n\\(\\displaystyle \\mathrm{TS} = \\frac{\\widehat{p} - p_0}{\\sqrt{\\frac{p_0(1 - p_0)}{n}}}\\)\n\\(z_{1 - \\alpha/2}\\) denotes the \\((\\alpha/2) \\times 100\\)th percentile of the standard normal distribution, scaled by negative 1."
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#the-test-1",
    "href": "Pages/Lectures/Lecture13/Lec13.html#the-test-1",
    "title": "PSTAT 5A: Lecture 13",
    "section": "The Test",
    "text": "The Test\n\nFor example, if \\(\\alpha = 0.05\\) then \\(c\\) is negative 1 times the 2.5th percentile of the standard normal distribution; i.e. 1.96 and our test becomes \\[ \\texttt{decision}(\\mathrm{TS}) = \\begin{cases} \\texttt{reject } H_0 & \\text{if } |\\mathrm{TS}| > 1.96 \\\\ \\texttt{fail to reject } H_0 & \\text{otherwise}\\\\ \\end{cases}  \\]\n\n\nCAUTION!!!\n\n\nAll of this is predicated on our invocation of the Central Limit Theorem for Proportions!\nIn other words, the test above was derived assuming \\[ \\widehat{P} \\stackrel{H_0}{\\sim} \\mathcal{N}\\left(p_0, \\ \\sqrt{\\frac{p_0(1 - p)}{n}} \\right) \\]"
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#the-assumptions",
    "href": "Pages/Lectures/Lecture13/Lec13.html#the-assumptions",
    "title": "PSTAT 5A: Lecture 13",
    "section": "The Assumptions",
    "text": "The Assumptions\n\n\nThis is not always true!\n\n\n\nWhen is this true? In other words, what conditions do we need in order for the above distributional statement to be true?\n\nThat’s right, the success-failure conditions.\n\nBut now, since we only need the above to be true for \\(p = p_0\\), we only need to verify that:\n\n\\(n p_0 \\geq 10\\)\n\\(n (1 - p_0) \\geq 10\\)\n\nIt is very important we check these conditions before conducting our test!"
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#the-test-2",
    "href": "Pages/Lectures/Lecture13/Lec13.html#the-test-2",
    "title": "PSTAT 5A: Lecture 13",
    "section": "The Test",
    "text": "The Test\n\n\n\n\n\n\n\nTwo-Sided Test for a Proportion:\n\n\n\nWhen testing \\(H_0: \\ p = p_0\\) vs \\(H_A: \\ p \\neq p_0\\) at an \\(\\alpha\\) level of significance, where \\(p\\) denotes a population proportion:\n\nCheck that the success-failure conditions hold. Namely, check that:\n\n\\(n p_0 \\geq 10\\)\n\\(n (1 - p_0) \\geq 10\\)\n\nCompute the test statistic \\[\\displaystyle \\mathrm{TS} = \\frac{\\widehat{p} - p_0}{\\sqrt{\\frac{p_0(1 - p_0)}{n}}}\\]\nCompute the critical value \\(z_{1 - \\alpha/2}\\), which is the the \\((\\alpha/2) \\times 100\\)th percentile of the standard normal distribution, scaled by negative 1.\nReject \\(H_0\\) if \\(|\\mathrm{TS}| > z_{1 - \\alpha/2}\\), and fail to reject \\(H_0\\) otherwise."
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#worked-out-example-3",
    "href": "Pages/Lectures/Lecture13/Lec13.html#worked-out-example-3",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\n\nWorked-Out Example 3\n\n\n\n\nForbes magazine has claimed that, as of May 2023, 91.7% of US households own a vehicle. To test that claim, we take a representative sample of 500 US households and observe that 89.4% of these households own a vehicle.\nConduct a two-sided hypothesis test at a \\(5\\%\\) level of significance on Forbes’s claim that 91.7% of US households own a vehicle. Be sure you phrase your conclusion clearly, and in the context of the problem."
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#solutions-2",
    "href": "Pages/Lectures/Lecture13/Lec13.html#solutions-2",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Solutions",
    "text": "Solutions\n\nAll we really need to do is follow the steps outlined in the previous slide.\n\n\nCheck Conditions\n\n\\(n p_0 = 500 \\cdot (0.917) = 458.5 \\geq 10 \\ \\checkmark\\)\n\\(n (1 - p_0) = 500 \\cdot (1 - 0.917) = 41.5 \\geq 10 \\ \\checkmark\\)\nSince both conditions are met, we can proceed.\n\nCompute the Test Statistic \\[ \\mathrm{TS} = \\frac{\\widehat{p} - p_0}{\\sqrt{\\frac{p_0 (1 - p_0)}{n}}} = \\frac{0.894 - 0.917}{\\sqrt{\\frac{0.917 (1 - 0.917)}{500}}} = -1.86 \\]\nCompute the critical value Because \\(\\alpha = 0.05\\), the critical value is \\(1.96\\)."
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#solutions-3",
    "href": "Pages/Lectures/Lecture13/Lec13.html#solutions-3",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Solutions",
    "text": "Solutions\n\nConduct the test: We see that \\(|\\mathrm{TS}| = |-1.86| = 1.86\\) which is not greater than \\(1.96\\). As such, we fail to reject the null.\n\n\nNow, the problem told us to phrase our conclusions carefully and in the context of the problem.\nIt is VERY important to include the level of significance in your final conclusions.\nSo, here is how we would phrase the final conclusion of our test:\n\n\n\nAt an \\(\\alpha = 0.05\\) level of significance, there is insufficient evidence to reject Forbes’s claim that 91.7% of US households own a vehicle.\n\n\n\nIt is very important to include the level of significance in our conclusion! This is because the final outcome of our test may change depending on which level of significance we use."
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#worked-out-example-4",
    "href": "Pages/Lectures/Lecture13/Lec13.html#worked-out-example-4",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Worked-Out Example",
    "text": "Worked-Out Example\n\n\n\n\n\n\n\nWorked-Out Example 4\n\n\n\n\nForbes magazine has claimed that, as of May 2023, 91.7% of US households own a vehicle. To test that claim, we take a representative sample of 500 US households and observe that 89.4% of these households own a vehicle.\nConduct a two-sided hypothesis test at a \\(1\\%\\) level of significance on Forbes’s claim that 91.7% of US households own a vehicle. Be sure you phrase your conclusion clearly, and in the context of the problem."
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#solutions-4",
    "href": "Pages/Lectures/Lecture13/Lec13.html#solutions-4",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Solutions",
    "text": "Solutions\n\nThe only thing that will change from before is our critical value.\nSince we are using an \\(\\alpha = 0.01\\) level of significance, we find the 0.5th percentile [since \\((0.01) / 2 \\times 100\\% = 0.05\\%\\)]\nThere are several ways we could find this critical value.\nThe first is to use our normal table: \\(2.575\\).\nThe second is to use our \\(t-\\)table: \\(2.58\\)\nThe third is to use Python:\n\n\n\nimport scipy.stats as sps\n-sps.norm.ppf(0.005)\n\n2.575829303548901"
  },
  {
    "objectID": "Pages/Lectures/Lecture13/Lec13.html#solutions-contd-1",
    "href": "Pages/Lectures/Lecture13/Lec13.html#solutions-contd-1",
    "title": "PSTAT 5A: Lecture 13",
    "section": "Solutions (cont’d)",
    "text": "Solutions (cont’d)\n\nSince the observed value of our test statistic \\(|\\mathrm{TS}| = 1.86\\) is smaller than all of these values, we fail to reject the null.\n\n\n\nAt an \\(\\alpha = 0.01\\) level of significance, there is insufficient evidence to reject Forbes’s claim that 91.7% of US households own a vehicle.\n\n\n\nBy the way, I’m sure some of you are wondering exactly which cutoff (the one from the \\(z-\\)table, the one from the \\(t-\\)table, or the one from Python) to use?\nIn a real-world setting, definitely use the one Python generated as it is the most accurate.\nOn a quiz, I will accept either the \\(z-\\)table or the \\(t-\\)table value, provided you VERY CLEARLY AND EXPLICITLY state which one you are using."
  },
  {
    "objectID": "Pages/Lectures/ProbRev/ProbRev.html#basics-of-probability",
    "href": "Pages/Lectures/ProbRev/ProbRev.html#basics-of-probability",
    "title": "PSTAT 5A: Lecture 21",
    "section": "Basics of Probability",
    "text": "Basics of Probability\n\nExperiment: A procedure we can repeat and infinite number of times, where each time we repeat the procedure the same fixed set of things (i.e. outcomes) can occur.\n\nOutcome Space: The set of all outcomes associated with an experiment\n\nDifferent ways to express an outcome space: as a set, using a table (for two-stage experiments), or using a tree.\nEvent: A subset of \\(\\Omega\\)\n\nI.e. an event is a set comprised of outcomes."
  },
  {
    "objectID": "Pages/Lectures/ProbRev/ProbRev.html#example",
    "href": "Pages/Lectures/ProbRev/ProbRev.html#example",
    "title": "PSTAT 5A: Lecture 21",
    "section": "Example",
    "text": "Example\nToss a fair coin twice, and record the outcome of each toss\n\nOutcome Space:\n\nAs a set: \\(\\Omega = \\{(H, H), \\ (H, T), \\ (T, H), \\ (T, T) \\}\\)\nAs a table:\n\n\n\n\n\n\n\n\n\nH\n\n\nT\n\n\n\n\nH\n\n\n(H, H)\n\n\n(H, T)\n\n\n\n\nT\n\n\n(T, H)\n\n\n(T, T)"
  },
  {
    "objectID": "Pages/Lectures/ProbRev/ProbRev.html#example-1",
    "href": "Pages/Lectures/ProbRev/ProbRev.html#example-1",
    "title": "PSTAT 5A: Lecture 21",
    "section": "Example",
    "text": "Example\nToss a fair coin twice, and record the outcome of each toss\n\n\nOutcome Space:\n\nAs a tree:\n\n\n\n\n\n\n\n\n\n\n\ntree_diagram\n\n  \n\nbase\n\no   \n\nH1\n\nH   \n\nbase->H1\n\n    \n\nT1\n\nT   \n\nbase->T1\n\n    \n\nH21\n\nH   \n\nH1->H21\n\n    \n\nT21\n\nT   \n\nH1->T21\n\n    \n\nH22\n\nH   \n\nT1->H22\n\n    \n\nT22\n\nT   \n\nT1->T22"
  },
  {
    "objectID": "Pages/Lectures/ProbRev/ProbRev.html#example-2",
    "href": "Pages/Lectures/ProbRev/ProbRev.html#example-2",
    "title": "PSTAT 5A: Lecture 21",
    "section": "Example",
    "text": "Example\nToss a fair coin twice, and record the outcome of each toss\n\nSome events:\n\n“At least one heads:” \\(\\{(H, T), \\ (T, H), \\ (H, H)\\}\\)\n“At most one heads:” \\(\\{(T, T), \\ (T, H), \\ (H, T)\\}\\)\n“No heads and no tails:” \\(\\varnothing\\)"
  },
  {
    "objectID": "Pages/Lectures/ProbRev/ProbRev.html#unions-intersections-complements",
    "href": "Pages/Lectures/ProbRev/ProbRev.html#unions-intersections-complements",
    "title": "PSTAT 5A: Lecture 21",
    "section": "Unions, Intersections, Complements",
    "text": "Unions, Intersections, Complements\n\n\n\n\n\n \\(A^\\complement\\)  (complement)\n\n\n\n\n\n\n\n\n \\(A \\cap B\\)  (intersection)\n\n\n\n\n\n\n\n\n \\(A \\cup B\\)  (union)"
  },
  {
    "objectID": "Pages/Lectures/ProbRev/ProbRev.html#demorgans-laws",
    "href": "Pages/Lectures/ProbRev/ProbRev.html#demorgans-laws",
    "title": "PSTAT 5A: Lecture 21",
    "section": "DeMorgan’s Laws",
    "text": "DeMorgan’s Laws\n\n\\((E \\cap F)^\\complement = (E^\\complement) \\cup (F^\\complement)\\)\n\nThe opposite of “E and F” is “either E did not occur, or F did not occur (or both)”\n\n\n\n\n\n\\((E \\cup F)^\\complement = (E^\\complement) \\cap (F^\\complement)\\)\n\nThe opposite of “E or F” is “neither E nor F occur”"
  },
  {
    "objectID": "Pages/Lectures/ProbRev/ProbRev.html#probability-1",
    "href": "Pages/Lectures/ProbRev/ProbRev.html#probability-1",
    "title": "PSTAT 5A: Lecture 21",
    "section": "Probability",
    "text": "Probability\n\nTwo main ways of defining the probability of an event \\(E\\).\nClassical Approach: \\(\\displaystyle \\mathbb{P}(E) = \\frac{\\#(E)}{\\#(\\Omega)}\\)\n\nCan be used only when we have equally likely outcomes.\nKeywords to look out for: at random, randomly, uniformly, etc.\n\nLong-Run Relative Frequency Approach: Define \\(\\mathbb{P}(E)\\) to be the relative frequency of the times we observe \\(E\\), after an infinite number of repetitions of our experiment."
  },
  {
    "objectID": "Pages/Lectures/ProbRev/ProbRev.html#relative-frequencies",
    "href": "Pages/Lectures/ProbRev/ProbRev.html#relative-frequencies",
    "title": "PSTAT 5A: Lecture 21",
    "section": "Relative Frequencies",
    "text": "Relative Frequencies\nSuppose we toss a coin and record whether the outcome lands heads or tails, and further suppose we observe the following tosses:\n\n\nH,  T,   T,   H,   T,   H,   H,   H,   T,   T\n\n\n\nTo compute the relative frequency of heads after each toss, we count the number of times we observed heads and divide by the total number of tosses observed."
  },
  {
    "objectID": "Pages/Lectures/ProbRev/ProbRev.html#conditional-probability",
    "href": "Pages/Lectures/ProbRev/ProbRev.html#conditional-probability",
    "title": "PSTAT 5A: Lecture 21",
    "section": "Conditional Probability",
    "text": "Conditional Probability\n\n\\(\\mathbb{P}(E \\mid F)\\): represents our updated beliefs on \\(E\\), in the presence of the information contained in \\(F\\).\n\nOnly defined when \\(\\mathbb{P}(F) \\neq 0\\)\nComputed as \\(\\displaystyle \\mathbb{P}(E \\mid F) = \\frac{\\mathbb{P}(E \\cap F)}{\\mathbb{P}(F)}\\)\n\nMultiplication Rule: \\(\\mathbb{P}(E \\cap F) = \\mathbb{P}(E \\mid F) \\cdot \\mathbb{P}(F) = \\mathbb{P}(F \\mid E) \\cdot \\mathbb{P}(E)\\)\nBayes’ Rule: \\(\\displaystyle \\mathbb{P}(E \\mid F) = \\frac{\\mathbb{P}(F \\mid E) \\cdot \\mathbb{P}(E)}{\\mathbb{P}(F)}\\)\nLaw of Total Probability: \\(\\displaystyle \\mathbb{P}(E) = \\mathbb{P}(E \\mid F) \\cdot \\mathbb{P}(F) + \\mathbb{P}(E \\mid F^\\complement) \\cdot \\mathbb{P}(F^\\complement)\\)"
  },
  {
    "objectID": "Pages/Lectures/ProbRev/ProbRev.html#independence",
    "href": "Pages/Lectures/ProbRev/ProbRev.html#independence",
    "title": "PSTAT 5A: Lecture 21",
    "section": "Independence",
    "text": "Independence\n\nTwo events \\(E\\) and \\(F\\) are independent if any of the following are true:\n\n\\(\\mathbb{P}(E \\mid F) = \\mathbb{P}(E)\\)\n\\(\\mathbb{P}(F \\mid E) = \\mathbb{P}(F)\\)\n\\(\\mathbb{P}(E \\cap F) = \\mathbb{P}(E) \\cdot \\mathbb{P}(F)\\)"
  },
  {
    "objectID": "Pages/Lectures/ProbRev/ProbRev.html#fundamental-principle-of-counting",
    "href": "Pages/Lectures/ProbRev/ProbRev.html#fundamental-principle-of-counting",
    "title": "PSTAT 5A: Lecture 21",
    "section": "Fundamental Principle of Counting",
    "text": "Fundamental Principle of Counting\n\n\n\n\n\n\n\nFundamental Principle of Counting\n\n\n\nIf an experiment consists of \\(k\\) stages, where the \\(i\\)th stage has \\(n_i\\) possible configurations, then the total number of elements in the outcome space is \\[ n_1 \\times n_2 \\times \\cdots \\times n_k \\]\n\n\n\n\n\n\nE.g.: number of ice cream scoops consisting of one flavor (Vanilla, Chocolate, or Matcha) and the one topping (sprinkles or coconut) is \\(3 \\times 2 = 6\\)."
  },
  {
    "objectID": "Pages/Lectures/ProbRev/ProbRev.html#counting-formulas",
    "href": "Pages/Lectures/ProbRev/ProbRev.html#counting-formulas",
    "title": "PSTAT 5A: Lecture 21",
    "section": "Counting Formulas",
    "text": "Counting Formulas\n\nn factorial: \\(n! = n \\times (n - 1) \\times \\cdots \\times (3) \\times (2) \\times (1)\\)\n\n\\(0! = 1\\)\n\nn order k: \\(\\displaystyle (n)_k = \\frac{n!}{(n - k)!}\\)\nn choose k: \\(\\displaystyle \\binom{n}{k} = \\frac{n!}{k! \\cdot (n - k)!}\\)\n\n\n\n\nFor more practice, I encourage you to take a look at Homework 2, along with some of the MT1 practice problems."
  },
  {
    "objectID": "Pages/Lectures/ProbRev/ProbRev.html#example-chalkboard",
    "href": "Pages/Lectures/ProbRev/ProbRev.html#example-chalkboard",
    "title": "PSTAT 5A: Lecture 21",
    "section": "Example (Chalkboard)",
    "text": "Example (Chalkboard)\n\n\n\n\n\n\nChalkboard Exercise 1\n\n\n\nAn observational study tracked whether or not a group of individuals were taking a particular drug, along with whether or not they had high blood pressure.\n\n\n            Blood.Pressure\nDrug         High Low\n  Not Taking   10  10\n  Taking       10  20\n\n\nA participant is selected at random.\n\nWhat is the probability that they have high blood pressure?\nWhat is the probability that they have either high blood pressure or are taking the drug?\nIf they have high blood pressure, what is the probability that they are taking the drug?\nAre the events “taking the drug” and “having high blood pressure” independent?"
  },
  {
    "objectID": "Pages/Lectures/ProbRev/ProbRev.html#example-chalkboard-1",
    "href": "Pages/Lectures/ProbRev/ProbRev.html#example-chalkboard-1",
    "title": "PSTAT 5A: Lecture 21",
    "section": "Example (Chalkboard)",
    "text": "Example (Chalkboard)\n\n\n\n\n\n\nChalkboard Exercise 2\n\n\n\nA recent survey at Ralph’s grocery store revealed that 25% of people buy soda and 40% of people by fruit. Additionally, 40% of people who buy soda also buy fruit. If a customer at Ralph’s is selected at random….\n\n… what is the probability that they buy either soda or fruit?\n… what is the probability that they buy neither soda nor fruit?"
  },
  {
    "objectID": "Pages/Lectures/ProbRev/ProbRev.html#random-variables-1",
    "href": "Pages/Lectures/ProbRev/ProbRev.html#random-variables-1",
    "title": "PSTAT 5A: Lecture 21",
    "section": "Random Variables",
    "text": "Random Variables\n\nA random variable, loosely speaking, is a variable that tracks some sort of outcome of an experiment.\n\nE.g. “number of heads in 10 coin tosses”\nE.g. “height of a randomly-selected building from downtown Santa Barbara”\n\nEvery random variable has a state space, which is the set of values the random variable can attain. We use the notation \\(S_X\\) to denote the state space of the random variable \\(X\\).\n\nIf \\(S_X\\) has jumps, we say \\(X\\) is discrete.\nOtherwise, we say \\(X\\) is continuous"
  },
  {
    "objectID": "Pages/Lectures/ProbRev/ProbRev.html#discrete-random-variables",
    "href": "Pages/Lectures/ProbRev/ProbRev.html#discrete-random-variables",
    "title": "PSTAT 5A: Lecture 21",
    "section": "Discrete Random Variables",
    "text": "Discrete Random Variables\n\nDiscrete random variables are characterized by a probability mass function (p.m.f.), which expresses not only the values the random variable can take but also the probability with which it attains those values.\n\nWe use the notation \\(\\mathbb{P}(X = k)\\) to denote the probability that a random variable \\(X\\) attains the value of \\(k\\).\n\nExpected Value: \\(\\displaystyle \\mathbb{E}[X] = \\sum_{\\text{all $k$}} k \\cdot \\mathbb{P}(X = k)\\)\nVariance:\n\n\\(\\displaystyle \\mathrm{Var}(X) = \\sum_{\\text{all $k$}}(k - \\mathbb{E}[X])^2 \\cdot \\mathbb{P}(X = k)\\)\n\\(\\displaystyle \\mathrm{Var}(X) = \\left( \\sum_{\\text{all $k$}} k^2 \\cdot \\mathbb{P}(X = k) \\right) - (\\mathbb{E}[X])^2\\)"
  },
  {
    "objectID": "Pages/Lectures/ProbRev/ProbRev.html#continuous-random-variables",
    "href": "Pages/Lectures/ProbRev/ProbRev.html#continuous-random-variables",
    "title": "PSTAT 5A: Lecture 21",
    "section": "Continuous Random Variables",
    "text": "Continuous Random Variables\n\nContinuous random variables are characterized by a probability density function (p.d.f.), which is a function \\(f_X(x)\\) satisfying:\n\nnonnegativity: \\(f_X(x) \\geq 0\\) for all \\(x \\in \\mathbb{R}\\)\narea equals 1: the area under the graph of \\(f_X(x)\\) should be 1\n\nThe term density curve refers to the graph of the p.d.f.\n\nProbabilities are found as areas underneath the density curve\n\nCumulative Distribution Function: $F_X(x) = (X )\n\\(\\mathbb{P}(X = k) = 0\\) if \\(X\\) is continuous."
  },
  {
    "objectID": "Pages/Lectures/ProbRev/ProbRev.html#distributions",
    "href": "Pages/Lectures/ProbRev/ProbRev.html#distributions",
    "title": "PSTAT 5A: Lecture 21",
    "section": "Distributions",
    "text": "Distributions\n\nBinomial: \\(X \\sim \\mathrm{Bin}(n, \\ p)\\)\nUniform: \\(X \\sim \\mathrm{Unif}(a, \\ b)\\)\nNormal: \\(X \\sim \\mathcal{N}(\\mu, \\ \\sigma)\\)\n\nStandardization"
  },
  {
    "objectID": "Pages/schedule.html",
    "href": "Pages/schedule.html",
    "title": "PSTAT 5A: Understanding Data",
    "section": "",
    "text": "Disclaimer\n\n\n\nThis schedule is tentative, and is subject to change. Please check back regularly for updates!\n\n\nLast Updated: Sunday, June 11, 2023\n\nAs a reminder: Monday sections are Discussion Sections and Wednesday sections are Lab Sections. There will not be a discussion section in week 1 (as, if there were one, it would take place before the first lecture!)\n\n\n\n\n\n\n\n\n\n\n\nWeek\nLectures\nDiscussion Worksheet\nLab\n\n\n\n\n1\n(4/03 - 4/09)\n\n00: Introduction \n01: Descriptive Statistics \n02: More Descriptive Statistics \n\nNo Discussion Section\nLab 01  (Introduction to Python, and Basic Data Types)\nSolns \n\n\n2\n(4/10 - 4/16)\n\n03: Intro to Probability \n04: Counting \n\nDiscussion Worksheet 01 \nSolns \nLab 02  (Data Classes)\nSolns \n\n\n3\n(4/17 - 4/23)\n\n05: Conditional Probabilities \n06: Review \n\nExam Information/Polices \n\nList of Topics \n\nPractice Problems \n\nPractice Problems Solns \n\nMT1 Coverpage \n\n\nDiscussion Worksheet 02 \nSolns \nLab 03 (Conditionals, Comparisons, and Functions)\nSolns \n\n\n4\n(4/24 - 4/30)\n\n07: Midterm Exam 1\n\nBlue Version: Blank Solns \nYellow Version: Blank Solns \n\n08: Discrete Random Variables \n\nDiscussion Worksheet 03 (Midterm 1 Review)\nLab 04 (Descriptive Statistics)\nSolns  \n\n\n5\n(5/01 - 5/07)\n\n09: Continuous Random Variables \n\nNormal Table\n\n10: Intro to Inferential Statistics \n\nIn-Lecture Demo \n\nExercise 2 Solns \n\n\nDiscussion Worksheet 04 \nSolns \nLab 05  Random Number Generation, and Simulations\nSolns \n\n\n\n6\n(5/08 - 5/14)\n\n11: Confidence Intervals for Proportions \n12: Inference on the Mean \n\nDiscussion Worksheet 05 \nSolns \nLab 06 Loops\n\nSolns \n\n\n7\n(5/15 - 5/21)\n\n13: Hypothesis Testing, Part I \n14: Review \n\nList of Topics \nExam Information/Polices \nPractice Problems \n\nSolns \n\nMC Practice \n\nSolns \n\n\n\n\nDiscussion Worksheet 06 \n\nCombined Tables\n\nSolns \nLab 07 Percentiles, and Testing Distributional Fits\nSolns \n\n\n8\n(5/22 - 5/28)\n\n15: Midterm Exam 2\n\nSalmon Version: Blank Solns \nYellow Version: Blank Solns \n\n16: Hypothesis Testing, Part II \n\nExercise 1(b) Solns \n\n\nDiscussion Worksheet 07 (Midterm 2 Review)\nLab 08 Markdown Syntax\n\n\n9\n(5/29 - 6/4)\n\n17: Hypothesis Testing in Multiple Samples \n18: Regression \n\nDiscussion Worksheet 08\nLab 09 Exploring a Dataset\n\n\n10\n(6/05 - 6/11)\n\n19: More Regression, and Sampling Procedures \n20: Review\n\nPost-MT2, Part I \n\nPost-MT2, Part II \n\nProbability Review \n\n\nDiscussion Worksheet 09\nLab 10: Regression\n\n\nFINAL EXAM\nTuesday, June 13 from 4-7pm\n\nList of Topics \n\nSample FR Questions Solns \n\nSample MC Questions Solns \n\nScantron \n\n\n\n\n\n\n\nFinal Grade Distribution: \nQuiz Formula Sheets\nPlease note: quiz sheets will NEVER be uploaded before the quiz takes place.\n\nQz01 \n\nQz02 \n\nQz03 \n\nQz04 \n\nQz05"
  },
  {
    "objectID": "Pages/calendar.html",
    "href": "Pages/calendar.html",
    "title": "PSTAT 5A: Understanding Data",
    "section": "",
    "text": "TA\n\n\nEmail\n\n\nSection Time\n\n\nLocation\n\n\n\n\n\n\nYuan Zhou\n\n\nyuan_zhou@pstat.ucsb.edu\n\n\n10 - 10:50am\n\n\nPsychology East 1805\n\n\n\n\nJason Teng\n\n\njteng@pstat.ucsb.edu\n\n\n11 - 11:50am\n\n\nPhelps 1525\n\n\n\n\nNickolas Thiessen\n\n\nnickolas@ucsb.edu\n\n\n12 - 12:50pm\n\n\nPhelps 1513\n\n\n\n1 - 1:50pm\n\n\nPhelps 1529"
  },
  {
    "objectID": "Pages/calendar.html#visual-weekly-schedule",
    "href": "Pages/calendar.html#visual-weekly-schedule",
    "title": "PSTAT 5A: Understanding Data",
    "section": "Visual Weekly Schedule",
    "text": "Visual Weekly Schedule"
  },
  {
    "objectID": "index2.html",
    "href": "index2.html",
    "title": "PSTAT 5A: Understanding Data",
    "section": "",
    "text": "Welcome to the official course site for PSTAT 5A (titled Understanding Data) at the University of California, Santa Barbara! Please note that this is the site for Lecture Section 200 of the Spring 2023 iteration of the course, with Ethan Marzban. (Lecture Section 100 will be utilizing Canvas.)\n\nAll relevant information for the course can be found on this site, with the (perhaps crucial) exception of quizzes, which will take place on the course Canvas site.\n\nSyllabus\nSchedule of Topics\nSections and Calendar\nCourse Staff / OH\nHW and Solns"
  }
]