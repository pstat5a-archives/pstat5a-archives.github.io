{"title":"PSTAT 5A: Lecture 17","markdown":{"yaml":{"title":"PSTAT 5A: Lecture 17","subtitle":"Testing Across Multiple Populations","author":"Ethan P. Marzban","date":"5/30/23","format":{"revealjs":{"html-math-method":"mathjax","theme":["default","custom.scss"],"incremental":true,"logo":"5a_hex.png","template-partials":["title-slide.html"]}},"editor":"source","title-slide-attributes":{"data-background-image":"5a_hex.png","data-background-size":"contain","data-background-opacity":"0.5","data-background-position":"left"}},"headingText":"Multiple Populations","containsRefs":false,"markdown":"\n\n```{r setup, include=FALSE}\nlibrary(tidyverse)\nlibrary(ggthemes)\n```\n\n\n- So far, we've talked about constructing confidence intervals and performing hypothesis tests for both population proportions and population means.\n\n- One crucial thing to note is that everything we've done has been in the context of a *single* population\n\n- Sometimes, as Data Scientists, we may want to make claims about the *differences* between two populations\n\n    - E.g. Is the average monthly income in Santa Barbara different from the average monthly income in San Francisco?\n    \n    - E.g. Is the proportion of people who test positive for a disease in one country different than the proportion that test positive in a second country?\n\n# Two Populations\n\n## Two Populations\n\n- Statistically: we are imagining *two* populations, Population 1 and Population 2, governed by parameters $\\theta_1$ and $\\theta_2$, respectively, and trying to make claims about the relationship between $\\theta_1$ and $\\theta_2$.\n\n    - For example, we could consider two populations with means $\\mu_1$ and $\\mu_2$, respectively, and try to make claims about whether or not $\\mu_1$ and $\\mu_2$ are equal.\n\n- The trick Statisticians use is to think in terms of the *difference* $\\theta_2 - \\theta_1.$ \n\n    - For example, if our null hypothesis is that $\\theta_1 = \\theta_2$, this can be rephrased as $H_0: \\ \\theta_2 - \\theta_1 = 0$. \n\n\n## Two Populations\n\n- The reason we do this is because we have now effectively reduced our two-parameter problem into a one-parameter problem, involving only the parameter $\\delta := \\theta_2 - \\theta_1$. \n\n- Now, we will need a point estimator of $\\delta$.\n\n- If $\\widehat{\\theta}_1$ and $\\widehat{\\theta}_2$ are point estimators of $\\theta_1$ and $\\theta_2$, respectively, then a natural point estimator of $\\delta$ is $\\widehat{\\delta} = \\widehat{\\theta}_2 - \\widehat{\\theta}_1$.\n\n    - For example, a natural point estimator for the difference $\\mu_2 - \\mu_1$ of *population* means is $\\overline{X}_2 - \\overline{X}_1$, the difference in *sample* means.\n\n\n## Two Populations\n\n- We will ultimately need access to the sampling distribution of $\\widehat{\\delta}$.\n\n- Before delving into that, however, we will need a little more probability knowledge; specifically, knowledge on how linear combinations of random variables work.\n\n\n# Linear Combinations of Random Variables\n\n## Linear Combinations of Random Variables\n\n- Recall, from many weeks ago, that a **random variable** $X$ is simply some numerical variable that tracks a random outcome of an experiment.\n\n    - E.g. number of heads in 10 tosses of a fair coin; number of people in a population that test positive for a disease; etc.\n    \n- A random variable $X$, whether it be **discrete** or **continuous**, has an **expected value** $\\mathbb{E}[X]$ and a **variance** $\\mathrm{Var}(X)$. \n\n- Now, suppose we have two random variables $X$ and $Y$, and three constants $a$, $b$, and $c$.\n\n- Our task for now is to say as much as we can about the quantity $aX + bY + c$.\n\n\n## Linear Combinations of Random Variables\n\n:::{.fragment}\n::: callout-important\n## Theorem\n\n:::{.nonincremental}\n::: {style=\"font-size: 30px\"}\nGiven two random variables $X$ and $Y$, and constants $a, \\ b,$ and $c$, \n$$ \\mathbb{E}[aX + bY + c] = a \\cdot \\mathbb{E}[X] + b \\cdot \\mathbb{E}[Y] + c $$\n:::\n:::\n:::\n:::\n\n- You will prove this in the discrete case on your upcoming homework.\n\n- As an example: if $\\mathbb{E}[X] = 2$ and $\\mathbb{E}[Y] = -1$, then \n$$\\mathbb{E}[2X + 3Y + 1] = 2(2) + 3(-1) + 1 = 2 $$\n\n\n## Linear Combinations of Random Variables\n\n:::{.fragment}\n::: callout-important\n## Theorem\n\n:::{.nonincremental}\n::: {style=\"font-size: 30px\"}\nGiven two **independent** random variables $X$ and $Y$, and constants $a, \\ b,$ and $c$,\n$$ \\mathrm{Var}(aX + bY + c) = a^2 \\mathrm{Var}(X) + b^2 \\mathrm{Var}(Y) $$\n:::\n:::\n:::\n:::\n\n- You will not be responsible for the proof of this fact.\n\n- Also, we haven't explicitly talked about what independence means in the context of random variables; for now, suffice it to say that it works analogously to the concept of independence of events. That is, if the random variables $X$ and $Y$ come from two experiments that don't have any relation to each otehr, then $X$ and $Y$ will be independent.\n\n# Back to our Two-Parameter Problem\n\n## Two Populations\n\n- Alright, so what does this mean in the context of our two-proportion problem?\n\n- Well, for one thing, we can easily construct a confidence interval for $(\\theta_2 - \\theta_1)$ using:\n$$ (\\widehat{\\theta}_2 - \\widehat{\\theta}_1) \\pm c \\cdot \\sqrt{\\mathrm{Var}(\\widehat{\\theta}_1) + \\mathrm{Var}(\\widehat{\\theta}_2)} $$\nwhere $c$ is a constant that is determined by both the sampling distribution of $\\widehat{\\theta}_2 - \\widehat{\\theta}_1$ as well as our confidence level.\n\n- By the way, can anyone tell me why the variances are *added*, and not *subtracted*?\n\n\n## Two Means\n\n- To make things more specific, let's consider comparing two **population means**. \n\n- Specifically: imagine we have two populations (which we will call Population 1 and Population 2), governed by population means $\\mu_1$ and $\\mu_2$, respectively.\n    \n- For now, let's focus a two-sided test, where our hypotheses are\n$$\\left[ \\begin{array}{rr}\n  H_0:    & \\mu_1 = \\mu_2   \\\\\n  H_A:    & \\mu_1 \\neq \\mu_2 \n\\end{array} \\right.$$\n\n\n## Two Means\n\n- Again, it's customary to rephrase things to be in terms of differences:\n$$\\left[ \\begin{array}{rr}\n  H_0:    & \\mu_2 - \\mu_1 = 0   \\\\\n  H_A:    & \\mu_2 - \\mu_1 \\neq 0\n\\end{array} \\right.$$\n\n\n- Now, we need data!\n\n- Suppose we have a sample $X = \\{X_i\\}_{i=1}^{n_1}$ taken from Population 1 and a sample $Y = \\{Y_i\\}_{i=1}^{n_2}$ taken from Population 2.\n    - Note that we are allowing for different sample sizes, $n_1$ and $n_2$!\n\n- Let's also assume that, in addition to being representative samples, the two samples are both independent within themselves and independent from each other (i.e. assume the $X_i$'s and $Y_i$'s are independent, and that the $X$'s are independent from the $Y$'s)\n\n## Two Means\n\n- Again, we are interested in finding a point estimator for $\\mu_2 - \\mu_1$.\n\n- Here's a question: do we have a natural point estimator for $\\mu_2$? What about for $\\mu_1$?\n\n- So, it seems that a natural point estimator for $\\delta = \\mu_2 - \\mu_1$ is\n$$ \\widehat{\\delta} = \\overline{Y} - \\overline{X} $$\n\n- What is the sampling distribution of $\\widehat{\\delta}$?\n\n- Well, there are a few cases to consider.\n\n\n## Sampling Distribution of $\\widehat{\\delta}$ \n\n- Suppose that our two populations had known variances $\\sigma_1^2$ and $\\sigma_2^2$, respectively.\n\n- Then, if both $\\overline{X}$ and $\\overline{Y}$ were normally distributed, we could use a fact (from probability theory) that linear combinations of normally distributed random variables are also normally distributed to conclude that \n$$ \\widehat{\\delta} \\sim \\mathcal{N}\\left( \\delta, \\ \\sqrt{ \\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2} } \\right) $$\n\n    - See the chalkboard for more details\n\n\n\n## The Test Statistc {style=\"font-size:32px\"}\n\n- In this case, a natural candidate for our test statistic would be\n$$ \\frac{\\widehat{\\delta}}{\\sqrt{ \\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}} =  \\frac{\\overline{Y} - \\overline{X}}{\\sqrt{ \\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}} $$\nas, under the null, this would follow a standard normal distribution.\n\n- However, there are a few problems with this.\n\n- For one, it requires both $\\overline{X}$ and $\\overline{Y}$ to be normally distributed, which we know is not always the case.\n\n- Alright, that's fine though- so long as our sample sizes are large enough, the Central Limit Theorem kicks in and we can be reasonably certain that $\\overline{X}$ and $\\overline{Y}$ will be pretty close to normally distributed.\n\n\n## The Test Statistic {style=\"font-size:30px\"}\n\n- However, the main problem in using this test statistic is that it requires access to the population variances $\\sigma_1^2$ and $\\sigma_2^2$!\n\n- Any ideas on how to remedy this?\n\n    - Right; let's just replace the population variances with their sample analogues:\n    $$ \\mathrm{TS} = \\frac{\\overline{Y} - \\overline{X}}{\\sqrt{ \\frac{s_X^2}{n_1} + \\frac{s_Y^2}{n_2}}}$$\n    where\n    \\begin{align*}\n      s_X^2   & = \\frac{1}{n_1 - 1} \\sum_{i=1}^{n_1} (X_i - \\overline{X})^2   \\\\\n      s_Y^2   & = \\frac{1}{n_2 - 1} \\sum_{i=1}^{n_2} (Y_i - \\overline{Y})^2\n    \\end{align*}\n\n\n## The Test Statistic {style=\"font-size:30px\"}\n\n- Any guesses on what distribution this follows under the null?\n\n- If you said _t_..... you'd be wrong! (But pretty close.)\n\n- It turns out that, under the null (i.e. assuming that $\\mu_1 = \\mu_2$, or, equivalently, that $\\delta = \\mu_2 - \\mu_1 = 0$), this test statistic **approximately** follows a _t_-distribution.\n\n- What degrees of freedom?\n\n- That's right:\n$$ \\mathrm{df} = \\mathrm{round}\\left\\{ \\frac{ \\left[ \\left( \\frac{s_X^2}{n_1} \\right) + \\left( \\frac{s_Y^2}{n_2} \\right) \\right]^2 }{ \\frac{\\left( \\frac{s_X^2}{n_1} \\right)^2}{n_1 - 1} + \\frac{\\left( \\frac{s_Y^2}{n_2} \\right)^2}{n_2 - 1} } \\right\\} $$\n\n    - This is related to what is known as the **Satterthwaite Approximation**, sometimes called the **Welch-Satterthwaite Equation**\n\n## The Test {style=\"font-size:30px\"}\n\n- Alright, so we finally have a test statistic:\n$$ \\mathrm{TS} = \\frac{\\overline{Y} - \\overline{X}}{\\sqrt{ \\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}} $$\nand its (approximate) distribution under the null:\n$$ \\mathrm{TS} \\stackrel{H_0}{\\sim} t_{\\nu} $$\nwhere $\\nu$ is given by the Satterthwaite Approximation.\n\n- Recall our hypotheses:\n$$ \\left[ \\begin{array}{rr}\n  H_0:    & \\mu_2 - \\mu_1 = 0   \\\\\n  H_A:    & \\mu_2 - \\mu_1 \\neq 0\n\\end{array} \\right. $$\n\n\n## The Test {style=\"font-size:30px\"}\n\n- We can see that large values of $|\\mathrm{TS}|$ lead credence to the alternative over the null; as such, our decision will take the form\n$$ \\texttt{decision}(\\mathrm{TS}) = \\begin{cases} \\texttt{reject } H_0 & \\text{if } |\\mathrm{TS}| > c \\\\ \\texttt{fail to reject } H_0 & \\text{otherwise}\\\\ \\end{cases}  $$\nwhere $c$ is the appropriately-selected quantile of the appropriate _t_-distribution.\n\n\n\n## Worked-Out Example {style=\"font-size:32px\"}\n\n:::{.fragment}\n::: callout-tip\n## Worked-Out Example 1\n\n:::{.nonincremental}\n::: {style=\"font-size: 28px\"}\n*Gaucho Gourmande* has two locations: one in Goleta and one in Santa Barbara. The owner would like to determine whether the average revenue generated by the two locations are equal or not. To that end, he computes the net revenue generated by the Goleta location over 30 days and also computes the net revenue generated by the Santa Barbara location over 35 days (assume all of the necessary independence conditions hold), and produced the following information:\n\n$$\\begin{array}{r|cc}\n                    & \\text{Sample Average}     & \\text{Sample Standard Deviation}    \\\\\n  \\hline\n  \\textbf{Goleta}   &     \\$13                    & \\$3.45        \\\\\n  \\textbf{Santa Barbara}   &     \\$15                    & \\$4.23\n\\end{array}$$\n\nTest the owner's claims at an $\\alpha = 0.05$ level of significance, using a two-sided alternative.\n\n:::\n:::\n:::\n:::\n\n## Solutions {style=\"font-size:30px\"}\n\n- Our first step should be to figure out what \"Population 1\" and \"Population 2\" are in the context of the problem.\n\n- Let \"Goleta Location\" be Population 1 and \"Santa Barbara Location\" be Population 2.\n    - It is perfectly acceptable to swap these two, but just be sure you remain consistent throughout the problem!\n    - Also, I will expect you to explicitly write out your definitions of the populations (like above), even if the problem doesn't explicitly ask you to do so. \n\n- In this way,\n$$ \\overline{X} = 13; \\quad s_X = 3.45; \\quad \\overline{Y} = 15; \\quad s_Y = 4.23 $$\n\n- Additionally, $n_1 = 30$ and $n_2 = 35$.\n\n## Solutions {style=\"font-size:28px\"}\n\n- Now, let's compute the value of the test statistic.\n$$ \\mathrm{TS} =   \\frac{\\overline{Y} - \\overline{X}}{\\sqrt{ \\frac{s_X^2}{n_1} + \\frac{s_Y^2}{n_2}}} = \\frac{15 - 13}{\\sqrt{\\frac{3.45^2}{30}  + \\frac{4.23^2}{35} }} = 2.10 $$\n\n- We should next figure out the degrees of freedom:\n\\begin{align*}\n  \\mathrm{df}   &  = \\mathrm{round}\\left\\{ \\frac{ \\left[ \\left( \\frac{s_X^2}{n_1} \\right) + \\left( \\frac{s_Y^2}{n_2} \\right) \\right]^2 }{ \\frac{\\left( \\frac{s_X^2}{n_1} \\right)^2}{n_1 - 1} + \\frac{\\left( \\frac{s_Y^2}{n_2} \\right)^2}{n_2 - 1} } \\right\\}     \\\\\n    & =  \\mathrm{round}\\left\\{ \\frac{ \\left[ \\left( \\frac{3.45^2}{30} \\right) + \\left( \\frac{4.23^2}{35} \\right) \\right]^2 }{ \\frac{\\left( \\frac{3.45^2}{30} \\right)^2}{30 - 1} + \\frac{\\left( \\frac{4.23^2}{35} \\right)^2}{35 - 1} } \\right\\} = 63 \n\\end{align*}\n\n## Solutions {style=\"font-size:30px\"}\n\n- At this point, we could either proceed using critical values or using _p_-values.\n\n- Let's use _p_-values, for practice.\n\n- Our _p_-value is computed as\n\n:::{.fragment}\n```{python}\n#| echo: True\n\nimport scipy.stats as sps\n2*sps.t.cdf(-2.10, 63)\n```\n:::\n\n- This is below our level of significance $\\alpha = 0.05$ meaning we would reject the null.\n\n- If we wanted to instead use critical values:\n\n:::{.fragment}\n```{python}\n#| echo: True\n\n-sps.t.ppf(0.05, 63)\n```\n:::\n\n- This means our critical value is 1.67; since $|\\mathrm{TS}| = |2.10| = 2.10 > 1.67$, we would again reject at an $\\alpha = 0.05$ level of significance.\n\n\n## Solutions\n\n> At a 5\\% level of significance, there was sufficient evidence to reject the owner's claims that the revenue generated by the two locations are equal, in favor of the alternative that the revenue generated by the two locations are not equal.\n\n## Extensions {style=\"font-size:32px\"}\n\n- Unsurprisingly, we can adapt the above procedure to account for one-sided alternatives as well.\n\n- For instance, suppose we wish to test\n$$ \\left[ \\begin{array}{rr}\n  H_0:    & \\mu_1 = \\mu_2   \\\\\n  H_A:    & \\mu_1 < \\mu_2\n\\end{array} \\right.$$\n\n- Again, we rephrase things as:\n$$ \\left[ \\begin{array}{rr}\n  H_0:    & \\mu_2 - \\mu_1 = 0   \\\\\n  H_A:    & \\mu_2 - \\mu_1 > 0\n\\end{array} \\right.$$\nwhich is now a familiar upper-tailed test on $\\delta = \\mu_2 - \\mu_1$ and $\\mu_0 = 0.$\n\n## Extensions {style=\"font-size:32px\"}\n\n- Specifically, we would take the same test statistic (which would still follow the same distribution under the null) and use the decision rule\n$$ \\texttt{decision}(\\mathrm{TS}) = \\begin{cases} \\texttt{reject } H_0 & \\text{if } \\mathrm{TS} > c \\\\ \\texttt{fail to reject } H_0 & \\text{otherwise}\\\\ \\end{cases}  $$\nwhere $c$ is the appropriate quantile of the approximate _t_ distribution (with degrees of freedom given by the Satterthwaite Approximation).\n\n# Multiple Populations, and ANOVA\n\n## Leadup\n\n- Consider the following situation: a new drug claims to significantly lower systolic blood pressure.\n\n- To ensure these claims are validated, a clinical trial collects several volunteers and groups them into four groups: a control group, and three groups which each are administered a different dosage of the drug.\n\n- If the drug is truly ineffective, we would imagine the average systolic blood pressure of each group to be fairly similar to the average systolic blood pressures of the other groups.\n\n- In other words, given $k$ groups, each with some population mean $\\mu_i$ (for $i = 1, 2, \\cdots, k$), we wish to determine whether or not all of the $\\mu_i$'s are the same.\n\n\n\n## ANOVA {style=\"font-size:32px\"}\n\n- This is the basic setup of **Analysis of Variance** (often abbreviated as **ANOVA**).\n\n- Given $k$ groups, each with mean $\\mu_i$, we wish to test the null hypothesis that all group means are equal (i.e. $H_0: \\ \\mu_1 = \\mu_2 = \\cdots = \\mu_k$) against the alternative that at least one of the group means differs significantly from the others.\n\n:::{.fragment style=\"font-size:40px\"}\n:::{.callout-caution}\n## **CAUTION**\n\nNote the alternative hypothesis!\n:::\n:::\n\n- It is **NOT** correct to write the alternative as $H_A: \\ \\mu_1 \\neq \\mu_2 \\neq \\cdots \\neq \\mu_k$. \n\n\n\n\n## ANOVA {style=\"font-size:32px\"}\n\n- Here is the general idea.\n\n- Observations within each group will have some amount of variability (by virtue of being *random* observations).\n\n- However, the sample means (of the groups) themselves will also have some variability (again, due to the fact that sample means are random).\n\n- The question ANOVA seeks to answer is: is the variability between sample means greater than what we would expect due to chance alone?\n\n    - If so, we may have reason to believe that at least one of the group means differs significantly from the others; i.e. we would have evidence to reject the null.\n    \n\n## ANOVA {style=\"font-size:30px\"}\n\n- In practice, ANOVA relies on what is known as the **_F_-statistic**.\n\n- The _F_-statistic is computed as\n$$ F = \\frac{\\mathrm{MS}_\\mathrm{G}}{\\mathrm{MS}_{\\mathrm{E}}} $$ \n\n    - $\\mathrm{MS}_{\\mathrm{G}}$ can be thought of as a measure of variability *between groups*; i.e. as a sort of variance of the sample means\n    \n    - $\\mathrm{MS}_{\\mathrm{E}}$ can be thought of as a measure of variability *within groups*; i.e. as a sort of variance due to error/randomness.\n    \n- As stated on the previous slide, when $\\mathrm{MS}_{\\mathrm{G}}$ is much larger than $\\mathrm{MS}_{\\mathrm{E}}$, i.e. when the variability between groups is much larger than the variability within groups, we would be more likely to reject the null that all group means are equal.\n\n    - Hence, we would reject $H_0$ for large values of $F$.\n    \n\n## ANOVA {style=\"font-size:32px\"}\n\n- The formulas for computing $\\mathrm{MS}_{\\mathrm{G}}$ and $\\mathrm{MS}_{\\mathrm{E}}$ are not overly complicated, but can be a bit tedious.\n\n    - We will return to them later.\n    \n- For now, let's talk a bit about the sampling distribution of $F$.\n\n- It turns out that, if we assume observations within each group are normally distributed (which ends up being a **very crucial** assumption), the statistic $F$ follows what is known as the **_F_-distribution**.\n\n- As such, the critical value of our test is the appropriate percentile of the _F_-distribution.\n\n\n\n## The _F_-Distribution {style=\"font-size:32px\"}\n\n- The _F_-distribution is quite different from the distributions we have encountered thus far.\n\n- For one thing, it admits only nonnegative values in its state space (i.e. it has state space $[0, \\infty)$).\n\n- Additionally, it takes two parameters, referred to as the **numerator degrees of freedom** and the **denominator degrees of freedom** (sometimes abbreviated as just \"degree of freedom 1\" and \"degree of freedom 2\".)\n\n- To notate the fact that a random variable $X$ follows the _F_-distribution with degrees of freedom `d1` and `d2`, respectively, we write\n$$ X \\sim F_{\\texttt{d1}, \\ \\texttt{d2}} $$\n\n\n\n\n## The _F_-Distribution {style=\"font-size:32px\"}\n\n\n```{r}\nlibrary(tidyverse)\ndata.frame(x = seq(0, 6, by = 0.01)) %>%\n  ggplot(aes(x = x)) +\n  stat_function(fun = df,\n                args = list(2, 2),\n                linewidth = 1,\n                aes(colour = \"2, 2\")) +\n  stat_function(fun = df,\n                args = list(3, 2),\n                linewidth = 1,\n                aes(colour = \"3, 2\"),\n                n = 200) +\n  stat_function(fun = df,\n                args = list(3, 10),\n                linewidth = 1,\n                aes(colour = \"3, 10\"),\n                n = 200) +\n  stat_function(fun = df,\n                args = list(20, 100),\n                linewidth = 1,\n                aes(colour = \"20, 100\"),\n                n = 200) +\n  theme_economist_white() +\n  theme(\n    panel.background = element_rect(\"#f0ebd8\"),\n    plot.background = element_rect(fill = \"#f0ebd8\")\n  ) +\n  xlab(\"\") +\n  ylab(\"\") +\n  labs(colour = \"df1, df2 = \")\n```\n\n\n\n\n## The Test Statistic {style=\"font-size:32px\"}\n\n- Recall that our test statistic in ANOVA is\n$$ F = \\frac{\\mathrm{MS}_\\mathrm{G}}{\\mathrm{MS}_{\\mathrm{E}}} $$ \n\n- As mentioned previously, if we assume normality within groups, then, under the null, $F$ follows the _F_-distribution with $k - 1$ and $n - k$ degrees of freedom, respectively, where $k$ is the number of groups and $n$ is the combined number of observations:\n$$ F = \\frac{\\mathrm{MS}_\\mathrm{G}}{\\mathrm{MS}_{\\mathrm{E}}}  \\stackrel{H_0}{\\sim} F_{k - 1, \\ n - k} $$\n\n- Since we reject only for large values of $F$, our _p_-values are always computed as upper-tail probabilities:\n\n\n\n\n\n\n## _p_-Values in ANOVA {style=\"font-size:32px\"}\n\n\n```{r}\nlibrary(tidyverse)\ndata.frame(x = seq(0, 6, by = 0.01)) %>%\n  ggplot(aes(x = x)) +\n  stat_function(fun = df,\n                args = list(3, 10),\n                xlim = c(2, 6),\n                geom = \"area\",\n                fill = \"#a4caeb\",\n                n = 300) +\n  stat_function(fun = df,\n                args = list(3, 10),\n                linewidth = 1,\n                n = 300) +\n  theme_economist_white() +\n  theme(\n    panel.background = element_rect(\"#f0ebd8\"),\n    plot.background = element_rect(fill = \"#f0ebd8\")\n  ) +\n  xlab(\"\") +\n  ylab(\"\")\n```\n\n\n\n## ANOVA Tables {style=\"font-size:30px\"}\n\n- As mentioned previously, computing $\\mathrm{MS}_{\\mathrm{G}}$ and $\\mathrm{MS}_{\\mathrm{E}}$ is not particularly challenging, but it can be quite tedious.\n\n- As such, computer software is usually utilized to carry out an ANOVA.\n\n- Often times, the results of such a computer-generated ANOVA are displayed in what is known as an **ANOVA Table**.\n\n- I find ANOVA tables to be best described by way of an example.\n\n## Example {style=\"font-size:30px\"}\n\n:::{.fragment}\n::: callout-tip\n## Reference Example 1\n\n:::{.nonincremental}\n::: {style=\"font-size: 28px\"}\nA state official would like to determine whether or not the average fluoride levels in the water supplies of Cities A, B, and C are the same.\n\nTo that end, they took a sample of 100 fluoride measurements from city A, 110 from city B, and 100 from city C.\n\n:::\n:::\n:::\n:::\n\n- After running an ANOVA in a computer software, the following output was produced:\n\n:::{.fragment style=\"font-size:28px\"}\n|  | **DF** | **Sum Sq** | **Mean Sq** | **F value** | **Pr(>F)** |\n|:-------:|:-------:|:-------:|:-------:|:-------:|:-------:|\n| Between Groups | 2\t| 0.541799\t| 0.2709\t| 1.30682497808\t | 0.272179497817 |\n| Residuals | 307\t| 63.6399\t| 0.207296\t|\n:::\n\n- Let's go through this table in more detail.\n\n## Interpreting an ANOVA Table {style=\"font-size:28px\"}\n\n\n:::{.fragment style=\"font-size:28px\"}\n|  | **DF** | **Sum Sq** | **Mean Sq** | **F value** | **Pr(>F)** |\n|:-------:|:-------:|:-------:|:-------:|:-------:|:-------:|\n| Between Groups | 2\t| 0.343981\t| 0.171991\t| 0.927001041587\t | 0.396843557892 |\n| Residuals | 307\t| 56.9591\t| 0.185534\t|\n:::\n\n\\\n\n- The **DF** column gives the degrees of freedom of the resulting _F_-statistic.\n\n    - Recall that these are meant to be $k - 1$ and $n - k$ respectively.\n    - $k$ is the number of groups (i.e. 3, in this example), hence the numerator d.f. of 2.\n    - $n$ is the total number of observations (i.e. 100 + 110 + 100 = 310, in this example), hence the denominator d.f. of 307 (310 - 3 = 307).\n    \n    \n- The rownames (\"Between Groups\" and \"Residuals\") refer to whether the specified entry is in relation to a *between group* calculation or a *within group* calculation. \n\n    - The reason for calling the second row \"Residuals\" instead of \"Within Group\" will become clear next week, after we talk about Regression.\n    \n## Interpreting an ANOVA Table {style=\"font-size:28px\"}\n\n|  | **DF** | **Sum Sq** | **Mean Sq** | **F value** | **Pr(>F)** |\n|:-------:|:-------:|:-------:|:-------:|:-------:|:-------:|\n| Between Groups | 2\t| 0.343981\t| 0.171991\t| 0.927001041587\t | 0.396843557892 |\n| Residuals | 307\t| 56.9591\t| 0.185534\t|\n\n\n\n\\\n\n- The **Sum Sq** column is a scaled version of the $\\mathrm{MS}_{\\mathrm{G}}$ and $\\mathrm{MS}_{\\mathrm{E}}$ quantities.\n\n- Don't worry too much about how those were computed for now.\n\n- The **Mean Sq** entries are found by dividing the corresponding **Sum Sq** entry by the corresponding degree of freedom.\n\n    - That is: 0.171991 = 0.343981 / 2\n    - And: 0.185534 = 56.9591 / 307\n    \n- Finally, the **F value** is simply the ration of the two **Mean Sq** values, and represents the value of our test statistic.\n\n    - The **Pr(>F)** is just our _p_-value.\n    \n## Analyzing the Data {style=\"font-size:30px\"}\n\n- Maybe that was a little too opaque.\n\n- If you'd like, there is the actual data:\n\n:::{.fragment}\n```{r}\nx1 <- read.csv(\"x1.csv\", col.names = F)\nx1 <- c(1.765793252, as.numeric(t(x1)))\n\nx2 <- read.csv(\"x2.csv\", col.names = F)\nx2 <- c(1.096990264, as.numeric(t(x2)))\n\nx3 <- read.csv(\"x3.csv\", col.names = F)\nx3 <- c(0.8893372313, as.numeric(t(x3)))\n\ndata.frame(A = c(x1, rep(\"NA\", 10)),\n  B = c(x2),\n  C = c(x3, rep(\"NA\", 10))\n)\n```\n:::\n\n## Analyzing the Data {style=\"font-size:30px\"}\n\n- Whoops- maybe that's too detailed.\n\n- Any ideas on how we might be able to get a better sense of the data that doesn't involve looking at all those numbers?\n\n- Maybe... something we learned in Week 1?\n\n:::{.fragment}\n```{r, fig.height = 4}\ndf <- data.frame(\n  fluoride = c(x1, x2, x3),\n  city = as.factor(c(rep(\"A\", 100), rep(\"B\", 110), rep(\"C\", 100)))\n)\n\n\ndf %>%\n  ggplot(aes(x = city, y = fluoride)) +\n  stat_boxplot(geom = \"errorbar\", \n               width = 0.25,\n               linewidth = 1) +\n  geom_boxplot(fill =  \"#7f9ab5\", \n               size = 1,\n               outlier.size = 4) +\n  theme_economist(base_size = 18) +\n  ggtitle(\"Fluoride Levels Across Cities\") +\n  theme(panel.background = element_rect(\"#f0ebd8\"),\n        plot.background = element_rect(fill = \"#f0ebd8\"),\n        axis.title.y = element_text(size = 16,\n                                    margin = margin(\n                                      t = 0, \n                                      r = 10,\n                                      b = 0, \n                                      l = 0)),\n        axis.title.x = element_text(size = 16,\n                                    margin = margin(\n                                      t = 10, \n                                      r = 0,\n                                      b = 0, \n                                      l = 0)),\n        title = element_text(size = 18,\n                                    margin = margin(\n                                      t = 0, \n                                      r = 0,\n                                      b = 10, \n                                      l = 0))\n  ) \n\n```\n:::\n\n## Checking Assumptions {style=\"font-size:30px\"}\n\n- Finally, I should mention: **every good statistician and data scientists starts by checking assumptions**.\n\n- One of the key assumptions in ANOVA is that observations within each group are normally distributed.\n\n- How can we check that?\n\n    - That's right: QQ-plots!\n\n- In lab, you'll begin to start talking about how to start statistical analyses. \n\n    - Specifically, you will learn about something called **Exploratory Data Analysis** (EDA), part of which entails producing any diagnostic tools you may need to produce in order to ensure assumptions are being satisfied!\n    \n## QQ-Plots for the Fluoride Dataset\n\n\\\n\n```{r, message = F}\nlibrary(gridExtra)\np1 <- data.frame(x1) %>% ggplot(aes(sample = x1)) +\n  geom_qq(size = 3) +\n  geom_qq_line() +\n  theme_economist(base_size = 14) +\n  theme(panel.background = element_rect(\"#f0ebd8\"),\n        plot.background = element_rect(fill = \"#f0ebd8\"),\n        axis.title.y = element_text(size = 16,\n                                    margin = margin(\n                                      t = 0, \n                                      r = 10,\n                                      b = 0, \n                                      l = 0)),\n        axis.title.x = element_text(size = 16,\n                                    margin = margin(\n                                      t = 10, \n                                      r = 0,\n                                      b = 0, \n                                      l = 0)),\n        title = element_text(size = 18,\n                                    margin = margin(\n                                      t = 0, \n                                      r = 0,\n                                      b = 10, \n                                      l = 0))\n  ) +\n  xlab(\"theoretical quantiles\") +\n  ylab(\"sample quantiles\") +\n  ggtitle(\"QQ-Plot; A\")\n\np2 <- data.frame(x2) %>% ggplot(aes(sample = x2)) +\n  geom_qq(size = 3) +\n  geom_qq_line() +\n  theme_economist(base_size = 14) +\n  theme(panel.background = element_rect(\"#f0ebd8\"),\n        plot.background = element_rect(fill = \"#f0ebd8\"),\n        axis.title.y = element_text(size = 16,\n                                    margin = margin(\n                                      t = 0, \n                                      r = 10,\n                                      b = 0, \n                                      l = 0)),\n        axis.title.x = element_text(size = 16,\n                                    margin = margin(\n                                      t = 10, \n                                      r = 0,\n                                      b = 0, \n                                      l = 0)),\n        title = element_text(size = 18,\n                                    margin = margin(\n                                      t = 0, \n                                      r = 0,\n                                      b = 10, \n                                      l = 0))\n  ) +\n  xlab(\"theoretical quantiles\") +\n  ylab(\"sample quantiles\") +\n  ggtitle(\"QQ-Plot; B\")\n\np3 <- data.frame(x3) %>% ggplot(aes(sample = x3)) +\n  geom_qq(size = 3) +\n  geom_qq_line() +\n  theme_economist(base_size = 14) +\n  theme(panel.background = element_rect(\"#f0ebd8\"),\n        plot.background = element_rect(fill = \"#f0ebd8\"),\n        axis.title.y = element_text(size = 16,\n                                    margin = margin(\n                                      t = 0, \n                                      r = 10,\n                                      b = 0, \n                                      l = 0)),\n        axis.title.x = element_text(size = 16,\n                                    margin = margin(\n                                      t = 10, \n                                      r = 0,\n                                      b = 0, \n                                      l = 0)),\n        title = element_text(size = 18,\n                                    margin = margin(\n                                      t = 0, \n                                      r = 0,\n                                      b = 10, \n                                      l = 0))\n  ) +\n  xlab(\"theoretical quantiles\") +\n  ylab(\"sample quantiles\") +\n  ggtitle(\"QQ-Plot; C\")\n\ngrid.arrange(p1, p2, p3, ncol = 3)\n```"},"formats":{"revealjs":{"execute":{"fig-width":10,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":false,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","html-math-method":"mathjax","slide-level":2,"to":"revealjs","incremental":true,"output-file":"Lec17.html"},"language":{},"metadata":{"lang":"en","fig-responsive":false,"quarto-version":"1.2.335","auto-stretch":true,"title":"PSTAT 5A: Lecture 17","subtitle":"Testing Across Multiple Populations","author":"Ethan P. Marzban","date":"5/30/23","editor":"source","title-slide-attributes":{"data-background-image":"5a_hex.png","data-background-size":"contain","data-background-opacity":"0.5","data-background-position":"left"},"theme":["default","custom.scss"],"logo":"5a_hex.png","template-partials":["title-slide.html"]}}}}