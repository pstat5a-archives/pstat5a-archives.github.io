{"title":"PSTAT 5A: Lecture 18","markdown":{"yaml":{"title":"PSTAT 5A: Lecture 18","subtitle":"Correlation, and an Intro to Regression","author":"Ethan P. Marzban","date":"6/1/23","format":{"revealjs":{"html-math-method":"mathjax","theme":["default","custom.scss"],"incremental":true,"logo":"5a_hex.png","template-partials":["title-slide.html"]}},"editor":"source","title-slide-attributes":{"data-background-image":"5a_hex.png","data-background-size":"contain","data-background-opacity":"0.5","data-background-position":"left"}},"headingText":"Review of Relevant Lecture 02 Material","containsRefs":false,"markdown":"\n\n```{r setup, include=FALSE}\nlibrary(tidyverse)\nlibrary(ggthemes)\n```\n\n\n## Scatterplots and Trends\n\n- Recall, from Lecture 2, that the best type of plot to visualize the relationship between to numerical variables is a **scatterplot**.\n\n- Based on the scatterplot, we can determine whether or not the two variables have an **association** (a.k.a. **trend**) or not.\n\n- Associations can be positive or negative, and linear or nonlinear (or, not present at all)\n\n---\n\n\n:::: {.columns}\n\n:::{.column width=\"50%\"} \n- Linear **Negative** Association:\n\n:::{.fragment}\n```{r}\nset.seed(123)\nx <- rnorm(100)\ny <- -2 * x + rnorm(100, 0, 2)\n\ndata.frame(x, y) %>%\n  ggplot(aes(x = x, y = y)) +\n  geom_point(size = 4) +\n  theme_economist(base_size = 24) +\n  ggtitle(\"Y vs. X\") +\n  theme(panel.background = element_rect(\"#f0ebd8\"),\n        plot.background = element_rect(fill = \"#f0ebd8\"),\n        axis.title.y = element_text(size = 16,\n                                    margin = margin(\n                                      t = 0, \n                                      r = 10,\n                                      b = 0, \n                                      l = 0)),\n        axis.title.x = element_text(size = 16,\n                                    margin = margin(\n                                      t = 10, \n                                      r = 0,\n                                      b = 0, \n                                      l = 0)),\n        title = element_text(size = 18)\n  )\n```\n:::\n:::\n\n:::{.column width=\"50%\"}\n- Nonlinear **Negative** Association:\n\n:::{.fragment}\n```{r}\nset.seed(123)\nx <- rchisq(100, 20)\ny <- (1 / x^2) + rnorm(100, 0, 0.0005)\n\ndata.frame(x, y) %>%\n  ggplot(aes(x = x, y = y)) +\n  geom_point(size = 4) +\n  theme_economist(base_size = 24) +\n  ggtitle(\"Y vs. X\") +\n  theme(panel.background = element_rect(\"#f0ebd8\"),\n        plot.background = element_rect(fill = \"#f0ebd8\"),\n        axis.title.y = element_text(size = 16,\n                                    margin = margin(\n                                      t = 0, \n                                      r = 10,\n                                      b = 0, \n                                      l = 0)),\n        axis.title.x = element_text(size = 16,\n                                    margin = margin(\n                                      t = 10, \n                                      r = 0,\n                                      b = 0, \n                                      l = 0)),\n        title = element_text(size = 18)\n  )\n```\n:::\n:::\n\n::::\n\n\n:::: {.columns}\n\n:::{.column width=\"50%\"} \n- Linear **Positive** Association:\n\n:::{.fragment}\n```{r}\nset.seed(123)\nx <- rnorm(100)\ny <- 2 * x + rnorm(100, 0, 2)\n\ndata.frame(x, y) %>%\n  ggplot(aes(x = x, y = y)) +\n  geom_point(size = 4) +\n  theme_economist(base_size = 24) +\n  ggtitle(\"Y vs. X\") +\n  theme(panel.background = element_rect(\"#f0ebd8\"),\n        plot.background = element_rect(fill = \"#f0ebd8\"),\n        axis.title.y = element_text(size = 16,\n                                    margin = margin(\n                                      t = 0, \n                                      r = 10,\n                                      b = 0, \n                                      l = 0)),\n        axis.title.x = element_text(size = 16,\n                                    margin = margin(\n                                      t = 10, \n                                      r = 0,\n                                      b = 0, \n                                      l = 0)),\n        title = element_text(size = 18)\n  )\n```\n:::\n:::\n\n:::{.column width=\"50%\"} \n- Nonlinear **Positive** Association:\n\n:::{.fragment}\n```{r}\nset.seed(123)\nx <- rchisq(100, 20)\ny <- 1 - (1/x^2)  + rnorm(100, 0, 0.0007)\n\ndata.frame(x, y) %>%\n  ggplot(aes(x = x, y = y)) +\n  geom_point(size = 4) +\n  theme_economist(base_size = 24) +\n  ggtitle(\"Y vs. X\") +\n  theme(panel.background = element_rect(\"#f0ebd8\"),\n        plot.background = element_rect(fill = \"#f0ebd8\"),\n        axis.title.y = element_text(size = 16,\n                                    margin = margin(\n                                      t = 0, \n                                      r = 10,\n                                      b = 0, \n                                      l = 0)),\n        axis.title.x = element_text(size = 16,\n                                    margin = margin(\n                                      t = 10, \n                                      r = 0,\n                                      b = 0, \n                                      l = 0)),\n        title = element_text(size = 18)\n  )\n```\n:::\n:::\n\n::::\n\n## No Relationship\n\n- Sometimes, two variables will have no relationship at all:\n\n:::{.fragment}\n```{r}\nset.seed(123)\nx <- rchisq(100, 20)\ny <- rchisq(100, 20)\n\ndata.frame(x, y) %>%\n  ggplot(aes(x = x, y = y)) +\n  geom_point(size = 4) +\n  theme_economist(base_size = 24) +\n  ggtitle(\"Y vs. X\") +\n  theme(panel.background = element_rect(\"#f0ebd8\"),\n        plot.background = element_rect(fill = \"#f0ebd8\"),\n        axis.title.y = element_text(size = 16,\n                                    margin = margin(\n                                      t = 0, \n                                      r = 10,\n                                      b = 0, \n                                      l = 0)),\n        axis.title.x = element_text(size = 16,\n                                    margin = margin(\n                                      t = 10, \n                                      r = 0,\n                                      b = 0, \n                                      l = 0)),\n        title = element_text(size = 18)\n  )\n```\n:::\n\n## Strength of a Relationship\n\n- There is another thing to be aware of.\n\n- For example, consider the following two scatterplots:\n\n\n\n:::: {.columns}\n\n:::{.column width=\"50%\"} \n:::{.fragment}\n```{r}\nset.seed(123)\nx <- rnorm(100)\ny <- 2 * x + rnorm(100, 0, 2.1)\n\ndata.frame(x, y) %>%\n  ggplot(aes(x = x, y = y)) +\n  geom_point(size = 4) +\n  theme_economist(base_size = 24) +\n  ggtitle(\"Y1 vs. X1\") +\n  theme(panel.background = element_rect(\"#f0ebd8\"),\n        plot.background = element_rect(fill = \"#f0ebd8\"),\n        axis.title.y = element_text(size = 16,\n                                    margin = margin(\n                                      t = 0, \n                                      r = 10,\n                                      b = 0, \n                                      l = 0)),\n        axis.title.x = element_text(size = 16,\n                                    margin = margin(\n                                      t = 10, \n                                      r = 0,\n                                      b = 0, \n                                      l = 0)),\n        title = element_text(size = 18)\n  )\n```\n:::\n:::\n\n:::{.column width=\"50%\"} \n:::{.fragment}\n```{r}\nset.seed(123)\nx <- rnorm(100)\ny <- 2 * x + rnorm(100, 0, 0.5)\n\ndata.frame(x, y) %>%\n  ggplot(aes(x = x, y = y)) +\n  geom_point(size = 4) +\n  theme_economist(base_size = 24) +\n  ggtitle(\"Y2 vs. X2\") +\n  theme(panel.background = element_rect(\"#f0ebd8\"),\n        plot.background = element_rect(fill = \"#f0ebd8\"),\n        axis.title.y = element_text(size = 16,\n                                    margin = margin(\n                                      t = 0, \n                                      r = 10,\n                                      b = 0, \n                                      l = 0)),\n        axis.title.x = element_text(size = 16,\n                                    margin = margin(\n                                      t = 10, \n                                      r = 0,\n                                      b = 0, \n                                      l = 0)),\n        title = element_text(size = 18)\n  )\n```\n:::\n:::\n\n::::\n\n- Both scatterplots display a positive linear trend. However, the relationship between `Y2` and `X2` seems to be \"stronger\" than the relationship between `Y1` and `X1`, does it not?\n\n\n## Correlation Coefficient\n\n- Ultimately, we would like to develop a mathematical metric to quantify not only the relationship between two variables, but also the *strength* of the relationship between these two variables.\n\n- This quantity is referred to as the **correlation coefficient**.\n\n- Now, it turns out there are actually a few *different* correlation coefficients out there. The one we will use in this class (and one of the metrics that is very widely used by statisticians) is called **Pearson's Correlation Coefficient**, or often just **Pearson's _r_** (as we use the letter _r_ to denote it.)\n\n## Pearson's _r_\n\n- Given two sets $X = \\{y_i\\}_{i=1}^{n}$ and $Y = \\{y_i\\}_{i=1}^{n}$ (note that we require the two sets to have the same number of elements!), we compute _r_ using the formula\n$$ r = \\frac{1}{n - 1} \\sum_{i=1}^{n} \\left( \\frac{x_i - \\overline{x}}{s_X} \\right) \\left( \\frac{y_i - \\overline{y}}{s_Y} \\right)  $$\nwhere:\n\n    - $\\overline{x}$ and $\\overline{y}$ denote the sample means of $X$ and $Y$, respectively\n    - $s_X$ and $s_Y$ denote the sample standard deviations of $X$ and $Y$, respectively.\n    \n## Example\n\n- I find it useful to sometimes consider extreme cases, and ensure that the math matches up with our intuition.\n\n- For example, consider the sets $X = \\{1, 2, 3\\}$ and $Y = \\{1, 2, 3\\}$.\n\n- From a scatterplot, I think we would all agree that $X$ and $Y$ have a positive linear relationship, and that the relationship is very strong!\n\n## Example\n\n- Indeed, $\\overline{x} = 2 = \\overline{y}$ and $s_X = 1 = s_Y$, meaning\n\\begin{align*}\n  r   & = \\frac{1}{3 - 1} \\left[ \\left( \\frac{1 - 2}{1} \\right) \\left( \\frac{1 - 2}{1} \\right)  + \\left( \\frac{2 - 2}{1} \\right) \\left( \\frac{2 - 2}{1} \\right)  \\right.   \\\\\n    & \\hspace{45mm} \\left. +  \\left( \\frac{3 - 2}{1} \\right) \\left( \\frac{3 - 2}{1} \\right)  \\right] \\\\\n    & = \\frac{1}{2} \\left[ 1 + 0 + 1 \\right] = \\boxed{1}\n\\end{align*}\n\n- It turns out, _r_ will always be between $-1$ and $1$, inclusive, regardless of what two sets we are comparing!\n\n## Interpretation\n\n- So, here is how we interpret the value of _r_.\n\n    - The sign of _r_ (i.e. whether it is positive or negative) indicates whether or not the linear association between the two variables is positive or negative.\n    \n    - The magnitude of _r_ indicates how strong the linear relationship between the two variables is, with magnitudes close to $1$ or $-1$ indicating very strong linear relationships.\n    \n    - An _r_ value of 0 indicates no linear relationship between the variables.\n    \n## Important Distinction\n\n- Now, something that is **very** important to mention is that _r_ only quantifies *linear* relationships- it is very bad at quantifying nonlinear relationships. \n\n- For example, consider the following scatterplot:\n\n:::{.fragment}\n```{r}\nset.seed(123)\nx <- rnorm(200)\ny <- x^2 + rnorm(100, 0, 0.5)\n\ndata.frame(x, y) %>%\n  ggplot(aes(x = x, y = y)) +\n  geom_point(size = 4) +\n  theme_economist(base_size = 24) +\n  ggtitle(\"Y vs. X\") +\n  theme(panel.background = element_rect(\"#f0ebd8\"),\n        plot.background = element_rect(fill = \"#f0ebd8\"),\n        axis.title.y = element_text(size = 16,\n                                    margin = margin(\n                                      t = 0, \n                                      r = 10,\n                                      b = 0, \n                                      l = 0)),\n        axis.title.x = element_text(size = 16,\n                                    margin = margin(\n                                      t = 10, \n                                      r = 0,\n                                      b = 0, \n                                      l = 0)),\n        title = element_text(size = 18)\n  )\n```\n:::\n\n## Important Distinction\n\n- I think we would all agree that `Y` and `X` have a fairly strong relationship.\n\n- However, the correlation between `Y` and `X` is actually only `r cor(x, y)`!\n\n- So, again- _r_ should only be used as a determination of the strength of *linear* trends, not *nonlinear* trends.\n\n## Your Turn!\n\n\n## Your Turn!\n\n:::{.fragment}\n::: callout-tip\n## Exercise 1\n\n:::{.nonincremental}\n::: {style=\"font-size: 30px\"}\nCompute the correlation between the following two sets of numbers:\n\\begin{align*}\n  \\boldsymbol{x}    & = \\{-1, \\ 0, \\ 1\\}    \\\\\n  \\boldsymbol{y}    & = \\{1, \\ 2, \\ 0\\}\n\\end{align*}\n:::\n:::\n:::\n:::\n\n## Leadup\n\n- There is another thing to note about correlation.\n\n- Let's see this by way of an example: consider the following two scatterplots:\n\n\n:::: {.columns}\n\n:::{.column width=\"50%\"} \n:::{.fragment}\n```{r}\nx <- c(1, 2, 3)\ny1 <- c(1, 2, 3)\ny2 <- c(1, 3, 5)\n\ndata.frame(x, y1) %>%\n  ggplot(aes(x = x, y = y1)) +\n  geom_point(size = 4) +\n  theme_economist(base_size = 24) +\n  ggtitle(\"Y1 vs. X\") +\n  theme(panel.background = element_rect(\"#f0ebd8\"),\n        plot.background = element_rect(fill = \"#f0ebd8\"),\n        axis.title.y = element_text(size = 16,\n                                    margin = margin(\n                                      t = 0, \n                                      r = 10,\n                                      b = 0, \n                                      l = 0)),\n        axis.title.x = element_text(size = 16,\n                                    margin = margin(\n                                      t = 10, \n                                      r = 0,\n                                      b = 0, \n                                      l = 0)),\n        title = element_text(size = 18)\n  ) +\n  ylim(c(1, 5))\n```\n:::\n:::\n\n:::{.column width=\"50%\"} \n:::{.fragment}\n```{r}\nx <- c(1, 2, 3)\ny1 <- c(1, 2, 3)\ny2 <- c(1, 3, 5)\n\ndata.frame(x, y1) %>%\n  ggplot(aes(x = x, y = y2)) +\n  geom_point(size = 4) +\n  theme_economist(base_size = 24) +\n  ggtitle(\"Y2 vs. X\") +\n  theme(panel.background = element_rect(\"#f0ebd8\"),\n        plot.background = element_rect(fill = \"#f0ebd8\"),\n        axis.title.y = element_text(size = 16,\n                                    margin = margin(\n                                      t = 0, \n                                      r = 10,\n                                      b = 0, \n                                      l = 0)),\n        axis.title.x = element_text(size = 16,\n                                    margin = margin(\n                                      t = 10, \n                                      r = 0,\n                                      b = 0, \n                                      l = 0)),\n        title = element_text(size = 18)\n  ) +\n  ylim(c(1, 5))\n```\n:::\n:::\n\n::::\n\n\n- Both cor(`X`, `Y1`) and cor(`X`, `Y2`) are equal to 1, despite the fact that a one unit increase in `x` corresponds to a different unit increase in `y1` as opposed to `y2`.\n\n## Leadup\n\n- So, don't be fooled- the magnitude of _r_ says nothing about how a one-unit increase in `x` translates to a change in `y`!\n\n    - Again, the magnitude of _r_ only tells us how *strongly* the two variables are related.\n    \n- A natural question that arises is then: how *can* we specify how a change in `x` translates to a change in `y`?\n\n\n## Leadup {style=\"font-size:30px\"}\n\n- To help ground our discussion, let's think in terms of height and weight. That is, let `x` denote `height` and `y` denote `weight`.\n\n- We would certainly expect some sort of positive association between `height` and `weight` (taller people tend to weigh slightly more than shorter people).\n\n- But, if we were to take a series of observations on `height` and `weight`, and plot these observations on a scatterplot, we would *not* get data that is perfectly linear.\n\n- Rather, we can imagine that there does exist some true linear \"fit\" (or \"trend\") between `height` and `weight`, but randomness would inject some \"error\" into our data causing our data to be modeled as something like\n$$ \\texttt{weight} = f(\\texttt{height}) + \\texttt{noise} $$\nwhere, in this case, we would expect the function $f()$ to be a linear function.\n\n## Leadup {style=\"font-size:30px\"}\n\n```{r}\nset.seed(123)\n\nx <- rnorm(100, 180, 10)\ny <- x + rnorm(100, 0, 4)\n\ndata.frame(x, y) %>%\n  ggplot(aes(x = x, y = y)) +\n  geom_point(size = 4) +\n  theme_economist(base_size = 24) +\n  ggtitle(\"Weight vs. Height\") +\n  theme(panel.background = element_rect(\"#f0ebd8\"),\n        plot.background = element_rect(fill = \"#f0ebd8\"),\n        axis.title.y = element_text(size = 16,\n                                    margin = margin(\n                                      t = 0, \n                                      r = 10,\n                                      b = 0, \n                                      l = 0)),\n        axis.title.x = element_text(size = 16,\n                                    margin = margin(\n                                      t = 10, \n                                      r = 0,\n                                      b = 0, \n                                      l = 0)),\n        title = element_text(size = 18)\n  ) +\n  ylab(\"weight (lbs)\") +\n  xlab(\"height (cm)\")\n```\n\n\n\n## Leadup {style=\"font-size:30px\"}\n\n```{r}\nset.seed(123)\n\ndata.frame(x, y) %>%\n  ggplot(aes(x = x, y = y)) +\n  geom_point(size = 4) +\n  theme_economist(base_size = 24) +\n  ggtitle(\"Weight vs. Height\") +\n  theme(panel.background = element_rect(\"#f0ebd8\"),\n        plot.background = element_rect(fill = \"#f0ebd8\"),\n        axis.title.y = element_text(size = 16,\n                                    margin = margin(\n                                      t = 0, \n                                      r = 10,\n                                      b = 0, \n                                      l = 0)),\n        axis.title.x = element_text(size = 16,\n                                    margin = margin(\n                                      t = 10, \n                                      r = 0,\n                                      b = 0, \n                                      l = 0)),\n        title = element_text(size = 18)\n  ) +\n  ylab(\"weight (lbs)\") +\n  xlab(\"height (cm)\") +\n  geom_abline(slope = 1, intercept = 0, col = \"red\", linewidth = 1)\n```\n\n- Here, the red line represents the true relationship between `height` and `weight`, and any deviations from the line are assumed to be due to chance.\n\n\n## Leadup {style=\"font-size:30px\"}\n\n- Recall that a line is specified by an intercept and a slope. Therefore, since we are assuming a linear relationship between `height` and `weight`, our model can be expressed as\n$$ \\texttt{weight} = \\beta_0 + \\beta_1 \\cdot \\texttt{height} + \\texttt{noise} $$ \n\n- In this way, we can see that $\\beta_0$ and $\\beta_1$ are effectively *population parameters*.\n\n- Our next goal will be to find suitable estimators, $\\widehat{\\beta_0}$ and $\\widehat{\\beta_1}$, of $\\beta_0$ and $\\beta_1$, respectively.\n\n# Simple Linear Regression\n\n## Model\n\n- In general, the goal of **regression** is to quantify the relationship between two variables, `x` and `y`.\n\n- We call `y` the **response variable** and `x` the **explanatory variable**. \n\n    - So, for example, in our `height` and `weight` example from above, `weight` was the response variable and `height` was the explanatory variable.\n    \n- Our model (assuming a linear relationship between `x` and `y`), is then\n$$ \\texttt{y} = \\beta_0 + \\beta_1 \\cdot \\texttt{x} + \\texttt{Noise} $$\n\n\n## Goals\n\n- Here's a visual way of thinking about what I said on the previous slide. Consider the following scatterplot:\n\n:::{.fragment}\n![](scatterblank.svg){width=\"50%\"}\n:::\n\n\n\n## Goals\n\n:::{.nonincremental}\n- We are assuming that there exists some true linear relationship (i.e. some \"fit\") between `Y` and `X`. But, because of natural variability due to randomness, we cannot figure out exactly *what* the true relationship is.\n:::\n\n:::{.fragment}\n![](scatter2.svg){width=\"50%\"}\n:::\n\n\n\n\n## Goals\n\n:::{.nonincremental}\n- Finding the \"best\" estimate of the fit is, therefore, akin to finding the line that \"best\" fits the data.\n:::\n\n:::{.fragment}\n![](scatter5.svg){width=\"50%\"}\n:::\n\n## Line of Best Fit\n\n- Now, if we are to find the line that best fits the data, we first need to quantify what we mean by \"best\".\n\n- Here is one idea: consider minimizing the average distance from the datapoints to the line.\n\n- As a measure of \"average distance from the points to the line\", we will use the so-called **residual sum of squares** (often abbreviated as RSS).\n\n\n## Residuals {style=\"font-size:32px\"}\n\n- The **_i_^th^ residual** is defined to be the quantity $e_i$ below:\n\n:::{.fragment style=\"text-align:center\"}\n![](rss.svg){width=\"50%\"}\n:::\n\n- RSS is then just $\\displaystyle \\mathrm{RSS} = \\sum_{i=1}^{n} e_i^2$\n\n## Results\n\n- It turns out, using a bit of Calculus, the estimators we seek (i.e. the ones that minimize the RSS) are\n\\begin{align*}\n  \\widehat{\\beta_1}   & = \\frac{\\sum\\limits_{i=1}^{n} (x_i - \\overline{x})(y_i - \\overline{y})}{\\sum\\limits_{i=1}^{n} (x_i - \\overline{x})^2}    \\\\\n  \\widehat{\\beta_0}   & = \\overline{y} - \\widehat{\\beta_1} \\overline{x}\n\\end{align*}\n\n- These are what are known as the **ordinary least squares** estimators of $\\beta_0$ and $\\beta_1$, and the line $\\widehat{\\beta_0} + \\widehat{\\beta_1} x$ is called the **least-squares regression line**.\n\n- Perhaps an example may illustrate what I am talking about. \n\n## Example\n\n```{r}\nset.seed(123)\nz <- rnorm(100)\nw <- -2 * z + rnorm(100, 0, 2)\n\ndata.frame(z, w) %>%\n  ggplot(aes(x = z, y = w)) +\n  geom_point(size = 4) +\n  theme_economist(base_size = 24) +\n  ggtitle(\"Y vs. X\") +\n  theme(panel.background = element_rect(\"#f0ebd8\"),\n        plot.background = element_rect(fill = \"#f0ebd8\"),\n        axis.title.y = element_text(size = 16,\n                                    margin = margin(\n                                      t = 0, \n                                      r = 10,\n                                      b = 0, \n                                      l = 0)),\n        axis.title.x = element_text(size = 16,\n                                    margin = margin(\n                                      t = 10, \n                                      r = 0,\n                                      b = 0, \n                                      l = 0)),\n        title = element_text(size = 18)\n  )\n```\n\n\n## Example\n\n```{r}\nset.seed(123)\nz <- rnorm(100)\nw <- -2 * z + rnorm(100, 0, 2)\n\ndata.frame(z, w) %>%\n  ggplot(aes(x = z, y = w)) +\n  geom_point(size = 4) +\n  theme_economist(base_size = 24) +\n  ggtitle(\"Y vs. X\") +\n  theme(panel.background = element_rect(\"#f0ebd8\"),\n        plot.background = element_rect(fill = \"#f0ebd8\"),\n        axis.title.y = element_text(size = 16,\n                                    margin = margin(\n                                      t = 0, \n                                      r = 10,\n                                      b = 0, \n                                      l = 0)),\n        axis.title.x = element_text(size = 16,\n                                    margin = margin(\n                                      t = 10, \n                                      r = 0,\n                                      b = 0, \n                                      l = 0)),\n        title = element_text(size = 18)\n  ) +\n  geom_smooth(method = \"lm\", se = FALSE, linewidth = 2)\n\nfit10 <- lm(w ~ z)\n```\n\n- $\\widehat{\\beta_0} =$ `r fit10$coefficients[1]`; \\quad $\\widehat{\\beta_1} =$ `r fit10$coefficients[2]`.\n\n- I.e. the equation of the line in blue is  `r fit10$coefficients[1]` + `r fit10$coefficients[2]` * `x`.\n\n## Fitted Values {style=\"font-size:30px\"}\n\n- Let's return to our cartoon picture of OLS regression:\n\n:::{.fragment style=\"text-align:center\"}\n![](rss.svg){width=\"50%\"}\n:::\n\n## Fitted Values {style=\"font-size:30px\"}\n\n- Notice that each point in our dataset (i.e. the blue points) have a corresponding point on the OLS regression line:\n\n\n:::{.fragment style=\"text-align:center\"}\n![](fitted.svg){width=\"50%\"}\n:::\n\n\n## Fitted Values {style=\"font-size:30px\"}\n\n- These points are referred to as **fitted values**; the _y_-values of the fitted values are denoted as $\\widehat{y}_i$.\n\n- In this way, the OLS regression line is commonly written as a relationship between the fitted values and the _x_-values:\n$$ \\widehat{y} = \\widehat{\\beta_0} + \\widehat{\\beta_1} x $$\n\n## Back to `height` and `weight` {style=\"font-size:30px\"}\n\n- Before we work through the math once, let's apply this technique to the height and weight data from before. \n\n:::{.fragment}\n```{r, fig.width = 7, fig.height = 5}\nset.seed(123)\n\ndata.frame(x, y) %>%\n  ggplot(aes(x = x, y = y)) +\n  geom_point(size = 4) +\n  theme_economist(base_size = 24) +\n  ggtitle(\"Weight vs. Height\") +\n  theme(panel.background = element_rect(\"#f0ebd8\"),\n        plot.background = element_rect(fill = \"#f0ebd8\"),\n        axis.title.y = element_text(size = 16,\n                                    margin = margin(\n                                      t = 0, \n                                      r = 10,\n                                      b = 0, \n                                      l = 0)),\n        axis.title.x = element_text(size = 16,\n                                    margin = margin(\n                                      t = 10, \n                                      r = 0,\n                                      b = 0, \n                                      l = 0)),\n        title = element_text(size = 18)\n  ) +\n  ylab(\"weight (lbs)\") +\n  xlab(\"height (cm)\") \n```\n:::\n\n\n## Back to `height` and `weight` {style=\"font-size:30px\"}\n\n- Using a computer software, the OLS regression line can be found to be:\n\n::::{.columns}\n\n:::{.column width=\"75%\"}\n:::{.fragment}\n```{r, fig.width = 7, fig.height = 5}\nset.seed(123)\n\ndata.frame(x, y) %>%\n  ggplot(aes(x = x, y = y)) +\n  geom_point(size = 4) +\n  theme_economist(base_size = 24) +\n  ggtitle(\"Weight vs. Height\") +\n  theme(panel.background = element_rect(\"#f0ebd8\"),\n        plot.background = element_rect(fill = \"#f0ebd8\"),\n        axis.title.y = element_text(size = 16,\n                                    margin = margin(\n                                      t = 0, \n                                      r = 10,\n                                      b = 0, \n                                      l = 0)),\n        axis.title.x = element_text(size = 16,\n                                    margin = margin(\n                                      t = 10, \n                                      r = 0,\n                                      b = 0, \n                                      l = 0)),\n        title = element_text(size = 18)\n  ) +\n  ylab(\"weight (lbs)\") +\n  xlab(\"height (cm)\") +\n  geom_smooth(method = \"lm\", se = F, col = \"blue\", linewidth = 1)\n\nhw1 <- lm(y ~ x)\n```\n:::\n:::\n\n:::{.column width=\"25%\"}\n- Specifically, $\\widehat{\\beta_0} =$ `r hw1$coefficients[1]` and $\\widehat{\\beta_1} =$ `r hw1$coefficients[2]`\n\n\n\\\n\n- We will return to the notion of fitted values in a bit.\n:::\n\n::::\n\n\n## Back to `height` and `weight`\n\n- A quick note:\n\n\n- Though there was no way to know this, the true $\\beta_1$ was actually $1.0$. Again, this is just to demonstrate that the OLS estimate $\\widehat{\\beta_1}$ is just that- an estimate!\n\n\n---\n\n:::{.fragment}\n```{r, fig.width = 7, fig.height = 5, fig.align = 'center'}\nset.seed(123)\n\ndata.frame(x, y) %>%\n  ggplot(aes(x = x, y = y)) +\n  geom_point(size = 4) +\n  theme_economist(base_size = 24) +\n  ggtitle(\"Weight vs. Height\") +\n  theme(panel.background = element_rect(\"#f0ebd8\"),\n        plot.background = element_rect(fill = \"#f0ebd8\"),\n        axis.title.y = element_text(size = 16,\n                                    margin = margin(\n                                      t = 0, \n                                      r = 10,\n                                      b = 0, \n                                      l = 0)),\n        axis.title.x = element_text(size = 16,\n                                    margin = margin(\n                                      t = 10, \n                                      r = 0,\n                                      b = 0, \n                                      l = 0)),\n        title = element_text(size = 18)\n  ) +\n  ylab(\"weight (lbs)\") +\n  xlab(\"height (cm)\") +\n  geom_smooth(method = \"lm\", se = F, col = \"blue\", linewidth = 1) +\n  geom_abline(slope = 1, col = \"red\", linewidth = 1)\n\nhw1 <- lm(y ~ x)\n```\n:::\n\n:::{.fragment}\n$$ \\widehat{\\texttt{weight}} = 3.367 + 0.979 \\cdot \\texttt{height} $$\n:::\n\n\n## Worked-Out Example\n\n- Alright, let's work through a computation by hand once.\n\n- Suppose we have the variables\n\\begin{align*}\n  \\boldsymbol{x}    & = \\{3, \\ 7, \\ 8\\}   \\\\\n  \\boldsymbol{y}    & = \\{20, \\ 14, \\ 17\\}\n\\end{align*}\nand suppose we wish to construct the least-squares regression line when regressing $\\boldsymbol{y}$ onto $\\boldsymbol{x}$.\n\n- First, we compute\n\\begin{align*}\n  \\overline{x}    & = 6   \\\\\n  \\overline{y}    & = 17\n\\end{align*}\n\n## Worked-Out Example {style=\"font-size:30px\"}\n\n- Next, we compute\n\\begin{align*}\n  \\sum_{i=1}^{n} (x_i - \\overline{x})^2   & = (3 - 6)^2 + (7 - 6)^2 + (8 - 6)^2 = 14   \\\\\n  \\sum_{i=1}^{n} (y_i - \\overline{y})^2   & = (20 - 17)^2 + (14 - 17)^2 + (17 - 17)^2 = 18\n\\end{align*}\n\n- Additionally,\n\\begin{align*}\n  \\sum_{i=1}^{n} (x_i - \\overline{x})(y_i - \\overline{y})   & = (3 - 6)(20 - 17) + (7 - 6)(14 - 17)    \\\\[-7mm]\n    & \\hspace{10mm} + (8 - 6)(17 - 17)    \\\\[5mm]\n    & = -12\n  \\end{align*}\n  \n## Worked-Out Example\n\n- Therefore,\n$$ \\widehat{\\beta_1} = \\frac{\\sum_{i=1}^{n} (x_i - \\overline{x})(y_i - \\overline{y})}{\\sum_{i=1}^{n} (x_i - \\overline{x})^2}   = \\frac{-12}{14} = - \\frac{6}{7} $$\n\n- Additionally,\n$$ \\widehat{\\beta_0} = \\overline{y} - \\widehat{\\beta_1} \\overline{x} = 17 - \\left( - \\frac{6}{7} \\right) (6) = \\frac{155}{7} $$\n\n- This means that the ordinary least-squares regression line is\n$$ \\boxed{\\widehat{y} = \\frac{1}{7} ( 155 - 6 x )} $$\n\n---\n\n\n:::{.fragment}\n```{r, fig.width = 7, fig.height = 5, fig.align = 'center'}\nset.seed(123)\n\ndata.frame(x = c(3, 7, 8), y = c(20, 14, 17)) %>%\n  ggplot(aes(x = x, y = y)) +\n  geom_point(size = 4) +\n  theme_economist(base_size = 24) +\n  ggtitle(\"Y vs. X\") +\n  theme(panel.background = element_rect(\"#f0ebd8\"),\n        plot.background = element_rect(fill = \"#f0ebd8\"),\n        axis.title.y = element_text(size = 16,\n                                    margin = margin(\n                                      t = 0, \n                                      r = 10,\n                                      b = 0, \n                                      l = 0)),\n        axis.title.x = element_text(size = 16,\n                                    margin = margin(\n                                      t = 10, \n                                      r = 0,\n                                      b = 0, \n                                      l = 0)),\n        title = element_text(size = 18)\n  ) +\n  ylab(\"y\") +\n  xlab(\"x\") +\n  geom_smooth(method = \"lm\", se = F, col = \"blue\", linewidth = 1) \n\nhw1 <- lm(y ~ x)\n```\n:::\n\n:::{.fragment}\n$$ \\widehat{y} = \\frac{1}{7} ( 155 - 6 x ) $$\n:::\n\n## Interpreting the Coefficients\n\n- Alright, so how do we interpret the OLS regression line? $$\\widehat{y} = \\widehat{\\beta_0} + \\widehat{\\beta_1} x$$\n\n- We can see that a one-unit increase in `x` corresponds to a $\\widehat{\\beta_1}$-unit increase in `y`. \n\n    - For example, in our `height` and `weight` example we found $$ \\widehat{\\texttt{weight}} = 3.367 + 0.979 \\cdot \\texttt{height} $$\n    \n    - This means that a one-cm change in height is associated with a (predicted/estimated) 0.979 lbs change in weight.\n\n## Prediction {style=\"font-size:30px\"}\n\n- We can also use the OLS regression line to perform **prediction**.\n\n- To see how this works, let's return to our toy example:\n\n:::{.fragment}\n```{r, fig.width = 7, fig.height = 5, fig.align = 'center'}\nset.seed(123)\n\ndata.frame(x = c(3, 7, 8), y = c(20, 14, 17)) %>%\n  ggplot(aes(x = x, y = y)) +\n  geom_point(size = 4) +\n  theme_economist(base_size = 24) +\n  ggtitle(\"Y vs. X\") +\n  theme(panel.background = element_rect(\"#f0ebd8\"),\n        plot.background = element_rect(fill = \"#f0ebd8\"),\n        axis.title.y = element_text(size = 16,\n                                    margin = margin(\n                                      t = 0, \n                                      r = 10,\n                                      b = 0, \n                                      l = 0)),\n        axis.title.x = element_text(size = 16,\n                                    margin = margin(\n                                      t = 10, \n                                      r = 0,\n                                      b = 0, \n                                      l = 0)),\n        title = element_text(size = 18)\n  ) +\n  ylab(\"weight (lbs)\") +\n  xlab(\"height (cm)\")\n```\n:::\n\n- Notice that we do not have an `x`-observation of 5. As such, we don't know what the `y`-value corresponding to an `x`-value of 5 is.\n\n## Prediction {style=\"font-size:30px\"}\n\n- However, we do have a decent *guess* as to what the `y`-value corresponding to an `x-`value of 5 is- the corresponding fitted value!\n\n:::{.fragment}\n```{r, fig.width = 7, fig.height = 5, fig.align = 'center'}\nset.seed(123)\n\ndata.frame(x = c(3, 7, 8), y = c(20, 14, 17)) %>%\n  ggplot(aes(x = x, y = y)) +\n  geom_point(size = 4) +\n  theme_economist(base_size = 24) +\n  ggtitle(\"Y vs. X\") +\n  theme(panel.background = element_rect(\"#f0ebd8\"),\n        plot.background = element_rect(fill = \"#f0ebd8\"),\n        axis.title.y = element_text(size = 16,\n                                    margin = margin(\n                                      t = 0, \n                                      r = 10,\n                                      b = 0, \n                                      l = 0)),\n        axis.title.x = element_text(size = 16,\n                                    margin = margin(\n                                      t = 10, \n                                      r = 0,\n                                      b = 0, \n                                      l = 0)),\n        title = element_text(size = 18)\n  ) +\n  ylab(\"weight (lbs)\") +\n  xlab(\"height (cm)\") +\n  geom_smooth(method = \"lm\", se = F, col = \"blue\", linewidth = 1) +\n  geom_point(aes(x = 5, y = (1/7) * (155 - 6 * 5)),\n             size = 5, \n             col = \"blue\")\n```\n:::\n\n:::{.fragment}\n$$ \\widehat{y}_5 = \\frac{1}{7} (155 - 6 \\cdot 5) \\approx 17.857 $$\n:::\n\n## Extrapolation, and the Dangers Thereof {style=\"font-size:30px\"}\n\n- Let's look at another toy dataset:\n\n:::{.fragment}\n```{r, fig.width = 7, fig.height = 5, fig.align = 'center'}\nset.seed(123)\n\nt <- rnorm(100, 0, 10)\nyt <- 0.5*(t - 80)^2 + rnorm(100, 0, 100)\n  \ndata.frame(t, yt) %>%\n  ggplot(aes(x = t, y = yt)) +\n  geom_point(size = 4) +\n  theme_economist(base_size = 24) +\n  ggtitle(\"Weight vs. Height\") +\n  theme(panel.background = element_rect(\"#f0ebd8\"),\n        plot.background = element_rect(fill = \"#f0ebd8\"),\n        axis.title.y = element_text(size = 16,\n                                    margin = margin(\n                                      t = 0, \n                                      r = 10,\n                                      b = 0, \n                                      l = 0)),\n        axis.title.x = element_text(size = 16,\n                                    margin = margin(\n                                      t = 10, \n                                      r = 0,\n                                      b = 0, \n                                      l = 0)),\n        title = element_text(size = 18)\n  ) +\n  ylab(\"y\") +\n  xlab(\"x\") +\n  geom_smooth(method = \"lm\", se = F, col = \"blue\", linewidth = 1)\n\nreg2 <- lm(yt ~ t)\n```\n:::\n\n- Looks pretty linear, right?\n\n## Extrapolation, and the Dangers Thereof {style=\"font-size:30px\"}\n\n- Say we want to predict the corresponding `y` value of an `x` value of, 40. \n\n- Following our steps from before, we would just find the fitted value corresponding to `x` = 40:\n\n:::{.fragment}\n```{r, fig.width = 7, fig.height = 5, fig.align = 'center'}\nset.seed(123)\n\nt <- rnorm(100, 0, 10)\nyt <- 0.5*(t - 80)^2 + rnorm(100, 0, 100)\n  \ndata.frame(t, yt) %>%\n  ggplot(aes(x = t, y = yt)) +\n  geom_point(size = 4) +\n  theme_economist(base_size = 24) +\n  ggtitle(\"Weight vs. Height\") +\n  theme(panel.background = element_rect(\"#f0ebd8\"),\n        plot.background = element_rect(fill = \"#f0ebd8\"),\n        axis.title.y = element_text(size = 16,\n                                    margin = margin(\n                                      t = 0, \n                                      r = 10,\n                                      b = 0, \n                                      l = 0)),\n        axis.title.x = element_text(size = 16,\n                                    margin = margin(\n                                      t = 10, \n                                      r = 0,\n                                      b = 0, \n                                      l = 0)),\n        title = element_text(size = 18)\n  ) +\n  ylab(\"y\") +\n  xlab(\"x\") +\n  geom_smooth(method = \"lm\", se = F, col = \"blue\", linewidth = 1) +\n  geom_point(aes(x = 40, y = reg2$coefficients[1] + reg2$coefficients[2] * 40),\n             col = \"blue\", size = 4)\n\nreg2 <- lm(yt ~ t)\n```\n:::\n\n## Extrapolation, and the Dangers Thereof {style=\"font-size:30px\"}\n\n- Here's the kicker: **the true fit was actually NOT linear!**\n\n    - Specifically, I used a quadratic relationship between `x` and `y` to generate the data.\n    \n    - When you zoom in close enough, parabolas look linear!\n    \n- Now, we wouldn't have had any way of knowing this. \n\n- This is why it is a bad idea to try to **extrapolate** too far.\n\n    - Extrapolation is the name we give to trying to apply a model estimate to values that are very far outside the realm of the original data.\n    \n    - How far is \"very far\"? Statisticians disagree on this front. For the purposes of this class, just use your best judgment.\n    \n## One Final Connection {style=\"font-size:30px\"}\n\n- Now, one final thing I'd like to mention: note that the slope of the OLS regression line is *not* just the correlation coefficient.\n\n    - Again, the magnitude of the correlation coefficient just gives us a measure of how strong the relationship between the two variables is.\n    \n- There is, however, a relationship between $\\widehat{\\beta_1}$ and _r_: it turns out that\n$$ \\widehat{\\beta_1} = \\frac{s_Y}{s_X} \\cdot r  $$\n\n- A question may arise: do we really believe our OLS estimate of the slope?\n\n- Remember that $\\widehat{\\beta_1}$ is just an *estimator* of $\\beta_1$. \n\n    - Next time, we'll talk about how to construct confidence intervals for $\\widehat{\\beta_1}$.\n\n\n## Your Turn!\n\n\n::: callout-tip\n## Exercise 2\n\n:::{.nonincremental}\n::: {style=\"font-size: 24px\"}\nAn airline is interested in determining the relationship between flight duration (in minutes) and the net amount of soda consumed (in oz.). Letting `x` denote `flight duration` (the explanatory variable) and `y` denote `amount of soda consumed` (the response variable), a sample of size 100 yielded the following results:\n$$ \\begin{array}{cc}\n  \\displaystyle \\sum_{i=1}^{n} x_i  = 10,\\!211.7;   & \\displaystyle \\sum_{i=1}^{n} (x_i - \\overline{x})^2 =  38,\\!760.68    \\\\\n  \\displaystyle \\sum_{i=1}^{n} y_i  = 14,\\!3995.8;   & \\displaystyle \\sum_{i=1}^{n} (y_i - \\overline{y})^2 =  87.23984   \\\\\n \\displaystyle \\sum_{i=1}^{n} (x_i - \\overline{x})(y_i - \\overline{y}) = 379.945 \\\\\n\\end{array} $$\n\na. Find the equation of the OLS Regression line.\nb. If a particular flight has a duration of 110 minutes, how many ounces of soda would we expect to be consumed on the flight?\n:::\n:::\n:::\n\n"},"formats":{"revealjs":{"execute":{"fig-width":10,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":false,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","html-math-method":"mathjax","slide-level":2,"to":"revealjs","incremental":true,"output-file":"Lec18.html"},"language":{},"metadata":{"lang":"en","fig-responsive":false,"quarto-version":"1.2.335","auto-stretch":true,"title":"PSTAT 5A: Lecture 18","subtitle":"Correlation, and an Intro to Regression","author":"Ethan P. Marzban","date":"6/1/23","editor":"source","title-slide-attributes":{"data-background-image":"5a_hex.png","data-background-size":"contain","data-background-opacity":"0.5","data-background-position":"left"},"theme":["default","custom.scss"],"logo":"5a_hex.png","template-partials":["title-slide.html"]}}}}